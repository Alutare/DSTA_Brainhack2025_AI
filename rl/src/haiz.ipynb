{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23fcaf9e-15d2-4db1-85bc-e1019f21928a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TILAIEnv:\n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"Initialize the TIL-AI environment.\"\"\"\n",
    "        self.grid_size = 16\n",
    "        self.max_steps = 100\n",
    "        self.obstacles = set()  # Will be initialized in reset()\n",
    "        self.reset(seed=seed)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.is_scout = bool(random.randint(0, 1))  # scout is 1, guard is 0\n",
    "        self.direction = random.randint(0, 3)  # 0: right, 1: down, 2: left, 3: up\n",
    "        \n",
    "        # Create obstacles - but not too many to ensure agent can move\n",
    "        self.obstacles = set()\n",
    "        for _ in range(30):\n",
    "            self.obstacles.add((random.randint(0, 15), random.randint(0, 15)))\n",
    "        \n",
    "        # Place agent at a location without obstacles\n",
    "        while True:\n",
    "            self.location = [random.randint(0, 15), random.randint(0, 15)]\n",
    "            if tuple(self.location) not in self.obstacles:\n",
    "                break\n",
    "        \n",
    "        # Generate recon points and missions, excluding obstacle and agent locations\n",
    "        self.recon_points = set()\n",
    "        self.missions = set()\n",
    "        \n",
    "        for _ in range(100):\n",
    "            while True:\n",
    "                point = (random.randint(0, 15), random.randint(0, 15))\n",
    "                if point not in self.obstacles and point != tuple(self.location):\n",
    "                    self.recon_points.add(point)\n",
    "                    break\n",
    "                    \n",
    "        for _ in range(20):\n",
    "            while True:\n",
    "                point = (random.randint(0, 15), random.randint(0, 15))\n",
    "                if point not in self.obstacles and point != tuple(self.location) and point not in self.recon_points:\n",
    "                    self.missions.add(point)\n",
    "                    break\n",
    "        \n",
    "        self.visited = set()\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Generate observation based on agent's current state.\"\"\"\n",
    "        # Create a simplified viewcone\n",
    "        viewcone = np.zeros((7, 5), dtype=np.uint8)\n",
    "        \n",
    "        # Add some simple content to the viewcone based on agent's surroundings\n",
    "        x, y = self.location\n",
    "        direction = self.direction\n",
    "        \n",
    "        # Fill the viewcone with simplified information\n",
    "        # This is a simplified implementation - the actual competition will have more complex logic\n",
    "        for i in range(7):\n",
    "            for j in range(5):\n",
    "                # Calculate relative position in the grid\n",
    "                dx = j - 2  # -2 to 2 (left to right)\n",
    "                dy = i - 2  # -2 to 4 (back to front, with more visibility forward)\n",
    "                \n",
    "                # Rotate based on agent's direction\n",
    "                if direction == 0:  # right\n",
    "                    nx, ny = x + dx, y + dy - 2  # Adjust forward visibility\n",
    "                elif direction == 1:  # down\n",
    "                    nx, ny = x - dy + 2, y + dx  # Rotate 90° clockwise\n",
    "                elif direction == 2:  # left\n",
    "                    nx, ny = x - dx, y - dy + 2  # Rotate 180°\n",
    "                elif direction == 3:  # up\n",
    "                    nx, ny = x + dy - 2, y - dx  # Rotate 270° clockwise\n",
    "                \n",
    "                # Set value based on what's at this position\n",
    "                if 0 <= nx < 16 and 0 <= ny < 16:  # Within grid bounds\n",
    "                    if (nx, ny) in self.obstacles:\n",
    "                        viewcone[i, j] = 129  # Empty tile (1) with walls (128)\n",
    "                    elif (nx, ny) in self.recon_points:\n",
    "                        viewcone[i, j] = 2  # Recon point\n",
    "                    elif (nx, ny) in self.missions:\n",
    "                        viewcone[i, j] = 3  # Mission\n",
    "                    else:\n",
    "                        viewcone[i, j] = 1  # Empty tile\n",
    "                else:\n",
    "                    viewcone[i, j] = 0  # No vision (out of bounds)\n",
    "        \n",
    "        return {\n",
    "            \"viewcone\": viewcone.tolist(),\n",
    "            \"direction\": self.direction,\n",
    "            \"location\": self.location,\n",
    "            \"scout\": int(self.is_scout),\n",
    "            \"step\": self.step_count\n",
    "        }\n",
    "\n",
    "    def _min_manhattan_distance(self):\n",
    "        \"\"\"Calculate minimum Manhattan distance to relevant targets.\"\"\"\n",
    "        targets = self.recon_points if self.is_scout else self.missions\n",
    "        if not targets:\n",
    "            return 0 \n",
    "        return min(abs(self.location[0] - tx) + abs(self.location[1] - ty) for tx, ty in targets)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the agent's action.\"\"\"\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True\n",
    "\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Store original location for debugging\n",
    "        original_location = self.location.copy()\n",
    "        \n",
    "        # Process the action\n",
    "        dx, dy = 0, 0\n",
    "        if action == 0:  # Move forward\n",
    "            dx, dy = self._move_vector(self.direction)\n",
    "        elif action == 1:  # Move backward\n",
    "            dx, dy = self._move_vector((self.direction + 2) % 4)\n",
    "        elif action == 2:  # Turn left\n",
    "            self.direction = (self.direction - 1) % 4\n",
    "        elif action == 3:  # Turn right\n",
    "            self.direction = (self.direction + 1) % 4\n",
    "        # Action 4 is stay (do nothing)\n",
    "\n",
    "        # Calculate new location\n",
    "        if action in [0, 1]:  # Only move for forward/backward actions\n",
    "            new_x = np.clip(self.location[0] + dx, 0, 15)\n",
    "            new_y = np.clip(self.location[1] + dy, 0, 15)\n",
    "            new_loc = (new_x, new_y)\n",
    "            \n",
    "            # Only update if not blocked by an obstacle\n",
    "            if new_loc not in self.obstacles:\n",
    "                self.location = [new_x, new_y]\n",
    "        \n",
    "        # Get current location as tuple for easier checking\n",
    "        loc_tuple = tuple(self.location)\n",
    "        \n",
    "        # Base reward slightly negative to encourage efficient paths\n",
    "        reward = -0.01\n",
    "        \n",
    "        # Penalty for revisiting locations\n",
    "        if loc_tuple in self.visited:\n",
    "            reward -= 0.01\n",
    "        else:\n",
    "            self.visited.add(loc_tuple)\n",
    "\n",
    "        # Role-specific rewards\n",
    "        if self.is_scout:\n",
    "            # Scout collects recon points\n",
    "            if loc_tuple in self.recon_points:\n",
    "                reward += 1  # Match competition reward\n",
    "                self.recon_points.remove(loc_tuple)\n",
    "                \n",
    "            # Scout completes missions\n",
    "            if loc_tuple in self.missions:\n",
    "                reward += 5  # Match competition reward\n",
    "                self.missions.remove(loc_tuple)\n",
    "                \n",
    "            # Small chance of capture (game ending for scout)\n",
    "            if random.random() < 0.01:\n",
    "                reward -= 50  # Match competition punishment\n",
    "                self.done = True\n",
    "                \n",
    "        else:  # Guard\n",
    "            # Guard captures scout (simulation)\n",
    "            if random.random() < 0.01:\n",
    "                reward += 50  # Match competition reward\n",
    "                self.done = True\n",
    "\n",
    "        # Add small reward for getting closer to objectives\n",
    "        reward += 0.01 * (1 / (1 + self._min_manhattan_distance()))\n",
    "        \n",
    "        # Check for episode termination\n",
    "        if self.step_count >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_obs(), reward, self.done\n",
    "\n",
    "    def _move_vector(self, direction):\n",
    "        \"\"\"Get the movement vector for a given direction.\"\"\"\n",
    "        return [(1, 0), (0, 1), (-1, 0), (0, -1)][direction]\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the environment.\"\"\"\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        \n",
    "        # Add obstacles, recon points, and missions to the grid\n",
    "        for ox, oy in self.obstacles:\n",
    "            grid[oy][ox] = 'X'\n",
    "        \n",
    "        for rx, ry in self.recon_points:\n",
    "            grid[ry][rx] = 'R'\n",
    "            \n",
    "        for mx, my in self.missions:\n",
    "            grid[my][mx] = 'M'\n",
    "        \n",
    "        # Add agent to the grid\n",
    "        x, y = self.location\n",
    "        grid[y][x] = 'S' if self.is_scout else 'G'\n",
    "        \n",
    "        # Print the grid\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(f\"Direction: {['Right', 'Down', 'Left', 'Up'][self.direction]}\")\n",
    "        print(f\"Step: {self.step_count}/{self.max_steps}\")\n",
    "        print()\n",
    "    \n",
    "    # NEW HELPER METHOD: Convert observation to tensor state for the model\n",
    "    def obs_to_tensor(self, obs):\n",
    "        \"\"\"Convert observation dict to flattened tensor state for the model.\"\"\"\n",
    "        # Extract components from observation\n",
    "        viewcone = torch.tensor(obs[\"viewcone\"], dtype=torch.float32).flatten()\n",
    "        direction = F.one_hot(torch.tensor(obs[\"direction\"]), num_classes=4).float()\n",
    "        location = torch.tensor(obs[\"location\"], dtype=torch.float32) / 15.0  # Normalize\n",
    "        scout = torch.tensor([obs[\"scout\"]], dtype=torch.float32)\n",
    "        step = torch.tensor([obs[\"step\"] / self.max_steps], dtype=torch.float32)  # Normalize\n",
    "        \n",
    "        # Concatenate all features\n",
    "        state = torch.cat([viewcone, direction, location, scout, step])\n",
    "        return state\n",
    "\n",
    "\n",
    "# Implement the Attention Layer as recommended\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure x is 2D for the attention mechanism\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        weights = self.attention(x)\n",
    "        return x * weights  # Element-wise multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e137a38-8841-4abd-bf70-20cdfc0d4ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 100/20000 | Time: 31.2s | Avg Reward: 9.95 | Scout: 7.73 | Guard: 2.40 | Scout Captures: 18 | Scout Collections: 23 | Guard Captures: 12 | Epsilon (S/G): 0.398/0.297\n",
      "Learning rates - Scout: 0.000047 | Guard: 0.000189\n",
      "Ep 200/20000 | Time: 64.1s | Avg Reward: -4.07 | Scout: 3.77 | Guard: 4.07 | Scout Captures: 40 | Scout Collections: 50 | Guard Captures: 23 | Epsilon (S/G): 0.395/0.295\n",
      "Learning rates - Scout: 0.000039 | Guard: 0.000164\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: 5.40 | Guard: -9.13\n",
      "New best Scout model! Reward: 5.40\n",
      "New best Guard model! Reward: -9.13\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Best model saved!\n",
      "Ep 300/20000 | Time: 103.0s | Avg Reward: 4.43 | Scout: 7.28 | Guard: -3.47 | Scout Captures: 58 | Scout Collections: 80 | Guard Captures: 30 | Epsilon (S/G): 0.393/0.293\n",
      "Learning rates - Scout: 0.000024 | Guard: 0.000137\n",
      "Ep 400/20000 | Time: 138.1s | Avg Reward: 5.13 | Scout: 6.20 | Guard: 4.78 | Scout Captures: 76 | Scout Collections: 111 | Guard Captures: 46 | Epsilon (S/G): 0.391/0.290\n",
      "Learning rates - Scout: 0.000012 | Guard: 0.000103\n",
      "Stored historical models at episode 500\n",
      "Ep 500/20000 | Time: 177.9s | Avg Reward: 0.47 | Scout: -4.07 | Guard: 0.80 | Scout Captures: 102 | Scout Collections: 145 | Guard Captures: 56 | Epsilon (S/G): 0.388/0.289\n",
      "Learning rates - Scout: 0.000005 | Guard: 0.000074\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: 0.92 | Guard: 16.71\n",
      "New best Guard model! Reward: 16.71\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Best model saved!\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Ep 600/20000 | Time: 224.5s | Avg Reward: 2.18 | Scout: 5.81 | Guard: 6.04 | Scout Captures: 119 | Scout Collections: 173 | Guard Captures: 69 | Epsilon (S/G): 0.386/0.286\n",
      "Learning rates - Scout: 0.000007 | Guard: 0.000046\n",
      "Ep 700/20000 | Time: 266.6s | Avg Reward: 6.31 | Scout: -0.75 | Guard: 2.24 | Scout Captures: 139 | Scout Collections: 198 | Guard Captures: 81 | Epsilon (S/G): 0.384/0.283\n",
      "Learning rates - Scout: 0.000015 | Guard: 0.000024\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: -12.57 | Guard: 15.69\n",
      "Ep 800/20000 | Time: 315.3s | Avg Reward: 0.74 | Scout: 1.66 | Guard: 11.96 | Scout Captures: 162 | Scout Collections: 230 | Guard Captures: 95 | Epsilon (S/G): 0.381/0.281\n",
      "Learning rates - Scout: 0.000029 | Guard: 0.000020\n",
      "Ep 900/20000 | Time: 366.1s | Avg Reward: 10.13 | Scout: 8.22 | Guard: 4.16 | Scout Captures: 180 | Scout Collections: 260 | Guard Captures: 108 | Epsilon (S/G): 0.379/0.279\n",
      "Learning rates - Scout: 0.000042 | Guard: 0.000027\n",
      "Stored historical models at episode 1000\n",
      "Ep 1000/20000 | Time: 414.5s | Avg Reward: 7.65 | Scout: 9.69 | Guard: 11.93 | Scout Captures: 200 | Scout Collections: 286 | Guard Captures: 121 | Epsilon (S/G): 0.377/0.277\n",
      "Learning rates - Scout: 0.000049 | Guard: 0.000048\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: -3.12 | Guard: -0.74\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Moving to curriculum phase 2\n",
      "Ep 1100/20000 | Time: 463.7s | Avg Reward: 5.68 | Scout: -0.69 | Guard: 12.74 | Scout Captures: 220 | Scout Collections: 314 | Guard Captures: 138 | Epsilon (S/G): 0.375/0.275\n",
      "Learning rates - Scout: 0.000046 | Guard: 0.000407\n",
      "Ep 1200/20000 | Time: 511.4s | Avg Reward: -0.31 | Scout: -0.29 | Guard: 4.81 | Scout Captures: 241 | Scout Collections: 336 | Guard Captures: 155 | Epsilon (S/G): 0.373/0.272\n",
      "Learning rates - Scout: 0.000042 | Guard: 0.000687\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: -8.69 | Guard: 16.05\n",
      "Ep 1300/20000 | Time: 569.1s | Avg Reward: -5.39 | Scout: 4.60 | Guard: -3.18 | Scout Captures: 254 | Scout Collections: 353 | Guard Captures: 165 | Epsilon (S/G): 0.371/0.269\n",
      "Learning rates - Scout: 0.000035 | Guard: 0.000929\n",
      "Ep 1400/20000 | Time: 622.1s | Avg Reward: 9.17 | Scout: 15.26 | Guard: 2.85 | Scout Captures: 267 | Scout Collections: 376 | Guard Captures: 180 | Epsilon (S/G): 0.369/0.266\n",
      "Learning rates - Scout: 0.000025 | Guard: 0.001030\n",
      "Stored historical models at episode 1500\n",
      "Ep 1500/20000 | Time: 677.0s | Avg Reward: 1.43 | Scout: 8.27 | Guard: 3.69 | Scout Captures: 282 | Scout Collections: 401 | Guard Captures: 194 | Epsilon (S/G): 0.367/0.264\n",
      "Learning rates - Scout: 0.000016 | Guard: 0.001002\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: -18.07 | Guard: 11.89\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Ep 1600/20000 | Time: 737.8s | Avg Reward: -2.30 | Scout: 1.29 | Guard: 2.92 | Scout Captures: 301 | Scout Collections: 427 | Guard Captures: 207 | Epsilon (S/G): 0.366/0.261\n",
      "Learning rates - Scout: 0.000009 | Guard: 0.000812\n",
      "Ep 1700/20000 | Time: 797.1s | Avg Reward: -2.76 | Scout: 0.97 | Guard: 3.31 | Scout Captures: 319 | Scout Collections: 451 | Guard Captures: 221 | Epsilon (S/G): 0.364/0.258\n",
      "Learning rates - Scout: 0.000005 | Guard: 0.000595\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: 3.77 | Guard: 12.00\n",
      "Ep 1800/20000 | Time: 860.7s | Avg Reward: 9.55 | Scout: 8.53 | Guard: 6.69 | Scout Captures: 333 | Scout Collections: 473 | Guard Captures: 238 | Epsilon (S/G): 0.362/0.256\n",
      "Learning rates - Scout: 0.000007 | Guard: 0.000349\n",
      "Ep 1900/20000 | Time: 919.8s | Avg Reward: -2.40 | Scout: 2.53 | Guard: -1.00 | Scout Captures: 350 | Scout Collections: 492 | Guard Captures: 251 | Epsilon (S/G): 0.360/0.253\n",
      "Learning rates - Scout: 0.000013 | Guard: 0.000132\n",
      "Stored historical models at episode 2000\n",
      "Ep 2000/20000 | Time: 983.3s | Avg Reward: -0.38 | Scout: -0.69 | Guard: 1.98 | Scout Captures: 370 | Scout Collections: 518 | Guard Captures: 267 | Epsilon (S/G): 0.358/0.251\n",
      "Learning rates - Scout: 0.000025 | Guard: 0.000031\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: 4.05 | Guard: 31.65\n",
      "New best Guard model! Reward: 31.65\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Best model saved!\n",
      "Safetensors model saved to ./model5/model.safetensors\n",
      "Ep 2100/20000 | Time: 1046.6s | Avg Reward: -1.09 | Scout: -11.88 | Guard: 7.39 | Scout Captures: 395 | Scout Collections: 544 | Guard Captures: 286 | Epsilon (S/G): 0.356/0.248\n",
      "Learning rates - Scout: 0.000036 | Guard: 0.000023\n",
      "Ep 2200/20000 | Time: 1101.5s | Avg Reward: 0.50 | Scout: 12.25 | Guard: -5.43 | Scout Captures: 405 | Scout Collections: 560 | Guard Captures: 300 | Epsilon (S/G): 0.355/0.246\n",
      "Learning rates - Scout: 0.000044 | Guard: 0.000043\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: 4.00 | Guard: 0.08\n",
      "Ep 2300/20000 | Time: 1171.8s | Avg Reward: 2.26 | Scout: 10.65 | Guard: -10.36 | Scout Captures: 418 | Scout Collections: 586 | Guard Captures: 307 | Epsilon (S/G): 0.353/0.244\n",
      "Learning rates - Scout: 0.000049 | Guard: 0.000077\n",
      "Ep 2400/20000 | Time: 1240.2s | Avg Reward: -0.19 | Scout: 2.00 | Guard: 9.66 | Scout Captures: 434 | Scout Collections: 609 | Guard Captures: 327 | Epsilon (S/G): 0.351/0.241\n",
      "Learning rates - Scout: 0.000049 | Guard: 0.000119\n",
      "Stored historical models at episode 2500\n",
      "Ep 2500/20000 | Time: 1309.7s | Avg Reward: 2.04 | Scout: 10.23 | Guard: -0.29 | Scout Captures: 447 | Scout Collections: 628 | Guard Captures: 342 | Epsilon (S/G): 0.349/0.239\n",
      "Learning rates - Scout: 0.000042 | Guard: 0.000155\n",
      "\n",
      "----- Running Evaluation -----\n",
      "Evaluating Guard...\n",
      "Evaluating Scout...\n",
      "Evaluation results - Scout: -0.16 | Guard: 3.15\n",
      "Safetensors model saved to ./model5/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# Attention Layer for enhanced feature focus\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, feature_dim),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure x is 2D for attention\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        weights = self.attention(x)\n",
    "        return x * weights\n",
    "\n",
    "# Enhanced DQN architecture with attention and residual connections\n",
    "class EnhancedDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(EnhancedDQN, self).__init__()\n",
    "        \n",
    "        # Attention mechanism for focusing on important state features\n",
    "        self.attention = AttentionLayer(state_dim)\n",
    "        \n",
    "        # First feature extraction layer\n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.LayerNorm(256),  # LayerNorm for better stability\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        )\n",
    "        \n",
    "        # Second feature extraction layer with residual connection\n",
    "        self.feature2 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better performance\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply attention mechanism\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Feature extraction with residual connection\n",
    "        f1 = self.feature1(x)\n",
    "        f2 = self.feature2(f1)\n",
    "        features = f1 + f2  # Residual connection\n",
    "        \n",
    "        # Value and advantage streams\n",
    "        value = self.value(features)\n",
    "        adv = self.advantage(features)\n",
    "        \n",
    "        # Dueling architecture combination\n",
    "        return value + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "# Enhanced Prioritized Replay Buffer with better prioritization\n",
    "class StabilizedReplayBuffer:\n",
    "    def __init__(self, capacity=100000, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        # Separate buffers for scout and guard\n",
    "        self.buffers = {\n",
    "            0: {'data': [], 'priorities': []},  # Guard\n",
    "            1: {'data': [], 'priorities': []}   # Scout\n",
    "        }\n",
    "        self.pos = {0: 0, 1: 0}\n",
    "        self.alpha = alpha\n",
    "        # Beta annealing for importance sampling\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame_idx = 0\n",
    "        \n",
    "    def get_beta(self):\n",
    "        \"\"\"Calculate current beta value for importance sampling with slower annealing\"\"\"\n",
    "        fraction = min(self.frame_idx / self.beta_frames, 1.0)\n",
    "        # Slower annealing curve\n",
    "        return min(self.beta_start + (fraction**0.75) * (1.0 - self.beta_start), 1.0)\n",
    "        \n",
    "    def push(self, s, a, r, ns, d, role):\n",
    "        \"\"\"Add experience to the buffer with improved priority assignment\"\"\"\n",
    "        self.frame_idx += 1\n",
    "        buffer = self.buffers[role]\n",
    "        \n",
    "        # Use max priority for new samples or a default value\n",
    "        max_prio = max(buffer['priorities'], default=1.0)\n",
    "        \n",
    "        # Boost priority for important experiences\n",
    "        if abs(r) > 5.0:  # High reward or penalty\n",
    "            max_prio = max_prio * 1.5\n",
    "        \n",
    "        data = (s, a, r, ns, d)\n",
    "        \n",
    "        if len(buffer['data']) < self.capacity:\n",
    "            buffer['data'].append(data)\n",
    "            buffer['priorities'].append(max_prio)\n",
    "        else:\n",
    "            buffer['data'][self.pos[role]] = data\n",
    "            buffer['priorities'][self.pos[role]] = max_prio\n",
    "            \n",
    "        self.pos[role] = (self.pos[role] + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size, role):\n",
    "        \"\"\"Sample with prioritization and proper beta scheduling\"\"\"\n",
    "        buffer = self.buffers[role]\n",
    "        if len(buffer['data']) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Get current beta value\n",
    "        beta = self.get_beta()\n",
    "            \n",
    "        # Calculate sampling probabilities with smoothing\n",
    "        probs = np.array(buffer['priorities']) ** self.alpha\n",
    "        probs = np.clip(probs, 1e-5, 1e5)  # Prevent numerical instabilities\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Sample with priority\n",
    "        indices = np.random.choice(len(buffer['data']), batch_size, p=probs)\n",
    "        samples = [buffer['data'][i] for i in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = (len(buffer['data']) * probs[indices]) ** -beta\n",
    "        weights /= weights.max()  # Normalize weights\n",
    "        \n",
    "        s, a, r, ns, d = zip(*samples)\n",
    "        return (np.array(s), np.array(a), np.array(r), np.array(ns),\n",
    "                np.array(d), weights, indices)\n",
    "                \n",
    "    def update_priorities(self, indices, priorities, role):\n",
    "        \"\"\"Update priorities with more sophisticated boosting for high rewards\"\"\"\n",
    "        # Clip priorities to prevent extreme values\n",
    "        priorities = np.clip(priorities, 0.01, 15.0)  # Higher upper clip as recommended\n",
    "        \n",
    "        # Boost priorities for experiences with high rewards\n",
    "        for i, p, idx in zip(range(len(indices)), priorities, indices):\n",
    "            # Extract the reward from the experience\n",
    "            _, _, r, _, _ = self.buffers[role]['data'][idx]\n",
    "            \n",
    "            # Apply boosting based on reward magnitude\n",
    "            if abs(r) > 10.0:  # Very important experience\n",
    "                p = p * 1.5\n",
    "            elif abs(r) > 5.0:  # Important experience\n",
    "                p = p * 1.2\n",
    "            elif abs(r) > 1.0:  # Somewhat important\n",
    "                p = p * 1.1\n",
    "            \n",
    "            # Additional boost for scout completing missions\n",
    "            if role == 1 and r > 4.5:  # Scout completing missions (reward ~5)\n",
    "                p = p * 1.3\n",
    "                \n",
    "            self.buffers[role]['priorities'][idx] = p\n",
    "            \n",
    "    def __len__(self, role):\n",
    "        return len(self.buffers[role]['data'])\n",
    "\n",
    "# Improved DQN Agent with recommended enhancements\n",
    "class RobustDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Role-specific configurations with refined parameters\n",
    "        self.configs = {\n",
    "            0: {  # Guard\n",
    "                \"lr\": 2e-4,  # Higher initial learning rate as recommended\n",
    "                \"update_freq\": 250,\n",
    "                \"grad_steps\": 1,\n",
    "                \"target_update_freq\": 1000  # Less frequent target updates for stability\n",
    "            },\n",
    "            1: {  # Scout\n",
    "                \"lr\": 5e-5,  # Lower for more stability\n",
    "                \"update_freq\": 200,\n",
    "                \"grad_steps\": 2,  # More gradient steps for scout\n",
    "                \"target_update_freq\": 800  # More frequent target updates for scout\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Build separate agents for scout and guard\n",
    "        self.agents = {\n",
    "            0: self._build_agent(state_dim, action_dim, self.configs[0]),  # Guard\n",
    "            1: self._build_agent(state_dim, action_dim, self.configs[1])   # Scout\n",
    "        }\n",
    "        \n",
    "        # Exploration parameters with slower decay as recommended\n",
    "        self.epsilon = {\n",
    "            0: 0.3,  # Start with higher exploration\n",
    "            1: 0.4   # Even higher for scout\n",
    "        }\n",
    "        self.epsilon_decay = {\n",
    "            0: 0.9998,  # Slower decay for guard\n",
    "            1: 0.9999   # Even slower decay for scout\n",
    "        }\n",
    "        self.epsilon_final = {\n",
    "            0: 0.08,    # Higher final epsilon\n",
    "            1: 0.12     # Even higher for scout\n",
    "        }\n",
    "        \n",
    "        # Training counters\n",
    "        self.update_counter = {0: 0, 1: 0}\n",
    "        \n",
    "        # Historical models for self-play (maintain this feature from original code)\n",
    "        self.historical_models = {\n",
    "            0: [],  # Guard history\n",
    "            1: []   # Scout history\n",
    "        }\n",
    "        self.historical_model_episodes = []\n",
    "        \n",
    "        # Parameter noise for exploration\n",
    "        self.param_noise_std = {0: 0.01, 1: 0.02}\n",
    "        self.param_noise_decay = 0.9995\n",
    "        \n",
    "    def _build_agent(self, state_dim, action_dim, config):\n",
    "        \"\"\"Build agent components with improved architecture\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = EnhancedDQN(state_dim, action_dim).to(device)\n",
    "        target = EnhancedDQN(state_dim, action_dim).to(device)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "        \n",
    "        # Freeze target network parameters\n",
    "        for param in target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Use Adam with improved parameters\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=config[\"lr\"], \n",
    "            eps=1e-5,  # For numerical stability\n",
    "            weight_decay=1e-5  # Light L2 regularization\n",
    "        )\n",
    "        \n",
    "        # Add learning rate scheduler for better convergence\n",
    "        # Use cosine annealing for smoother learning rate decay\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, \n",
    "            T_max=10000,  # Longer cycle\n",
    "            eta_min=config[\"lr\"] / 10  # Don't reduce LR too much\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"model\": model, \n",
    "            \"target\": target, \n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler,\n",
    "            \"device\": device,\n",
    "            \"config\": config\n",
    "        }\n",
    "    \n",
    "    # Store historical models for self-play\n",
    "    def store_historical_model(self, episode):\n",
    "        \"\"\"Store a snapshot of current models for later self-play\"\"\"\n",
    "        guard_model = copy.deepcopy(self.agents[0][\"model\"].state_dict())\n",
    "        scout_model = copy.deepcopy(self.agents[1][\"model\"].state_dict())\n",
    "        \n",
    "        self.historical_models[0].append(guard_model)\n",
    "        self.historical_models[1].append(scout_model)\n",
    "        self.historical_model_episodes.append(episode)\n",
    "        \n",
    "        # Keep only last 5 historical models to save memory\n",
    "        if len(self.historical_models[0]) > 5:\n",
    "            self.historical_models[0].pop(0)\n",
    "            self.historical_models[1].pop(0)\n",
    "            self.historical_model_episodes.pop(0)\n",
    "        \n",
    "        print(f\"Stored historical models at episode {episode}\")\n",
    "    \n",
    "    def add_parameter_noise(self, role):\n",
    "        \"\"\"Add noise to model parameters for better exploration\"\"\"\n",
    "        if self.param_noise_std[role] > 0.001:  # Only add noise if it's significant\n",
    "            for param in self.agents[role][\"model\"].parameters():\n",
    "                noise = torch.randn_like(param.data) * self.param_noise_std[role]\n",
    "                param.data += noise\n",
    "            \n",
    "            # Decay noise standard deviation\n",
    "            self.param_noise_std[role] *= self.param_noise_decay\n",
    "    \n",
    "    # Act using either current or historical model\n",
    "    def act(self, state, role, evaluation=False, use_historical=False, historical_idx=None):\n",
    "        \"\"\"Select action using epsilon-greedy with optional historical model\"\"\"\n",
    "        epsilon = 0.01 if evaluation else self.epsilon[role]  # Use minimal exploration in eval mode\n",
    "        device = self.agents[role][\"device\"]\n",
    "        \n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 4)\n",
    "        \n",
    "        # Convert state to tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use historical model if specified\n",
    "        if use_historical and historical_idx is not None and len(self.historical_models[role]) > historical_idx:\n",
    "            # Create temporary model with historical weights\n",
    "            # Access the dimensions correctly from the model itself\n",
    "            in_features = self.agents[role][\"model\"].feature1[0].in_features\n",
    "            out_features = self.agents[role][\"model\"].advantage[-1].out_features\n",
    "            \n",
    "            temp_model = EnhancedDQN(in_features, out_features).to(device)\n",
    "            temp_model.load_state_dict(self.historical_models[role][historical_idx])\n",
    "            temp_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = temp_model(state)\n",
    "                action = q_values.argmax().item()\n",
    "                \n",
    "            return action\n",
    "        \n",
    "        # Otherwise use current model\n",
    "        self.agents[role][\"model\"].eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.agents[role][\"model\"](state)\n",
    "            action = q_values.argmax().item()\n",
    "        self.agents[role][\"model\"].train()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def decay_epsilon(self, role):\n",
    "        \"\"\"Decay epsilon according to recommended slower schedule\"\"\"\n",
    "        self.epsilon[role] = max(\n",
    "            self.epsilon_final[role],\n",
    "            self.epsilon[role] * self.epsilon_decay[role]\n",
    "        )\n",
    "            \n",
    "    def update(self, buffer, role, batch_size=64):\n",
    "        \"\"\"Update the agent with improved training stability measures\"\"\"\n",
    "        result = buffer.sample(batch_size, role)\n",
    "        if result is None:\n",
    "            return\n",
    "            \n",
    "        s, a, r, ns, d, w, idx = result\n",
    "        agent = self.agents[role]\n",
    "        device = agent[\"device\"]\n",
    "        config = agent[\"config\"]\n",
    "        \n",
    "        # Convert numpy arrays to tensors\n",
    "        s_t = torch.tensor(s, dtype=torch.float32).to(device)\n",
    "        ns_t = torch.tensor(ns, dtype=torch.float32).to(device)\n",
    "        a_t = torch.tensor(a, dtype=torch.long).unsqueeze(1).to(device)\n",
    "        r_t = torch.tensor(r, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        d_t = torch.tensor(d, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        w_t = torch.tensor(w, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Ensure model is in training mode for batch updates\n",
    "        agent[\"model\"].train()\n",
    "        agent[\"target\"].eval()\n",
    "        \n",
    "        loss_sum = 0\n",
    "        \n",
    "        # Update step with multiple gradient accumulations for stability\n",
    "        for _ in range(config[\"grad_steps\"]):\n",
    "            # Calculate current Q values\n",
    "            q_vals = agent[\"model\"](s_t).gather(1, a_t)\n",
    "            \n",
    "            # Double DQN update - reduces overestimation bias\n",
    "            with torch.no_grad():\n",
    "                # Select actions using online network\n",
    "                next_actions = agent[\"model\"](ns_t).argmax(1, keepdim=True)\n",
    "                # Evaluate Q-values using target network\n",
    "                next_q = agent[\"target\"](ns_t).gather(1, next_actions)\n",
    "                # Calculate expected Q values\n",
    "                expected = r_t + self.gamma * next_q * (1 - d_t)\n",
    "            \n",
    "            # Use Huber loss for more stable updates\n",
    "            loss = F.smooth_l1_loss(q_vals, expected, reduction='none')\n",
    "            weighted_loss = (w_t * loss).mean()\n",
    "            \n",
    "            loss_sum += weighted_loss.item()\n",
    "            \n",
    "            # Calculate TD errors for priority update\n",
    "            with torch.no_grad():\n",
    "                td_err = (q_vals - expected).abs().detach().cpu().numpy().flatten()\n",
    "            \n",
    "            # Optimization step\n",
    "            agent[\"optimizer\"].zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(agent[\"model\"].parameters(), max_norm=10.0)\n",
    "            \n",
    "            agent[\"optimizer\"].step()\n",
    "        \n",
    "        # Update priorities in the replay buffer\n",
    "        buffer.update_priorities(idx, td_err, role)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        agent[\"scheduler\"].step()\n",
    "        \n",
    "        # Target network update with role-specific frequency\n",
    "        self.update_counter[role] += 1\n",
    "        if self.update_counter[role] % config[\"target_update_freq\"] == 0:\n",
    "            agent[\"target\"].load_state_dict(agent[\"model\"].state_dict())\n",
    "            \n",
    "        return loss_sum / config[\"grad_steps\"]\n",
    "    \n",
    "    def save(self, path_prefix=\"./model5/\"):\n",
    "        \"\"\"Save both scout and guard models using safetensors.\"\"\"\n",
    "        try:\n",
    "            from safetensors.torch import save_file\n",
    "            os.makedirs(path_prefix, exist_ok=True)\n",
    "            flattened = {}\n",
    "            for role_name, role_id in [(\"guard\", 0), (\"scout\", 1)]:\n",
    "                for part in [\"model\", \"target\"]:\n",
    "                    state_dict = self.agents[role_id][part].state_dict()\n",
    "                    for key, tensor in state_dict.items():\n",
    "                        if isinstance(tensor, torch.Tensor):\n",
    "                            flattened[f\"{role_name}_{part}.{key}\"] = tensor\n",
    "            \n",
    "            save_file(flattened, os.path.join(path_prefix, \"model.safetensors\"))\n",
    "            print(f\"Safetensors model saved to {path_prefix}model.safetensors\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "            \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model from safetensors file\"\"\"\n",
    "        try:\n",
    "            from safetensors.torch import load_file\n",
    "            loaded = load_file(path)\n",
    "            \n",
    "            # Extract and load the model weights\n",
    "            for role_name, role_id in [(\"guard\", 0), (\"scout\", 1)]:\n",
    "                for part in [\"model\", \"target\"]:\n",
    "                    model = self.agents[role_id][part]\n",
    "                    state_dict = model.state_dict()\n",
    "                    \n",
    "                    # Update state dict with loaded weights\n",
    "                    for key in state_dict:\n",
    "                        loaded_key = f\"{role_name}_{part}.{key}\"\n",
    "                        if loaded_key in loaded:\n",
    "                            state_dict[key] = loaded[loaded_key]\n",
    "                    \n",
    "                    # Load updated state dict\n",
    "                    model.load_state_dict(state_dict)\n",
    "            print(f\"Model loaded from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "\n",
    "# Enhanced flatten observation function with normalization\n",
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Convert observation to a flat vector with consistent size and better normalization.\n",
    "    \"\"\"\n",
    "    # Safety check for None observation\n",
    "    if obs is None:\n",
    "        print(\"Warning: Received None observation in preprocess_observation. Returning None.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Get viewcone and normalize to [0,1]\n",
    "        if 'viewcone' not in obs:\n",
    "            print(\"Warning: 'viewcone' not found in observation. Using zeros.\")\n",
    "            flat_view = np.zeros(35)  # Assuming 7x5 viewcone\n",
    "        else:\n",
    "            flat_view = np.array(obs['viewcone']).flatten() / 255.0\n",
    "        \n",
    "        # Direction as one-hot encoding\n",
    "        if 'direction' not in obs:\n",
    "            print(\"Warning: 'direction' not found in observation. Using zeros.\")\n",
    "            direction_onehot = np.zeros(4)\n",
    "        else:\n",
    "            direction_onehot = np.zeros(4)\n",
    "            direction_onehot[obs['direction']] = 1\n",
    "        \n",
    "        # Role indicator (scout or guard)\n",
    "        if 'scout' not in obs:\n",
    "            print(\"Warning: 'scout' not found in observation. Using default False.\")\n",
    "            is_scout = np.array([0])\n",
    "        else:\n",
    "            is_scout = np.array([obs['scout']])\n",
    "        \n",
    "        # Location normalized to [0,1]\n",
    "        if 'location' not in obs:\n",
    "            print(\"Warning: 'location' not found in observation. Using zeros.\")\n",
    "            location = np.zeros(2)\n",
    "        else:\n",
    "            location = np.array(obs['location']) / 15.0\n",
    "        \n",
    "        # Step count normalized\n",
    "        if 'step' not in obs:\n",
    "            print(\"Warning: 'step' not found in observation. Using zero.\")\n",
    "            step = np.array([0])\n",
    "        else:\n",
    "            step = np.array([obs['step'] / 100.0])\n",
    "        \n",
    "        # Combine all features\n",
    "        return np.concatenate([flat_view, direction_onehot, is_scout, location, step])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_observation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Improved reward shaping function with better gradients\n",
    "def compute_improved_reward(obs, reward, role, env, current_phase=0):\n",
    "    \"\"\"Shaped rewards with better learning signals and curriculum adaptation\"\"\"\n",
    "    # Safety check for None observation\n",
    "    if obs is None:\n",
    "        return reward\n",
    "        \n",
    "    agent_pos = obs['location']\n",
    "    \n",
    "    # Start with original reward\n",
    "    shaped_reward = reward\n",
    "    \n",
    "    if role == 1:  # scout\n",
    "        # Scout reward modifiers\n",
    "        \n",
    "        # 1. Make capture penalty less severe\n",
    "        if reward <= -40:  # Captured\n",
    "            shaped_reward = -10  # Less severe than before\n",
    "        \n",
    "        # 2. Increase proximity rewards\n",
    "        targets = env.recon_points.union(env.missions) if hasattr(env, 'recon_points') and hasattr(env, 'missions') else set()\n",
    "        if targets:\n",
    "            # Calculate distances to targets with mission prioritization\n",
    "            distances = []\n",
    "            for tx, ty in targets:\n",
    "                # Manhattan distance\n",
    "                dist = abs(agent_pos[0] - tx) + abs(agent_pos[1] - ty)\n",
    "                # Prioritize missions (higher reward targets)\n",
    "                is_mission = (tx, ty) in env.missions if hasattr(env, 'missions') else False\n",
    "                if is_mission:\n",
    "                    dist *= 0.8  # Make missions seem \"closer\" for reward calculation\n",
    "                distances.append(dist)\n",
    "                \n",
    "            min_dist = min(distances) if distances else 16\n",
    "            \n",
    "            # Stronger proximity reward with sigmoid curve for smoother transitions\n",
    "            proximity_reward = 1.0 / (1.0 + np.exp((min_dist - 3) / 2))\n",
    "            shaped_reward += proximity_reward\n",
    "            \n",
    "            # Progress reward - increase for getting closer\n",
    "            if hasattr(env, 'last_scout_pos') and env.last_scout_pos is not None:\n",
    "                old_distances = [abs(env.last_scout_pos[0] - tx) + abs(env.last_scout_pos[1] - ty) \n",
    "                               for tx, ty in targets]\n",
    "                old_min_dist = min(old_distances) if old_distances else 16\n",
    "                \n",
    "                if min_dist < old_min_dist:\n",
    "                    # Reward proportional to progress made\n",
    "                    progress = old_min_dist - min_dist\n",
    "                    shaped_reward += 0.3 * progress\n",
    "        \n",
    "        # Store position for next step\n",
    "        env.last_scout_pos = agent_pos.copy()\n",
    "        \n",
    "        # 3. Amplify collection rewards\n",
    "        if reward >= 4.5:  # Completed mission (original reward ~5)\n",
    "            shaped_reward *= 1.3\n",
    "        elif reward >= 0.8:  # Collected recon point (original reward ~1)\n",
    "            shaped_reward *= 1.2\n",
    "            \n",
    "    else:  # guard\n",
    "        # Add small positive reward just for being a guard to offset negative bias\n",
    "        shaped_reward += 0.1\n",
    "        \n",
    "        # Focus more on patrolling high-value areas\n",
    "        targets = env.missions if hasattr(env, 'missions') else set()\n",
    "        if targets:\n",
    "            distances = [abs(agent_pos[0] - tx) + abs(agent_pos[1] - ty) for tx, ty in targets]\n",
    "            min_dist = min(distances) if distances else 16\n",
    "            \n",
    "            # Stronger rewards for guards near targets\n",
    "            shaped_reward += 0.3 * np.exp(-0.2 * min_dist)\n",
    "        \n",
    "        # Reward guards for captures\n",
    "        if reward > 40:  # If there's a capture reward\n",
    "            shaped_reward *= 1.4  # Amplify it\n",
    "    \n",
    "    # Add time-dependent behavior\n",
    "    time_progress = obs['step'] / 100.0\n",
    "    if role == 1:  # Scout - incentivize acting quickly\n",
    "        time_factor = 1.0 - time_progress * 0.3  # Linear decay from 1.0 to 0.7\n",
    "        shaped_reward *= time_factor\n",
    "    else:  # Guard - be more aggressive later\n",
    "        if time_progress > 0.7:  # Last 30% of episode\n",
    "            shaped_reward *= (1.0 + 0.3 * time_progress)  # Up to 30% boost late in episode\n",
    "    \n",
    "    # Phase-specific reward adjustments\n",
    "    if current_phase >= 1:  # In later phases\n",
    "        # Add small exploration bonus to encourage trying new strategies\n",
    "        shaped_reward += 0.05 * random.random()\n",
    "    \n",
    "    return shaped_reward\n",
    "\n",
    "# Main training function with improved curriculum\n",
    "def train_with_curriculum(env_class, episodes=5000, save_interval=100):\n",
    "    \"\"\"Training with curriculum learning, improved stability, and self-play\"\"\"\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = env_class()\n",
    "    state_dim = 43  # Based on flatten_obs output\n",
    "    action_dim = 5\n",
    "    \n",
    "    # Create agent with optimized parameters\n",
    "    agent = RobustDQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    \n",
    "    # Use replay buffer with proper prioritization\n",
    "    buffer = StabilizedReplayBuffer(capacity=100000)\n",
    "    \n",
    "    # Training tracking\n",
    "    rewards_window = {\n",
    "        'all': [],\n",
    "        'scout': [],\n",
    "        'guard': []\n",
    "    }\n",
    "    scout_captures = 0\n",
    "    scout_collections = 0\n",
    "    guard_captures = 0\n",
    "    \n",
    "    # Ensure environment has attributes we need\n",
    "    if not hasattr(env, 'last_scout_pos'):\n",
    "        env.last_scout_pos = None\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Modified curriculum learning phases with gentler transitions\n",
    "    curriculum = [\n",
    "        {'episodes': 1000, 'scout_ratio': 0.6, 'scout_lr': 5e-5, 'guard_lr': 2e-4},\n",
    "        {'episodes': 2000, 'scout_ratio': 0.5, 'scout_lr': 4.5e-5, 'guard_lr': 1.8e-4},\n",
    "        {'episodes': 2000, 'scout_ratio': 0.5, 'scout_lr': 4e-5, 'guard_lr': 1.5e-4},\n",
    "        {'episodes': 5000, 'scout_ratio': 0.5, 'scout_lr': 3.5e-5, 'guard_lr': 1.2e-4},\n",
    "        {'episodes': 10000, 'scout_ratio': 0.5, 'scout_lr': 3e-5, 'guard_lr': 1e-4}\n",
    "    ]\n",
    "    \n",
    "    # Track progress through curriculum\n",
    "    current_phase = 0\n",
    "    phase_progress = 0\n",
    "    phase_role_counts = {0: 0, 1: 0}\n",
    "    \n",
    "    # Initialize counter for role balancing\n",
    "    episodes_since_role = {0: 0, 1: 0}\n",
    "    \n",
    "    # Historical model snapshots\n",
    "    historical_snapshot_interval = 500\n",
    "    \n",
    "    # Optional: Enable periodic evaluation\n",
    "    evaluation_interval = 250\n",
    "    best_eval_reward = {0: -float('inf'), 1: -float('inf')}\n",
    "    \n",
    "    for ep in range(1, episodes+1):\n",
    "        # Update curriculum phase if needed\n",
    "        if current_phase < len(curriculum) - 1:\n",
    "            if phase_progress >= curriculum[current_phase]['episodes']:\n",
    "                current_phase += 1\n",
    "                phase_progress = 0\n",
    "                phase_role_counts = {0: 0, 1: 0}\n",
    "                print(f\"Moving to curriculum phase {current_phase+1}\")\n",
    "                \n",
    "                # Update learning rates\n",
    "                for role in [0, 1]:\n",
    "                    new_lr = curriculum[current_phase][f\"{'scout' if role==1 else 'guard'}_lr\"]\n",
    "                    for param_group in agent.agents[role][\"optimizer\"].param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "        \n",
    "        # Store historical models periodically\n",
    "        if ep % historical_snapshot_interval == 0:\n",
    "            agent.store_historical_model(ep)\n",
    "        \n",
    "        # Apply parameter noise periodically for enhanced exploration\n",
    "        if ep % 10 == 0:\n",
    "            for role in [0, 1]:\n",
    "                agent.add_parameter_noise(role)\n",
    "        \n",
    "        # Role selection logic for balanced training\n",
    "        scout_ratio = curriculum[current_phase]['scout_ratio']\n",
    "        expected_scout_count = phase_progress * scout_ratio\n",
    "        expected_guard_count = phase_progress * (1 - scout_ratio)\n",
    "        \n",
    "        # Force a scout episode if we're behind or it's been too long\n",
    "        if phase_role_counts[1] < expected_scout_count - 5 or episodes_since_role[1] > 15:\n",
    "            role = 1  # Scout\n",
    "            episodes_since_role[1] = 0\n",
    "            episodes_since_role[0] += 1\n",
    "        # Force a guard episode if we're behind or it's been too long\n",
    "        elif phase_role_counts[0] < expected_guard_count - 5 or episodes_since_role[0] > 15:\n",
    "            role = 0  # Guard\n",
    "            episodes_since_role[0] = 0\n",
    "            episodes_since_role[1] += 1\n",
    "        # Regular probability-based selection\n",
    "        else:\n",
    "            if random.random() < scout_ratio:\n",
    "                role = 1  # Scout\n",
    "                episodes_since_role[1] = 0\n",
    "                episodes_since_role[0] += 1\n",
    "            else:\n",
    "                role = 0  # Guard\n",
    "                episodes_since_role[0] = 0\n",
    "                episodes_since_role[1] += 1\n",
    "            \n",
    "        # Update phase tracking\n",
    "        phase_progress += 1\n",
    "        phase_role_counts[role] += 1\n",
    "        \n",
    "        # Set environment role\n",
    "        env.is_scout = (role == 1)\n",
    "        \n",
    "        # Use historical opponent occasionally for self-play\n",
    "        use_historical_opponent = (len(agent.historical_models[0]) > 0 and\n",
    "                                  random.random() < 0.2)  # 20% chance\n",
    "        historical_idx = random.randint(0, len(agent.historical_models[0])-1) if use_historical_opponent else None\n",
    "        \n",
    "        # Reset environment\n",
    "        obs = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Track recon/mission collection\n",
    "        initial_recon_count = len(env.recon_points) if hasattr(env, 'recon_points') else 0\n",
    "        initial_mission_count = len(env.missions) if hasattr(env, 'missions') else 0\n",
    "        \n",
    "        # Episode loop\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (potentially using historical model for opponent)\n",
    "            action = agent.act(state, role, use_historical=use_historical_opponent, historical_idx=historical_idx)\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            \n",
    "            # Shape reward for better learning signal\n",
    "            shaped_reward = compute_improved_reward(next_obs, reward, role, env, current_phase)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            buffer.push(state, action, shaped_reward, next_state, done, role)\n",
    "            \n",
    "            # Update agent with adaptive frequency\n",
    "            update_freq = 4  # Less frequent updates for better stability\n",
    "            if steps % update_freq == 0:\n",
    "                # Multiple updates per batch\n",
    "                for _ in range(2):\n",
    "                    loss = agent.update(buffer, role, batch_size=128)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Optional: Early stopping if episode is very long\n",
    "            if steps >= 150:  # Safeguard against very long episodes\n",
    "                done = True\n",
    "        \n",
    "        # Check for scout captures or collections\n",
    "        if role == 1:  # scout\n",
    "            if reward <= -40:  # Captured\n",
    "                scout_captures += 1\n",
    "            \n",
    "            # Check for collections\n",
    "            if hasattr(env, 'recon_points') and hasattr(env, 'missions'):\n",
    "                recon_collected = initial_recon_count - len(env.recon_points)\n",
    "                missions_completed = initial_mission_count - len(env.missions)\n",
    "                if recon_collected > 0 or missions_completed > 0:\n",
    "                    scout_collections += 1\n",
    "        else:  # guard\n",
    "            # Assume positive reward for guard might be capture\n",
    "            if reward > 40:\n",
    "                guard_captures += 1\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon(role)\n",
    "        \n",
    "        # Track rewards by role (using sliding window)\n",
    "        window_size = 50\n",
    "        if len(rewards_window['all']) >= window_size:\n",
    "            rewards_window['all'].pop(0)\n",
    "        rewards_window['all'].append(episode_reward)\n",
    "        \n",
    "        if role == 1:  # scout\n",
    "            if len(rewards_window['scout']) >= window_size:\n",
    "                rewards_window['scout'].pop(0)\n",
    "            rewards_window['scout'].append(episode_reward)\n",
    "        else:  # guard\n",
    "            if len(rewards_window['guard']) >= window_size:\n",
    "                rewards_window['guard'].pop(0)\n",
    "            rewards_window['guard'].append(episode_reward)\n",
    "        \n",
    "        # Log training progress periodically\n",
    "        if ep % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Calculate stats\n",
    "            avg_r = np.mean(rewards_window['all']) if rewards_window['all'] else 0\n",
    "            avg_scout = np.mean(rewards_window['scout']) if rewards_window['scout'] else 0\n",
    "            avg_guard = np.mean(rewards_window['guard']) if rewards_window['guard'] else 0\n",
    "            \n",
    "            # Log training status\n",
    "            print(f\"Ep {ep}/{episodes} | \"\n",
    "                  f\"Time: {elapsed:.1f}s | \"\n",
    "                  f\"Avg Reward: {avg_r:.2f} | \"\n",
    "                  f\"Scout: {avg_scout:.2f} | \"\n",
    "                  f\"Guard: {avg_guard:.2f} | \"\n",
    "                  f\"Scout Captures: {scout_captures} | \"\n",
    "                  f\"Scout Collections: {scout_collections} | \"\n",
    "                  f\"Guard Captures: {guard_captures} | \"\n",
    "                  f\"Epsilon (S/G): {agent.epsilon[1]:.3f}/{agent.epsilon[0]:.3f}\")\n",
    "            \n",
    "            # Print current learning rates\n",
    "            print(f\"Learning rates - Scout: {agent.agents[1]['optimizer'].param_groups[0]['lr']:.6f} | \"\n",
    "                  f\"Guard: {agent.agents[0]['optimizer'].param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Periodic evaluation\n",
    "        if ep % evaluation_interval == 0:\n",
    "            eval_rewards = {0: [], 1: []}\n",
    "            \n",
    "            # Run evaluation episodes\n",
    "            print(\"\\n----- Running Evaluation -----\")\n",
    "            for eval_role in [0, 1]:\n",
    "                role_name = \"Scout\" if eval_role == 1 else \"Guard\"\n",
    "                print(f\"Evaluating {role_name}...\")\n",
    "                \n",
    "                for _ in range(10):  # 10 episodes per role\n",
    "                    env.is_scout = (eval_role == 1)\n",
    "                    eval_obs = env.reset()\n",
    "                    eval_state = preprocess_observation(eval_obs)\n",
    "                    eval_episode_reward = 0\n",
    "                    eval_done = False\n",
    "                    eval_steps = 0\n",
    "                    \n",
    "                    while not eval_done and eval_steps < 150:\n",
    "                        # Use evaluation mode (minimal exploration)\n",
    "                        eval_action = agent.act(eval_state, eval_role, evaluation=True)\n",
    "                        eval_next_obs, eval_reward, eval_done = env.step(eval_action)\n",
    "                        eval_next_state = preprocess_observation(eval_next_obs)\n",
    "                        eval_episode_reward += eval_reward\n",
    "                        eval_state = eval_next_state\n",
    "                        eval_steps += 1\n",
    "                    \n",
    "                    eval_rewards[eval_role].append(eval_episode_reward)\n",
    "            \n",
    "            # Calculate average evaluation rewards\n",
    "            avg_eval_scout = np.mean(eval_rewards[1]) if eval_rewards[1] else 0\n",
    "            avg_eval_guard = np.mean(eval_rewards[0]) if eval_rewards[0] else 0\n",
    "            \n",
    "            print(f\"Evaluation results - Scout: {avg_eval_scout:.2f} | Guard: {avg_eval_guard:.2f}\")\n",
    "            \n",
    "            # Save models if they're better than previous best\n",
    "            improved = False\n",
    "            \n",
    "            if avg_eval_scout > best_eval_reward[1]:\n",
    "                best_eval_reward[1] = avg_eval_scout\n",
    "                improved = True\n",
    "                print(f\"New best Scout model! Reward: {avg_eval_scout:.2f}\")\n",
    "            \n",
    "            if avg_eval_guard > best_eval_reward[0]:\n",
    "                best_eval_reward[0] = avg_eval_guard\n",
    "                improved = True\n",
    "                print(f\"New best Guard model! Reward: {avg_eval_guard:.2f}\")\n",
    "            \n",
    "            if improved:\n",
    "                agent.save(\"./model5/\")\n",
    "                print(\"Best model saved!\")\n",
    "        \n",
    "        # Regular model saving\n",
    "        if ep % save_interval == 0:\n",
    "            agent.save()\n",
    "    \n",
    "    # Final save\n",
    "    agent.save(\"./model5/\")\n",
    "    print(f\"Training complete! Scout episodes: {phase_role_counts[1]}, \"\n",
    "          f\"Guard episodes: {phase_role_counts[0]}\")\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Start improved training\n",
    "    agent = train_with_curriculum(TILAIEnv, episodes=20000, save_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cda1e7-e5b1-4875-9d97-7a27f7e9d0b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "harikernel",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Hari Training Kernel",
   "language": "python",
   "name": "harikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
