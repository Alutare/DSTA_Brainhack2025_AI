{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ee73d-6c3a-4073-91e4-16f7d8c50690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.1.0 torchvision==0.16.0\n",
    "!pip install gym==0.26.2 numpy==1.23.5 matplotlib==3.7.1\n",
    "!pip install fastapi==0.105.0 uvicorn==0.24.0.post1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "97015299-874a-48c1-98ec-93e26d8b640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ab8566d-4410-495d-9254-aa54fffc9cba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TILAIEnv: #this might or might not be compatible with the testing environment???\n",
    "    def __init__(self, seed=None): # Chat suggested this seed nonsense might help\n",
    "        self.grid_size = 16\n",
    "        self.max_steps = 100\n",
    "        self.obstacles = {(random.randint(0, 15), random.randint(0, 15)) for _ in range(30)} # add obstacles\n",
    "        self.reset(seed=seed) # to reset the episode\n",
    "\n",
    "    def reset(self, seed=None): # resetting the whole episode\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.is_scout = bool(random.randint(0, 1)) # scout is 1 and then guard is 0\n",
    "        self.direction = random.randint(0, 3) # 0 for right, 1 down, 2 left and 3 up from the repo\n",
    "\n",
    "        while True:\n",
    "            self.location = [random.randint(0, 15), random.randint(0, 15)] # random starting position\n",
    "            if tuple(self.location) not in self.obstacles:\n",
    "                break\n",
    "\n",
    "        self.recon_points = {(random.randint(0, 15), random.randint(0, 15)) for _ in range(100)}\n",
    "        self.missions = {(random.randint(0, 15), random.randint(0, 15)) for _ in range(20)}\n",
    "        self.visited = set()\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self): # the viewcone needs to be 7 * 5\n",
    "        viewcone = np.zeros((7, 5), dtype=np.uint8)\n",
    "        return {\n",
    "            \"viewcone\": viewcone,\n",
    "            \"direction\": self.direction,\n",
    "            \"location\": self.location,\n",
    "            \"scout\": int(self.is_scout),\n",
    "            \"step\": self.step_count\n",
    "        } # pack into a dicttttttt\n",
    "\n",
    "    def _min_manhattan_distance(self):\n",
    "        targets = self.recon_points if self.is_scout else self.missions # this is to do mission if its scout, if guard ignore\n",
    "        if not targets:\n",
    "            return 0 \n",
    "        return min(abs(self.location[0] - tx) + abs(self.location[1] - ty) for tx, ty in targets)\n",
    "\n",
    "    def step(self, action):# make the thing move\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True\n",
    "\n",
    "        self.step_count += 1\n",
    "        dx, dy = 0, 0\n",
    "        if action == 0:\n",
    "            dx, dy = self._move_vector(self.direction) # move forward\n",
    "        elif action == 1:\n",
    "            dx, dy = self._move_vector((self.direction + 2) % 4) # move backword\n",
    "        elif action == 2:\n",
    "            self.direction = (self.direction - 1) % 4 #turn left\n",
    "        elif action == 3:\n",
    "            self.direction = (self.direction + 1) % 4 # turn right\n",
    "\n",
    "        new_location = [np.clip(self.location[0] + dx, 0, 15), np.clip(self.location[1] + dy, 0, 15)]\n",
    "        if tuple(new_location) not in self.obstacles:\n",
    "            self.location = new_location\n",
    "        else:\n",
    "            # print(f\"Step {self.step_count}: Blocked by obstacle at {tuple(new_location)}\")\n",
    "            pass\n",
    "\n",
    "        loc_tuple = tuple(self.location)\n",
    "        reward = -0.01 # Maybe up this?? Maybe more incentive to move?\n",
    "\n",
    "        if loc_tuple in self.visited:\n",
    "            reward -= 0.01 #Penalise revisiting also\n",
    "        else:\n",
    "            self.visited.add(loc_tuple)\n",
    "\n",
    "        if self.is_scout:\n",
    "            if loc_tuple in self.recon_points:\n",
    "                reward += 5\n",
    "                self.recon_points.remove(loc_tuple)\n",
    "            elif loc_tuple in self.missions:\n",
    "                reward += 20\n",
    "                self.missions.remove(loc_tuple)\n",
    "            if random.random() < 0.01:\n",
    "                reward -= 10\n",
    "                self.done = True\n",
    "        else:\n",
    "            if random.random() < 0.01:\n",
    "                reward += 30\n",
    "                self.done = True\n",
    "\n",
    "        reward += 0.01 * (1 / (1 + self._min_manhattan_distance()))\n",
    "        if self.step_count >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_obs(), reward, self.done\n",
    "\n",
    "    def _move_vector(self, direction):\n",
    "        return [(1, 0), (0, 1), (-1, 0), (0, -1)][direction]\n",
    "\n",
    "    def render(self): # print the grid\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        for ox, oy in self.obstacles:\n",
    "            grid[oy][ox] = 'X'\n",
    "        x, y = self.location\n",
    "        grid[y][x] = 'S' if self.is_scout else 'G'\n",
    "        # print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        # print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "557841a2-6b2f-4664-8429-897400f109ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.feature = nn.Sequential(nn.Linear(input_dim, 256), nn.ReLU())\n",
    "        self.value = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        self.advantage = nn.Sequential(nn.Linear(256, 128), nn.ReLU(), nn.Linear(128, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value(x)\n",
    "        adv = self.advantage(x)\n",
    "        return value + adv - adv.mean(dim=1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ea67ecb-4e06-4361-b939-5c25dd20af87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer: #needed to sample more important experiences more frequently\n",
    "    def __init__(self, capacity=100000, alpha=0.6): #capacity is the number of exp it holds and alpha is how much of priority those experiences are\n",
    "        self.capacity = capacity\n",
    "        self.buffer, self.priorities = [], []\n",
    "        self.pos, self.alpha = 0, alpha\n",
    "\n",
    "    def push(self, s, a, r, ns, d, role): #push the experience\n",
    "        max_prio = max(self.priorities, default=1.0) # the newest one most priority so it can be looked at asap\n",
    "        data = (s, a, r, ns, d, role)\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(data)\n",
    "            self.priorities.append(max_prio)\n",
    "        else: #this is if it is full to overwrite the oldest data\n",
    "            self.buffer[self.pos] = data\n",
    "            self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity #increment in a circular fashion\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4): #0.4 is it too low?\n",
    "        probs = np.array(self.priorities) ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[i] for i in indices]\n",
    "        weights = ((len(self.buffer) * probs[indices]) ** -beta).astype(np.float32) #compute the importance of a sample\n",
    "        weights /= weights.max()\n",
    "        s, a, r, ns, d, roles = zip(*samples)\n",
    "        return (np.array(s), np.array(a), np.array(r), np.array(ns),\n",
    "                np.array(d), np.array(roles), weights, indices)\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for i, p in zip(indices, priorities):\n",
    "            self.priorities[i] = p\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85b91a65-3820-4efe-86e4-c4ecf476456c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DQNAgent:# manages the two roles\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99): #how big should the neural network be?\n",
    "        self.gamma = gamma\n",
    "        self.agents = {\n",
    "            0: self._build_agent(state_dim, action_dim),\n",
    "            1: self._build_agent(state_dim, action_dim)\n",
    "        }\n",
    "\n",
    "    def _build_agent(self, state_dim, action_dim):\n",
    "        model = DQN(state_dim, action_dim).cuda()\n",
    "        target = DQN(state_dim, action_dim).cuda()\n",
    "        target.load_state_dict(model.state_dict())\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        return {\"model\": model, \"target\": target, \"optimizer\": optim}\n",
    "\n",
    "    def act(self, state, role, epsilon=0.1): # should episolon be higher so more exploring\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 4)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            return self.agents[role][\"model\"](state).argmax().item()\n",
    "\n",
    "    def update(self, buffer, batch_size=64, beta=0.4):\n",
    "        if len(buffer) < batch_size:\n",
    "            return\n",
    "        s, a, r, ns, d, roles, w, idx = buffer.sample(batch_size, beta)\n",
    "        for role in [0, 1]:\n",
    "            ri = [i for i, rl in enumerate(roles) if rl == role] # filter by the roles\n",
    "            if not ri: continue\n",
    "\n",
    "            agent = self.agents[role] # use the role specific agent\n",
    "            s_t = torch.tensor(s[ri], dtype=torch.float32).cuda()\n",
    "            ns_t = torch.tensor(ns[ri], dtype=torch.float32).cuda()\n",
    "            a_t = torch.tensor(a)[ri].long().unsqueeze(1).cuda()\n",
    "            r_t = torch.tensor(r)[ri].unsqueeze(1).cuda()\n",
    "            d_t = torch.tensor(d)[ri].unsqueeze(1).float().cuda()\n",
    "            w_t = torch.tensor(w)[ri].unsqueeze(1).cuda()\n",
    "\n",
    "            q_vals = agent[\"model\"](s_t).gather(1, a_t)\n",
    "            next_actions = agent[\"model\"](ns_t).argmax(1, keepdim=True)\n",
    "            next_q = agent[\"target\"](ns_t).gather(1, next_actions)\n",
    "            expected = r_t + self.gamma * next_q * (1 - d_t)\n",
    "            loss = (w_t * (q_vals - expected.detach()).pow(2)).mean()\n",
    "\n",
    "            td_err = (q_vals - expected.detach()).abs().detach().cpu().numpy().flatten()\n",
    "            buffer.update_priorities([idx[i] for i in ri], td_err)\n",
    "\n",
    "            agent[\"optimizer\"].zero_grad()\n",
    "            loss.backward()\n",
    "            agent[\"optimizer\"].step()\n",
    "\n",
    "            if random.random() < 0.01:\n",
    "                agent[\"target\"].load_state_dict(agent[\"model\"].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7345d-3c02-4c89-82d5-7f372a9790f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "visited_counts = defaultdict(int)\n",
    "reward_min, reward_max = float('inf'), float('-inf')\n",
    "last_position = {0: None, 1: None}\n",
    "\n",
    "def flatten_obs(obs):\n",
    "    flat_view = np.array(obs['viewcone']).flatten() / 255.0\n",
    "    return flat_view\n",
    "\n",
    "def get_state_key(obs):\n",
    "    return tuple(obs['location'])\n",
    "\n",
    "def normalize_reward(r):\n",
    "    global reward_min, reward_max\n",
    "    reward_min = min(reward_min, r)\n",
    "    reward_max = max(reward_max, r)\n",
    "    if reward_max == reward_min:\n",
    "        return 0.0  # return neutral if no variation yet\n",
    "    return (r - reward_min) / (reward_max - reward_min) * 2 - 1\n",
    "\n",
    "def compute_shaped_reward(obs, reward, role):\n",
    "    state_key = get_state_key(obs)\n",
    "    visited_counts[state_key] += 1\n",
    "\n",
    "    rep_penalty = -0.01 * (visited_counts[state_key] - 1)\n",
    "    idle_penalty = -0.02 if last_position[role] == obs['location'] else 0.0\n",
    "    last_position[role] = obs['location']\n",
    "\n",
    "    # i think closer means better\n",
    "    agent_pos = obs['location']\n",
    "    if role == 1:  # scout\n",
    "        targets = env.recon_points or env.missions\n",
    "    else:\n",
    "        targets = env.missions\n",
    "    if targets:\n",
    "        min_dist = min(abs(agent_pos[0] - tx) + abs(agent_pos[1] - ty) for tx, ty in targets)\n",
    "        dist_bonus = 0.01 * (1 / (1 + min_dist))\n",
    "    else:\n",
    "        dist_bonus = 0.0\n",
    "\n",
    "    shaped = reward + rep_penalty + idle_penalty + dist_bonus\n",
    "    return normalize_reward(shaped)\n",
    "\n",
    "env = TILAIEnv()\n",
    "state_dim = 40\n",
    "action_dim = 5\n",
    "agent = DQNAgent(state_dim, action_dim)\n",
    "buffer = PrioritizedReplayBuffer()\n",
    "\n",
    "epsilon = 1.0\n",
    "final_epsilon = 0.05\n",
    "decay_steps = 10000\n",
    "decay_rate = (epsilon - final_epsilon) / decay_steps\n",
    "episodes = 5000\n",
    "all_rewards = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    obs = env.reset()\n",
    "    role = obs['scout']\n",
    "    state = flatten_obs(obs)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    visited_counts.clear()\n",
    "    last_position[0] = None\n",
    "    last_position[1] = None\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state, role, epsilon)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        next_state = flatten_obs(next_obs)\n",
    "\n",
    "        shaped_reward = compute_shaped_reward(next_obs, reward, role)\n",
    "\n",
    "        buffer.push(state, action, shaped_reward, next_state, done, role)\n",
    "        agent.update(buffer)\n",
    "\n",
    "        if ep % 500 == 0:\n",
    "            print(f\"[Ep {ep}] Role: {role} | Loc: {obs['location']} | A: {action} | R: {reward:.2f} -> SR: {shaped_reward:.2f}\")\n",
    "            env.render()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    epsilon = max(final_epsilon, epsilon - decay_rate)\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    if ep % 1000 == 0:\n",
    "        avg_r = np.mean(all_rewards[-1000:])\n",
    "        print(f\"Ep {ep} | Avg Reward (last 1k): {avg_r:.2f} | Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "plt.plot(all_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Reward Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c533b-c20a-4de1-988c-fe25ce7334e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install safetensors\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "save_dir = \"./model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "flattened = {}\n",
    "for role in [\"scout\", \"guard\"]:\n",
    "    for part in [\"model\", \"target\", \"optimizer\"]:\n",
    "        sd = agent.agents[0 if role == \"scout\" else 1][part].state_dict()\n",
    "        for key, tensor in sd.items():\n",
    "            if isinstance(tensor, torch.Tensor):\n",
    "                flattened[f\"{role}_{part}.{key}\"] = tensor\n",
    "\n",
    "save_file(flattened, os.path.join(save_dir, \"model.safetensors\"))\n",
    "\n",
    "with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"state_dim\": state_dim,\n",
    "        \"action_dim\": action_dim,\n",
    "        \"gamma\": agent.gamma,\n",
    "        \"episodes\": episodes,\n",
    "        \"final_epsilon\": epsilon,\n",
    "        \"all_rewards\": all_rewards  # optional: full list of rewards per episode\n",
    "    }, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4030b4a9-96af-487a-b199-ec3e25e9b6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TILAIEnv:\n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"Initialize the TIL-AI environment.\"\"\"\n",
    "        self.grid_size = 16\n",
    "        self.max_steps = 100\n",
    "        self.obstacles = set()  # Will be initialized in reset()\n",
    "        self.reset(seed=seed)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.is_scout = bool(random.randint(0, 1))  # scout is 1, guard is 0\n",
    "        self.direction = random.randint(0, 3)  # 0: right, 1: down, 2: left, 3: up\n",
    "        \n",
    "        # Create obstacles - but not too many to ensure agent can move\n",
    "        self.obstacles = set()\n",
    "        for _ in range(30):\n",
    "            self.obstacles.add((random.randint(0, 15), random.randint(0, 15)))\n",
    "        \n",
    "        # Place agent at a location without obstacles\n",
    "        while True:\n",
    "            self.location = [random.randint(0, 15), random.randint(0, 15)]\n",
    "            if tuple(self.location) not in self.obstacles:\n",
    "                break\n",
    "        \n",
    "        # Generate recon points and missions, excluding obstacle and agent locations\n",
    "        self.recon_points = set()\n",
    "        self.missions = set()\n",
    "        \n",
    "        for _ in range(100):\n",
    "            while True:\n",
    "                point = (random.randint(0, 15), random.randint(0, 15))\n",
    "                if point not in self.obstacles and point != tuple(self.location):\n",
    "                    self.recon_points.add(point)\n",
    "                    break\n",
    "                    \n",
    "        for _ in range(20):\n",
    "            while True:\n",
    "                point = (random.randint(0, 15), random.randint(0, 15))\n",
    "                if point not in self.obstacles and point != tuple(self.location) and point not in self.recon_points:\n",
    "                    self.missions.add(point)\n",
    "                    break\n",
    "        \n",
    "        self.visited = set()\n",
    "        self.done = False\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Generate observation based on agent's current state.\"\"\"\n",
    "        # Create a simplified viewcone\n",
    "        viewcone = np.zeros((7, 5), dtype=np.uint8)\n",
    "        \n",
    "        # Add some simple content to the viewcone based on agent's surroundings\n",
    "        x, y = self.location\n",
    "        direction = self.direction\n",
    "        \n",
    "        # Fill the viewcone with simplified information\n",
    "        # This is a simplified implementation - the actual competition will have more complex logic\n",
    "        for i in range(7):\n",
    "            for j in range(5):\n",
    "                # Calculate relative position in the grid\n",
    "                dx = j - 2  # -2 to 2 (left to right)\n",
    "                dy = i - 2  # -2 to 4 (back to front, with more visibility forward)\n",
    "                \n",
    "                # Rotate based on agent's direction\n",
    "                if direction == 0:  # right\n",
    "                    nx, ny = x + dx, y + dy - 2  # Adjust forward visibility\n",
    "                elif direction == 1:  # down\n",
    "                    nx, ny = x - dy + 2, y + dx  # Rotate 90° clockwise\n",
    "                elif direction == 2:  # left\n",
    "                    nx, ny = x - dx, y - dy + 2  # Rotate 180°\n",
    "                elif direction == 3:  # up\n",
    "                    nx, ny = x + dy - 2, y - dx  # Rotate 270° clockwise\n",
    "                \n",
    "                # Set value based on what's at this position\n",
    "                if 0 <= nx < 16 and 0 <= ny < 16:  # Within grid bounds\n",
    "                    if (nx, ny) in self.obstacles:\n",
    "                        viewcone[i, j] = 129  # Empty tile (1) with walls (128)\n",
    "                    elif (nx, ny) in self.recon_points:\n",
    "                        viewcone[i, j] = 2  # Recon point\n",
    "                    elif (nx, ny) in self.missions:\n",
    "                        viewcone[i, j] = 3  # Mission\n",
    "                    else:\n",
    "                        viewcone[i, j] = 1  # Empty tile\n",
    "                else:\n",
    "                    viewcone[i, j] = 0  # No vision (out of bounds)\n",
    "        \n",
    "        return {\n",
    "            \"viewcone\": viewcone.tolist(),\n",
    "            \"direction\": self.direction,\n",
    "            \"location\": self.location,\n",
    "            \"scout\": int(self.is_scout),\n",
    "            \"step\": self.step_count\n",
    "        }\n",
    "\n",
    "    def _min_manhattan_distance(self):\n",
    "        \"\"\"Calculate minimum Manhattan distance to relevant targets.\"\"\"\n",
    "        targets = self.recon_points if self.is_scout else self.missions\n",
    "        if not targets:\n",
    "            return 0 \n",
    "        return min(abs(self.location[0] - tx) + abs(self.location[1] - ty) for tx, ty in targets)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment based on the agent's action.\"\"\"\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0, True\n",
    "\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Store original location for debugging\n",
    "        original_location = self.location.copy()\n",
    "        \n",
    "        # Process the action\n",
    "        dx, dy = 0, 0\n",
    "        if action == 0:  # Move forward\n",
    "            dx, dy = self._move_vector(self.direction)\n",
    "        elif action == 1:  # Move backward\n",
    "            dx, dy = self._move_vector((self.direction + 2) % 4)\n",
    "        elif action == 2:  # Turn left\n",
    "            self.direction = (self.direction - 1) % 4\n",
    "        elif action == 3:  # Turn right\n",
    "            self.direction = (self.direction + 1) % 4\n",
    "        # Action 4 is stay (do nothing)\n",
    "\n",
    "        # Calculate new location\n",
    "        if action in [0, 1]:  # Only move for forward/backward actions\n",
    "            new_x = np.clip(self.location[0] + dx, 0, 15)\n",
    "            new_y = np.clip(self.location[1] + dy, 0, 15)\n",
    "            new_loc = (new_x, new_y)\n",
    "            \n",
    "            # Only update if not blocked by an obstacle\n",
    "            if new_loc not in self.obstacles:\n",
    "                self.location = [new_x, new_y]\n",
    "        \n",
    "        # Debug movement\n",
    "        # if original_location != self.location:\n",
    "        #     print(f\"Moved from {original_location} to {self.location}, action={action}\")\n",
    "        # elif action in [0, 1]:\n",
    "        #     print(f\"Movement blocked from {original_location}, action={action}\")\n",
    "\n",
    "        # Get current location as tuple for easier checking\n",
    "        loc_tuple = tuple(self.location)\n",
    "        \n",
    "        # Base reward slightly negative to encourage efficient paths\n",
    "        reward = -0.01\n",
    "        \n",
    "        # Penalty for revisiting locations\n",
    "        if loc_tuple in self.visited:\n",
    "            reward -= 0.01\n",
    "        else:\n",
    "            self.visited.add(loc_tuple)\n",
    "\n",
    "        # Role-specific rewards\n",
    "        if self.is_scout:\n",
    "            # Scout collects recon points\n",
    "            if loc_tuple in self.recon_points:\n",
    "                reward += 1  # Match competition reward\n",
    "                self.recon_points.remove(loc_tuple)\n",
    "                # print(f\"Collected recon point at {loc_tuple}! Reward +1\")\n",
    "                \n",
    "            # Scout completes missions\n",
    "            if loc_tuple in self.missions:\n",
    "                reward += 5  # Match competition reward\n",
    "                self.missions.remove(loc_tuple)\n",
    "                # print(f\"Completed mission at {loc_tuple}! Reward +5\")\n",
    "                \n",
    "            # Small chance of capture (game ending for scout)\n",
    "            if random.random() < 0.01:\n",
    "                reward -= 50  # Match competition punishment\n",
    "                self.done = True\n",
    "                # print(f\"Scout captured! Reward -50\")\n",
    "                \n",
    "        else:  # Guard\n",
    "            # Guard captures scout (simulation)\n",
    "            if random.random() < 0.01:\n",
    "                reward += 50  # Match competition reward\n",
    "                self.done = True\n",
    "                # print(f\"Guard captured scout! Reward +50\")\n",
    "\n",
    "        # Add small reward for getting closer to objectives\n",
    "        reward += 0.01 * (1 / (1 + self._min_manhattan_distance()))\n",
    "        \n",
    "        # Check for episode termination\n",
    "        if self.step_count >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_obs(), reward, self.done\n",
    "\n",
    "    def _move_vector(self, direction):\n",
    "        \"\"\"Get the movement vector for a given direction.\"\"\"\n",
    "        return [(1, 0), (0, 1), (-1, 0), (0, -1)][direction]\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render the current state of the environment.\"\"\"\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        \n",
    "        # Add obstacles, recon points, and missions to the grid\n",
    "        for ox, oy in self.obstacles:\n",
    "            grid[oy][ox] = 'X'\n",
    "        \n",
    "        for rx, ry in self.recon_points:\n",
    "            grid[ry][rx] = 'R'\n",
    "            \n",
    "        for mx, my in self.missions:\n",
    "            grid[my][mx] = 'M'\n",
    "        \n",
    "        # Add agent to the grid\n",
    "        x, y = self.location\n",
    "        grid[y][x] = 'S' if self.is_scout else 'G'\n",
    "        \n",
    "        # Print the grid\n",
    "        # print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        # print(f\"Direction: {['Right', 'Down', 'Left', 'Up'][self.direction]}\")\n",
    "        # print(f\"Step: {self.step_count}/{self.max_steps}\")\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1203bb4c-e014-4e6d-902e-04a094b3fa18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1, Progress: 9/1000\n",
      "Scout count: 7, Guard count: 2\n",
      "Scout ratio: 0.6, Expected guard count: 3.6\n",
      "Phase 1, Progress: 19/1000\n",
      "Scout count: 12, Guard count: 7\n",
      "Scout ratio: 0.6, Expected guard count: 7.6\n",
      "Phase 1, Progress: 29/1000\n",
      "Scout count: 19, Guard count: 10\n",
      "Scout ratio: 0.6, Expected guard count: 11.6\n",
      "Phase 1, Progress: 39/1000\n",
      "Scout count: 25, Guard count: 14\n",
      "Scout ratio: 0.6, Expected guard count: 15.6\n",
      "Phase 1, Progress: 49/1000\n",
      "Scout count: 31, Guard count: 18\n",
      "Scout ratio: 0.6, Expected guard count: 19.6\n",
      "Phase 1, Progress: 59/1000\n",
      "Scout count: 39, Guard count: 20\n",
      "Scout ratio: 0.6, Expected guard count: 23.6\n",
      "Phase 1, Progress: 69/1000\n",
      "Scout count: 46, Guard count: 23\n",
      "Scout ratio: 0.6, Expected guard count: 27.6\n",
      "Phase 1, Progress: 79/1000\n",
      "Scout count: 50, Guard count: 29\n",
      "Scout ratio: 0.6, Expected guard count: 31.6\n",
      "Phase 1, Progress: 89/1000\n",
      "Scout count: 56, Guard count: 33\n",
      "Scout ratio: 0.6, Expected guard count: 35.6\n",
      "Phase 1, Progress: 99/1000\n",
      "Scout count: 62, Guard count: 37\n",
      "Scout ratio: 0.6, Expected guard count: 39.6\n",
      "[Ep 100] Role: 1 | Loc: [6, 14] | Dir: 2\n",
      "Ep 100/5000 | Time: 49.1s | Avg Reward: -0.79 | Scout: 3.57 | Guard: -6.09 | Scout Captures: 19 | Scout Collections: 25 | Guard Captures: 9 | Epsilon (S/G): 0.296/0.098\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 1, Progress: 109/1000\n",
      "Scout count: 68, Guard count: 41\n",
      "Scout ratio: 0.6, Expected guard count: 43.6\n",
      "Phase 1, Progress: 119/1000\n",
      "Scout count: 75, Guard count: 44\n",
      "Scout ratio: 0.6, Expected guard count: 47.6\n",
      "Phase 1, Progress: 129/1000\n",
      "Scout count: 82, Guard count: 47\n",
      "Scout ratio: 0.6, Expected guard count: 51.6\n",
      "Phase 1, Progress: 139/1000\n",
      "Scout count: 86, Guard count: 53\n",
      "Scout ratio: 0.6, Expected guard count: 55.6\n",
      "Phase 1, Progress: 149/1000\n",
      "Scout count: 93, Guard count: 56\n",
      "Scout ratio: 0.6, Expected guard count: 59.6\n",
      "Phase 1, Progress: 159/1000\n",
      "Scout count: 99, Guard count: 60\n",
      "Scout ratio: 0.6, Expected guard count: 63.6\n",
      "Phase 1, Progress: 169/1000\n",
      "Scout count: 106, Guard count: 63\n",
      "Scout ratio: 0.6, Expected guard count: 67.6\n",
      "Phase 1, Progress: 179/1000\n",
      "Scout count: 111, Guard count: 68\n",
      "Scout ratio: 0.6, Expected guard count: 71.6\n",
      "Phase 1, Progress: 189/1000\n",
      "Scout count: 118, Guard count: 71\n",
      "Scout ratio: 0.6, Expected guard count: 75.6\n",
      "Phase 1, Progress: 199/1000\n",
      "Scout count: 122, Guard count: 77\n",
      "Scout ratio: 0.6, Expected guard count: 79.6\n",
      "[Ep 200] Role: 1 | Loc: [8, 4] | Dir: 3\n",
      "Ep 200/5000 | Time: 96.8s | Avg Reward: 7.34 | Scout: 15.93 | Guard: -5.21 | Scout Captures: 30 | Scout Collections: 46 | Guard Captures: 18 | Epsilon (S/G): 0.293/0.096\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 1, Progress: 209/1000\n",
      "Scout count: 125, Guard count: 84\n",
      "Scout ratio: 0.6, Expected guard count: 83.6\n",
      "Phase 1, Progress: 219/1000\n",
      "Scout count: 132, Guard count: 87\n",
      "Scout ratio: 0.6, Expected guard count: 87.6\n",
      "Phase 1, Progress: 229/1000\n",
      "Scout count: 138, Guard count: 91\n",
      "Scout ratio: 0.6, Expected guard count: 91.6\n",
      "Phase 1, Progress: 239/1000\n",
      "Scout count: 143, Guard count: 96\n",
      "Scout ratio: 0.6, Expected guard count: 95.6\n",
      "Stored historical models at episode 250\n",
      "Phase 1, Progress: 249/1000\n",
      "Scout count: 149, Guard count: 100\n",
      "Scout ratio: 0.6, Expected guard count: 99.6\n",
      "Using historical opponent from episode 250\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -13.05, Scout: -17.50, Guard: -8.61\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 0.27\n",
      "Combined Evaluation Score: -10.39\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: -10.39\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 259/1000\n",
      "Scout count: 158, Guard count: 101\n",
      "Scout ratio: 0.6, Expected guard count: 103.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 269/1000\n",
      "Scout count: 161, Guard count: 108\n",
      "Scout ratio: 0.6, Expected guard count: 107.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 279/1000\n",
      "Scout count: 167, Guard count: 112\n",
      "Scout ratio: 0.6, Expected guard count: 111.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 289/1000\n",
      "Scout count: 172, Guard count: 117\n",
      "Scout ratio: 0.6, Expected guard count: 115.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 299/1000\n",
      "Scout count: 175, Guard count: 124\n",
      "Scout ratio: 0.6, Expected guard count: 119.6\n",
      "[Ep 300] Role: 1 | Loc: [6, 4] | Dir: 1\n",
      "Ep 300/5000 | Time: 152.6s | Avg Reward: 7.93 | Scout: 8.34 | Guard: 7.14 | Scout Captures: 41 | Scout Collections: 71 | Guard Captures: 35 | Epsilon (S/G): 0.290/0.094\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 309/1000\n",
      "Scout count: 185, Guard count: 124\n",
      "Scout ratio: 0.6, Expected guard count: 123.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 319/1000\n",
      "Scout count: 191, Guard count: 128\n",
      "Scout ratio: 0.6, Expected guard count: 127.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 329/1000\n",
      "Scout count: 197, Guard count: 132\n",
      "Scout ratio: 0.6, Expected guard count: 131.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 339/1000\n",
      "Scout count: 202, Guard count: 137\n",
      "Scout ratio: 0.6, Expected guard count: 135.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 349/1000\n",
      "Scout count: 208, Guard count: 141\n",
      "Scout ratio: 0.6, Expected guard count: 139.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 359/1000\n",
      "Scout count: 217, Guard count: 142\n",
      "Scout ratio: 0.6, Expected guard count: 143.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 369/1000\n",
      "Scout count: 223, Guard count: 146\n",
      "Scout ratio: 0.6, Expected guard count: 147.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 379/1000\n",
      "Scout count: 229, Guard count: 150\n",
      "Scout ratio: 0.6, Expected guard count: 151.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 389/1000\n",
      "Scout count: 236, Guard count: 153\n",
      "Scout ratio: 0.6, Expected guard count: 155.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 399/1000\n",
      "Scout count: 239, Guard count: 160\n",
      "Scout ratio: 0.6, Expected guard count: 159.6\n",
      "[Ep 400] Role: 0 | Loc: [2, 3] | Dir: 2\n",
      "Ep 400/5000 | Time: 207.1s | Avg Reward: 3.70 | Scout: 2.55 | Guard: 7.48 | Scout Captures: 64 | Scout Collections: 99 | Guard Captures: 51 | Epsilon (S/G): 0.286/0.092\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 409/1000\n",
      "Scout count: 243, Guard count: 166\n",
      "Scout ratio: 0.6, Expected guard count: 163.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 419/1000\n",
      "Scout count: 249, Guard count: 170\n",
      "Scout ratio: 0.6, Expected guard count: 167.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 429/1000\n",
      "Scout count: 255, Guard count: 174\n",
      "Scout ratio: 0.6, Expected guard count: 171.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 439/1000\n",
      "Scout count: 262, Guard count: 177\n",
      "Scout ratio: 0.6, Expected guard count: 175.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 449/1000\n",
      "Scout count: 268, Guard count: 181\n",
      "Scout ratio: 0.6, Expected guard count: 179.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 459/1000\n",
      "Scout count: 273, Guard count: 186\n",
      "Scout ratio: 0.6, Expected guard count: 183.6\n",
      "Phase 1, Progress: 469/1000\n",
      "Scout count: 278, Guard count: 191\n",
      "Scout ratio: 0.6, Expected guard count: 187.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 479/1000\n",
      "Scout count: 285, Guard count: 194\n",
      "Scout ratio: 0.6, Expected guard count: 191.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 489/1000\n",
      "Scout count: 293, Guard count: 196\n",
      "Scout ratio: 0.6, Expected guard count: 195.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Stored historical models at episode 500\n",
      "Phase 1, Progress: 499/1000\n",
      "Scout count: 298, Guard count: 201\n",
      "Scout ratio: 0.6, Expected guard count: 199.6\n",
      "[Ep 500] Role: 0 | Loc: [10, 13] | Dir: 0\n",
      "Ep 500/5000 | Time: 262.9s | Avg Reward: 10.62 | Scout: 0.32 | Guard: 10.81 | Scout Captures: 84 | Scout Collections: 129 | Guard Captures: 71 | Epsilon (S/G): 0.283/0.090\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 5.06, Scout: -29.29, Guard: 39.40\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 1.97\n",
      "Combined Evaluation Score: 4.44\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: 4.44\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 1, Progress: 509/1000\n",
      "Scout count: 304, Guard count: 205\n",
      "Scout ratio: 0.6, Expected guard count: 203.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 519/1000\n",
      "Scout count: 309, Guard count: 210\n",
      "Scout ratio: 0.6, Expected guard count: 207.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 529/1000\n",
      "Scout count: 316, Guard count: 213\n",
      "Scout ratio: 0.6, Expected guard count: 211.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 539/1000\n",
      "Scout count: 323, Guard count: 216\n",
      "Scout ratio: 0.6, Expected guard count: 215.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 549/1000\n",
      "Scout count: 328, Guard count: 221\n",
      "Scout ratio: 0.6, Expected guard count: 219.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 559/1000\n",
      "Scout count: 336, Guard count: 223\n",
      "Scout ratio: 0.6, Expected guard count: 223.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 569/1000\n",
      "Scout count: 342, Guard count: 227\n",
      "Scout ratio: 0.6, Expected guard count: 227.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 579/1000\n",
      "Scout count: 347, Guard count: 232\n",
      "Scout ratio: 0.6, Expected guard count: 231.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 589/1000\n",
      "Scout count: 355, Guard count: 234\n",
      "Scout ratio: 0.6, Expected guard count: 235.6\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 599/1000\n",
      "Scout count: 361, Guard count: 238\n",
      "Scout ratio: 0.6, Expected guard count: 239.6\n",
      "[Ep 600] Role: 1 | Loc: [13, 2] | Dir: 2\n",
      "Ep 600/5000 | Time: 327.0s | Avg Reward: 4.76 | Scout: 9.70 | Guard: 3.15 | Scout Captures: 99 | Scout Collections: 154 | Guard Captures: 83 | Epsilon (S/G): 0.279/0.089\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 609/1000\n",
      "Scout count: 367, Guard count: 242\n",
      "Scout ratio: 0.6, Expected guard count: 243.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 619/1000\n",
      "Scout count: 372, Guard count: 247\n",
      "Scout ratio: 0.6, Expected guard count: 247.6\n",
      "Phase 1, Progress: 629/1000\n",
      "Scout count: 380, Guard count: 249\n",
      "Scout ratio: 0.6, Expected guard count: 251.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 639/1000\n",
      "Scout count: 382, Guard count: 257\n",
      "Scout ratio: 0.6, Expected guard count: 255.6\n",
      "Phase 1, Progress: 649/1000\n",
      "Scout count: 390, Guard count: 259\n",
      "Scout ratio: 0.6, Expected guard count: 259.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 659/1000\n",
      "Scout count: 397, Guard count: 262\n",
      "Scout ratio: 0.6, Expected guard count: 263.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 669/1000\n",
      "Scout count: 401, Guard count: 268\n",
      "Scout ratio: 0.6, Expected guard count: 267.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 679/1000\n",
      "Scout count: 403, Guard count: 276\n",
      "Scout ratio: 0.6, Expected guard count: 271.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 689/1000\n",
      "Scout count: 410, Guard count: 279\n",
      "Scout ratio: 0.6, Expected guard count: 275.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 699/1000\n",
      "Scout count: 413, Guard count: 286\n",
      "Scout ratio: 0.6, Expected guard count: 279.6\n",
      "Using historical opponent from episode 250\n",
      "[Ep 700] Role: 0 | Loc: [13, 2] | Dir: 1\n",
      "Ep 700/5000 | Time: 397.4s | Avg Reward: -2.61 | Scout: 1.65 | Guard: -3.83 | Scout Captures: 114 | Scout Collections: 172 | Guard Captures: 97 | Epsilon (S/G): 0.276/0.087\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 709/1000\n",
      "Scout count: 417, Guard count: 292\n",
      "Scout ratio: 0.6, Expected guard count: 283.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 719/1000\n",
      "Scout count: 424, Guard count: 295\n",
      "Scout ratio: 0.6, Expected guard count: 287.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 729/1000\n",
      "Scout count: 431, Guard count: 298\n",
      "Scout ratio: 0.6, Expected guard count: 291.6\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 739/1000\n",
      "Scout count: 436, Guard count: 303\n",
      "Scout ratio: 0.6, Expected guard count: 295.6\n",
      "Using historical opponent from episode 500\n",
      "Stored historical models at episode 750\n",
      "Phase 1, Progress: 749/1000\n",
      "Scout count: 440, Guard count: 309\n",
      "Scout ratio: 0.6, Expected guard count: 299.6\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -4.64, Scout: -20.29, Guard: 11.02\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 7.45\n",
      "Combined Evaluation Score: -2.22\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 759/1000\n",
      "Scout count: 447, Guard count: 312\n",
      "Scout ratio: 0.6, Expected guard count: 303.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 769/1000\n",
      "Scout count: 452, Guard count: 317\n",
      "Scout ratio: 0.6, Expected guard count: 307.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 779/1000\n",
      "Scout count: 458, Guard count: 321\n",
      "Scout ratio: 0.6, Expected guard count: 311.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 789/1000\n",
      "Scout count: 462, Guard count: 327\n",
      "Scout ratio: 0.6, Expected guard count: 315.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 799/1000\n",
      "Scout count: 467, Guard count: 332\n",
      "Scout ratio: 0.6, Expected guard count: 319.6\n",
      "[Ep 800] Role: 1 | Loc: [9, 11] | Dir: 1\n",
      "Ep 800/5000 | Time: 459.6s | Avg Reward: -0.74 | Scout: 6.62 | Guard: -1.86 | Scout Captures: 131 | Scout Collections: 189 | Guard Captures: 109 | Epsilon (S/G): 0.273/0.085\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 809/1000\n",
      "Scout count: 475, Guard count: 334\n",
      "Scout ratio: 0.6, Expected guard count: 323.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 819/1000\n",
      "Scout count: 484, Guard count: 335\n",
      "Scout ratio: 0.6, Expected guard count: 327.6\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 829/1000\n",
      "Scout count: 490, Guard count: 339\n",
      "Scout ratio: 0.6, Expected guard count: 331.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 839/1000\n",
      "Scout count: 498, Guard count: 341\n",
      "Scout ratio: 0.6, Expected guard count: 335.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 849/1000\n",
      "Scout count: 503, Guard count: 346\n",
      "Scout ratio: 0.6, Expected guard count: 339.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 859/1000\n",
      "Scout count: 508, Guard count: 351\n",
      "Scout ratio: 0.6, Expected guard count: 343.6\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 869/1000\n",
      "Scout count: 514, Guard count: 355\n",
      "Scout ratio: 0.6, Expected guard count: 347.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 879/1000\n",
      "Scout count: 523, Guard count: 356\n",
      "Scout ratio: 0.6, Expected guard count: 351.6\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 889/1000\n",
      "Scout count: 528, Guard count: 361\n",
      "Scout ratio: 0.6, Expected guard count: 355.6\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 899/1000\n",
      "Scout count: 532, Guard count: 367\n",
      "Scout ratio: 0.6, Expected guard count: 359.6\n",
      "[Ep 900] Role: 1 | Loc: [4, 2] | Dir: 1\n",
      "Ep 900/5000 | Time: 534.0s | Avg Reward: 3.33 | Scout: -1.48 | Guard: 3.19 | Scout Captures: 154 | Scout Collections: 221 | Guard Captures: 123 | Epsilon (S/G): 0.270/0.083\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 909/1000\n",
      "Scout count: 541, Guard count: 368\n",
      "Scout ratio: 0.6, Expected guard count: 363.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 919/1000\n",
      "Scout count: 546, Guard count: 373\n",
      "Scout ratio: 0.6, Expected guard count: 367.6\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 929/1000\n",
      "Scout count: 551, Guard count: 378\n",
      "Scout ratio: 0.6, Expected guard count: 371.6\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 939/1000\n",
      "Scout count: 557, Guard count: 382\n",
      "Scout ratio: 0.6, Expected guard count: 375.6\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 949/1000\n",
      "Scout count: 561, Guard count: 388\n",
      "Scout ratio: 0.6, Expected guard count: 379.6\n",
      "Using historical opponent from episode 500\n",
      "Phase 1, Progress: 959/1000\n",
      "Scout count: 567, Guard count: 392\n",
      "Scout ratio: 0.6, Expected guard count: 383.6\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 969/1000\n",
      "Scout count: 572, Guard count: 397\n",
      "Scout ratio: 0.6, Expected guard count: 387.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Phase 1, Progress: 979/1000\n",
      "Scout count: 579, Guard count: 400\n",
      "Scout ratio: 0.6, Expected guard count: 391.6\n",
      "Using historical opponent from episode 750\n",
      "Phase 1, Progress: 989/1000\n",
      "Scout count: 583, Guard count: 406\n",
      "Scout ratio: 0.6, Expected guard count: 395.6\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 750\n",
      "Stored historical models at episode 1000\n",
      "Phase 1, Progress: 999/1000\n",
      "Scout count: 590, Guard count: 409\n",
      "Scout ratio: 0.6, Expected guard count: 399.6\n",
      "[Ep 1000] Role: 1 | Loc: [6, 11] | Dir: 0\n",
      "Ep 1000/5000 | Time: 603.6s | Avg Reward: -0.47 | Scout: 2.40 | Guard: 2.90 | Scout Captures: 172 | Scout Collections: 246 | Guard Captures: 135 | Epsilon (S/G): 0.267/0.082\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -15.20, Scout: -10.08, Guard: -20.31\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 9.43\n",
      "Combined Evaluation Score: -10.27\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Moving to curriculum phase 2\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 9/2000\n",
      "Scout count: 4, Guard count: 5\n",
      "Scout ratio: 0.5, Expected guard count: 4.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 19/2000\n",
      "Scout count: 9, Guard count: 10\n",
      "Scout ratio: 0.5, Expected guard count: 9.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 29/2000\n",
      "Scout count: 14, Guard count: 15\n",
      "Scout ratio: 0.5, Expected guard count: 14.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 39/2000\n",
      "Scout count: 19, Guard count: 20\n",
      "Scout ratio: 0.5, Expected guard count: 19.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 49/2000\n",
      "Scout count: 23, Guard count: 26\n",
      "Scout ratio: 0.5, Expected guard count: 24.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 59/2000\n",
      "Scout count: 29, Guard count: 30\n",
      "Scout ratio: 0.5, Expected guard count: 29.5\n",
      "Phase 2, Progress: 69/2000\n",
      "Scout count: 34, Guard count: 35\n",
      "Scout ratio: 0.5, Expected guard count: 34.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 79/2000\n",
      "Scout count: 38, Guard count: 41\n",
      "Scout ratio: 0.5, Expected guard count: 39.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 89/2000\n",
      "Scout count: 42, Guard count: 47\n",
      "Scout ratio: 0.5, Expected guard count: 44.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 99/2000\n",
      "Scout count: 48, Guard count: 51\n",
      "Scout ratio: 0.5, Expected guard count: 49.5\n",
      "[Ep 1100] Role: 0 | Loc: [11, 4] | Dir: 2\n",
      "Ep 1100/5000 | Time: 658.5s | Avg Reward: 1.56 | Scout: -2.78 | Guard: 6.05 | Scout Captures: 186 | Scout Collections: 269 | Guard Captures: 157 | Epsilon (S/G): 0.264/0.079\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 2, Progress: 109/2000\n",
      "Scout count: 52, Guard count: 57\n",
      "Scout ratio: 0.5, Expected guard count: 54.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 119/2000\n",
      "Scout count: 58, Guard count: 61\n",
      "Scout ratio: 0.5, Expected guard count: 59.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 129/2000\n",
      "Scout count: 62, Guard count: 67\n",
      "Scout ratio: 0.5, Expected guard count: 64.5\n",
      "Phase 2, Progress: 139/2000\n",
      "Scout count: 67, Guard count: 72\n",
      "Scout ratio: 0.5, Expected guard count: 69.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 149/2000\n",
      "Scout count: 69, Guard count: 80\n",
      "Scout ratio: 0.5, Expected guard count: 74.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 159/2000\n",
      "Scout count: 71, Guard count: 88\n",
      "Scout ratio: 0.5, Expected guard count: 79.5\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 169/2000\n",
      "Scout count: 79, Guard count: 90\n",
      "Scout ratio: 0.5, Expected guard count: 84.5\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 179/2000\n",
      "Scout count: 84, Guard count: 95\n",
      "Scout ratio: 0.5, Expected guard count: 89.5\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 189/2000\n",
      "Scout count: 90, Guard count: 99\n",
      "Scout ratio: 0.5, Expected guard count: 94.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 199/2000\n",
      "Scout count: 95, Guard count: 104\n",
      "Scout ratio: 0.5, Expected guard count: 99.5\n",
      "[Ep 1200] Role: 1 | Loc: [4, 13] | Dir: 0\n",
      "Ep 1200/5000 | Time: 715.1s | Avg Reward: 10.04 | Scout: 12.07 | Guard: 3.43 | Scout Captures: 195 | Scout Collections: 286 | Guard Captures: 174 | Epsilon (S/G): 0.261/0.077\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 2, Progress: 209/2000\n",
      "Scout count: 100, Guard count: 109\n",
      "Scout ratio: 0.5, Expected guard count: 104.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 219/2000\n",
      "Scout count: 105, Guard count: 114\n",
      "Scout ratio: 0.5, Expected guard count: 109.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 229/2000\n",
      "Scout count: 111, Guard count: 118\n",
      "Scout ratio: 0.5, Expected guard count: 114.5\n",
      "Phase 2, Progress: 239/2000\n",
      "Scout count: 116, Guard count: 123\n",
      "Scout ratio: 0.5, Expected guard count: 119.5\n",
      "Using historical opponent from episode 250\n",
      "Stored historical models at episode 1250\n",
      "Phase 2, Progress: 249/2000\n",
      "Scout count: 121, Guard count: 128\n",
      "Scout ratio: 0.5, Expected guard count: 124.5\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -19.07, Scout: -47.48, Guard: 9.35\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 1.48\n",
      "Combined Evaluation Score: -14.96\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 259/2000\n",
      "Scout count: 128, Guard count: 131\n",
      "Scout ratio: 0.5, Expected guard count: 129.5\n",
      "Phase 2, Progress: 269/2000\n",
      "Scout count: 132, Guard count: 137\n",
      "Scout ratio: 0.5, Expected guard count: 134.5\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 279/2000\n",
      "Scout count: 136, Guard count: 143\n",
      "Scout ratio: 0.5, Expected guard count: 139.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 289/2000\n",
      "Scout count: 143, Guard count: 146\n",
      "Scout ratio: 0.5, Expected guard count: 144.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 299/2000\n",
      "Scout count: 146, Guard count: 153\n",
      "Scout ratio: 0.5, Expected guard count: 149.5\n",
      "[Ep 1300] Role: 0 | Loc: [15, 11] | Dir: 2\n",
      "Ep 1300/5000 | Time: 773.2s | Avg Reward: 2.77 | Scout: 4.00 | Guard: 2.67 | Scout Captures: 212 | Scout Collections: 311 | Guard Captures: 192 | Epsilon (S/G): 0.259/0.075\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 309/2000\n",
      "Scout count: 152, Guard count: 157\n",
      "Scout ratio: 0.5, Expected guard count: 154.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 319/2000\n",
      "Scout count: 158, Guard count: 161\n",
      "Scout ratio: 0.5, Expected guard count: 159.5\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 329/2000\n",
      "Scout count: 167, Guard count: 162\n",
      "Scout ratio: 0.5, Expected guard count: 164.5\n",
      "Phase 2, Progress: 339/2000\n",
      "Scout count: 172, Guard count: 167\n",
      "Scout ratio: 0.5, Expected guard count: 169.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 349/2000\n",
      "Scout count: 177, Guard count: 172\n",
      "Scout ratio: 0.5, Expected guard count: 174.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 359/2000\n",
      "Scout count: 184, Guard count: 175\n",
      "Scout ratio: 0.5, Expected guard count: 179.5\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 369/2000\n",
      "Scout count: 188, Guard count: 181\n",
      "Scout ratio: 0.5, Expected guard count: 184.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 379/2000\n",
      "Scout count: 192, Guard count: 187\n",
      "Scout ratio: 0.5, Expected guard count: 189.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 389/2000\n",
      "Scout count: 200, Guard count: 189\n",
      "Scout ratio: 0.5, Expected guard count: 194.5\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 399/2000\n",
      "Scout count: 205, Guard count: 194\n",
      "Scout ratio: 0.5, Expected guard count: 199.5\n",
      "[Ep 1400] Role: 0 | Loc: [5, 8] | Dir: 1\n",
      "Ep 1400/5000 | Time: 843.0s | Avg Reward: 5.99 | Scout: 5.30 | Guard: -2.49 | Scout Captures: 225 | Scout Collections: 335 | Guard Captures: 203 | Epsilon (S/G): 0.256/0.074\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 409/2000\n",
      "Scout count: 207, Guard count: 202\n",
      "Scout ratio: 0.5, Expected guard count: 204.5\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 419/2000\n",
      "Scout count: 211, Guard count: 208\n",
      "Scout ratio: 0.5, Expected guard count: 209.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 429/2000\n",
      "Scout count: 218, Guard count: 211\n",
      "Scout ratio: 0.5, Expected guard count: 214.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 439/2000\n",
      "Scout count: 223, Guard count: 216\n",
      "Scout ratio: 0.5, Expected guard count: 219.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 449/2000\n",
      "Scout count: 230, Guard count: 219\n",
      "Scout ratio: 0.5, Expected guard count: 224.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 459/2000\n",
      "Scout count: 233, Guard count: 226\n",
      "Scout ratio: 0.5, Expected guard count: 229.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 469/2000\n",
      "Scout count: 236, Guard count: 233\n",
      "Scout ratio: 0.5, Expected guard count: 234.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 479/2000\n",
      "Scout count: 242, Guard count: 237\n",
      "Scout ratio: 0.5, Expected guard count: 239.5\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 250\n",
      "Phase 2, Progress: 489/2000\n",
      "Scout count: 244, Guard count: 245\n",
      "Scout ratio: 0.5, Expected guard count: 244.5\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Using historical opponent from episode 250\n",
      "Stored historical models at episode 1500\n",
      "Phase 2, Progress: 499/2000\n",
      "Scout count: 249, Guard count: 250\n",
      "Scout ratio: 0.5, Expected guard count: 249.5\n",
      "[Ep 1500] Role: 1 | Loc: [8, 6] | Dir: 2\n",
      "Ep 1500/5000 | Time: 917.6s | Avg Reward: -5.62 | Scout: -3.73 | Guard: -0.24 | Scout Captures: 242 | Scout Collections: 359 | Guard Captures: 218 | Epsilon (S/G): 0.254/0.072\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 10.56, Scout: 12.49, Guard: 8.62\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -17.03\n",
      "Combined Evaluation Score: 5.04\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: 5.04\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 509/2000\n",
      "Scout count: 256, Guard count: 253\n",
      "Scout ratio: 0.5, Expected guard count: 254.5\n",
      "Phase 2, Progress: 519/2000\n",
      "Scout count: 260, Guard count: 259\n",
      "Scout ratio: 0.5, Expected guard count: 259.5\n",
      "Phase 2, Progress: 529/2000\n",
      "Scout count: 266, Guard count: 263\n",
      "Scout ratio: 0.5, Expected guard count: 264.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 539/2000\n",
      "Scout count: 271, Guard count: 268\n",
      "Scout ratio: 0.5, Expected guard count: 269.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 549/2000\n",
      "Scout count: 275, Guard count: 274\n",
      "Scout ratio: 0.5, Expected guard count: 274.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 559/2000\n",
      "Scout count: 279, Guard count: 280\n",
      "Scout ratio: 0.5, Expected guard count: 279.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 569/2000\n",
      "Scout count: 285, Guard count: 284\n",
      "Scout ratio: 0.5, Expected guard count: 284.5\n",
      "Phase 2, Progress: 579/2000\n",
      "Scout count: 290, Guard count: 289\n",
      "Scout ratio: 0.5, Expected guard count: 289.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 589/2000\n",
      "Scout count: 294, Guard count: 295\n",
      "Scout ratio: 0.5, Expected guard count: 294.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 599/2000\n",
      "Scout count: 300, Guard count: 299\n",
      "Scout ratio: 0.5, Expected guard count: 299.5\n",
      "Using historical opponent from episode 500\n",
      "[Ep 1600] Role: 1 | Loc: [3, 0] | Dir: 0\n",
      "Ep 1600/5000 | Time: 987.1s | Avg Reward: 6.63 | Scout: 0.83 | Guard: 10.14 | Scout Captures: 258 | Scout Collections: 380 | Guard Captures: 237 | Epsilon (S/G): 0.251/0.070\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 609/2000\n",
      "Scout count: 304, Guard count: 305\n",
      "Scout ratio: 0.5, Expected guard count: 304.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 619/2000\n",
      "Scout count: 307, Guard count: 312\n",
      "Scout ratio: 0.5, Expected guard count: 309.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 629/2000\n",
      "Scout count: 312, Guard count: 317\n",
      "Scout ratio: 0.5, Expected guard count: 314.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 639/2000\n",
      "Scout count: 318, Guard count: 321\n",
      "Scout ratio: 0.5, Expected guard count: 319.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 649/2000\n",
      "Scout count: 323, Guard count: 326\n",
      "Scout ratio: 0.5, Expected guard count: 324.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 659/2000\n",
      "Scout count: 326, Guard count: 333\n",
      "Scout ratio: 0.5, Expected guard count: 329.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 669/2000\n",
      "Scout count: 332, Guard count: 337\n",
      "Scout ratio: 0.5, Expected guard count: 334.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 679/2000\n",
      "Scout count: 339, Guard count: 340\n",
      "Scout ratio: 0.5, Expected guard count: 339.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 500\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 689/2000\n",
      "Scout count: 345, Guard count: 344\n",
      "Scout ratio: 0.5, Expected guard count: 344.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 699/2000\n",
      "Scout count: 350, Guard count: 349\n",
      "Scout ratio: 0.5, Expected guard count: 349.5\n",
      "[Ep 1700] Role: 0 | Loc: [3, 15] | Dir: 2\n",
      "Ep 1700/5000 | Time: 1051.5s | Avg Reward: 15.01 | Scout: 15.46 | Guard: 1.52 | Scout Captures: 269 | Scout Collections: 396 | Guard Captures: 254 | Epsilon (S/G): 0.249/0.068\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 709/2000\n",
      "Scout count: 355, Guard count: 354\n",
      "Scout ratio: 0.5, Expected guard count: 354.5\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 719/2000\n",
      "Scout count: 361, Guard count: 358\n",
      "Scout ratio: 0.5, Expected guard count: 359.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 729/2000\n",
      "Scout count: 368, Guard count: 361\n",
      "Scout ratio: 0.5, Expected guard count: 364.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 500\n",
      "Phase 2, Progress: 739/2000\n",
      "Scout count: 375, Guard count: 364\n",
      "Scout ratio: 0.5, Expected guard count: 369.5\n",
      "Stored historical models at episode 1750\n",
      "Phase 2, Progress: 749/2000\n",
      "Scout count: 378, Guard count: 371\n",
      "Scout ratio: 0.5, Expected guard count: 374.5\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 10.16, Scout: 10.13, Guard: 10.18\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -5.96\n",
      "Combined Evaluation Score: 6.93\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: 6.93\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 759/2000\n",
      "Scout count: 383, Guard count: 376\n",
      "Scout ratio: 0.5, Expected guard count: 379.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 769/2000\n",
      "Scout count: 388, Guard count: 381\n",
      "Scout ratio: 0.5, Expected guard count: 384.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 779/2000\n",
      "Scout count: 395, Guard count: 384\n",
      "Scout ratio: 0.5, Expected guard count: 389.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 789/2000\n",
      "Scout count: 400, Guard count: 389\n",
      "Scout ratio: 0.5, Expected guard count: 394.5\n",
      "Phase 2, Progress: 799/2000\n",
      "Scout count: 403, Guard count: 396\n",
      "Scout ratio: 0.5, Expected guard count: 399.5\n",
      "Using historical opponent from episode 750\n",
      "[Ep 1800] Role: 1 | Loc: [4, 15] | Dir: 0\n",
      "Ep 1800/5000 | Time: 1131.7s | Avg Reward: 0.01 | Scout: 6.88 | Guard: -6.38 | Scout Captures: 283 | Scout Collections: 417 | Guard Captures: 266 | Epsilon (S/G): 0.246/0.067\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 809/2000\n",
      "Scout count: 406, Guard count: 403\n",
      "Scout ratio: 0.5, Expected guard count: 404.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 819/2000\n",
      "Scout count: 410, Guard count: 409\n",
      "Scout ratio: 0.5, Expected guard count: 409.5\n",
      "Phase 2, Progress: 829/2000\n",
      "Scout count: 414, Guard count: 415\n",
      "Scout ratio: 0.5, Expected guard count: 414.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 839/2000\n",
      "Scout count: 418, Guard count: 421\n",
      "Scout ratio: 0.5, Expected guard count: 419.5\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 849/2000\n",
      "Scout count: 425, Guard count: 424\n",
      "Scout ratio: 0.5, Expected guard count: 424.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 859/2000\n",
      "Scout count: 429, Guard count: 430\n",
      "Scout ratio: 0.5, Expected guard count: 429.5\n",
      "Phase 2, Progress: 869/2000\n",
      "Scout count: 437, Guard count: 432\n",
      "Scout ratio: 0.5, Expected guard count: 434.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 879/2000\n",
      "Scout count: 444, Guard count: 435\n",
      "Scout ratio: 0.5, Expected guard count: 439.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 889/2000\n",
      "Scout count: 447, Guard count: 442\n",
      "Scout ratio: 0.5, Expected guard count: 444.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 899/2000\n",
      "Scout count: 450, Guard count: 449\n",
      "Scout ratio: 0.5, Expected guard count: 449.5\n",
      "[Ep 1900] Role: 0 | Loc: [12, 11] | Dir: 1\n",
      "Ep 1900/5000 | Time: 1209.5s | Avg Reward: 9.62 | Scout: 5.23 | Guard: 6.90 | Scout Captures: 298 | Scout Collections: 436 | Guard Captures: 286 | Epsilon (S/G): 0.244/0.065\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 909/2000\n",
      "Scout count: 455, Guard count: 454\n",
      "Scout ratio: 0.5, Expected guard count: 454.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 919/2000\n",
      "Scout count: 460, Guard count: 459\n",
      "Scout ratio: 0.5, Expected guard count: 459.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 929/2000\n",
      "Scout count: 464, Guard count: 465\n",
      "Scout ratio: 0.5, Expected guard count: 464.5\n",
      "Using historical opponent from episode 750\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 939/2000\n",
      "Scout count: 471, Guard count: 468\n",
      "Scout ratio: 0.5, Expected guard count: 469.5\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 949/2000\n",
      "Scout count: 477, Guard count: 472\n",
      "Scout ratio: 0.5, Expected guard count: 474.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 959/2000\n",
      "Scout count: 482, Guard count: 477\n",
      "Scout ratio: 0.5, Expected guard count: 479.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 969/2000\n",
      "Scout count: 488, Guard count: 481\n",
      "Scout ratio: 0.5, Expected guard count: 484.5\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 979/2000\n",
      "Scout count: 495, Guard count: 484\n",
      "Scout ratio: 0.5, Expected guard count: 489.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 750\n",
      "Phase 2, Progress: 989/2000\n",
      "Scout count: 499, Guard count: 490\n",
      "Scout ratio: 0.5, Expected guard count: 494.5\n",
      "Stored historical models at episode 2000\n",
      "Phase 2, Progress: 999/2000\n",
      "Scout count: 505, Guard count: 494\n",
      "Scout ratio: 0.5, Expected guard count: 499.5\n",
      "[Ep 2000] Role: 0 | Loc: [15, 8] | Dir: 1\n",
      "Ep 2000/5000 | Time: 1285.4s | Avg Reward: 8.22 | Scout: 8.76 | Guard: 8.00 | Scout Captures: 314 | Scout Collections: 456 | Guard Captures: 300 | Epsilon (S/G): 0.241/0.064\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -5.05, Scout: -19.05, Guard: 8.96\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 7.30\n",
      "Combined Evaluation Score: -2.58\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 2, Progress: 1009/2000\n",
      "Scout count: 507, Guard count: 502\n",
      "Scout ratio: 0.5, Expected guard count: 504.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1019/2000\n",
      "Scout count: 514, Guard count: 505\n",
      "Scout ratio: 0.5, Expected guard count: 509.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1029/2000\n",
      "Scout count: 518, Guard count: 511\n",
      "Scout ratio: 0.5, Expected guard count: 514.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1039/2000\n",
      "Scout count: 522, Guard count: 517\n",
      "Scout ratio: 0.5, Expected guard count: 519.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 1049/2000\n",
      "Scout count: 526, Guard count: 523\n",
      "Scout ratio: 0.5, Expected guard count: 524.5\n",
      "Phase 2, Progress: 1059/2000\n",
      "Scout count: 530, Guard count: 529\n",
      "Scout ratio: 0.5, Expected guard count: 529.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1069/2000\n",
      "Scout count: 534, Guard count: 535\n",
      "Scout ratio: 0.5, Expected guard count: 534.5\n",
      "Phase 2, Progress: 1079/2000\n",
      "Scout count: 540, Guard count: 539\n",
      "Scout ratio: 0.5, Expected guard count: 539.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1089/2000\n",
      "Scout count: 545, Guard count: 544\n",
      "Scout ratio: 0.5, Expected guard count: 544.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1099/2000\n",
      "Scout count: 553, Guard count: 546\n",
      "Scout ratio: 0.5, Expected guard count: 549.5\n",
      "[Ep 2100] Role: 0 | Loc: [5, 0] | Dir: 1\n",
      "Ep 2100/5000 | Time: 1361.8s | Avg Reward: 5.87 | Scout: 9.68 | Guard: 2.94 | Scout Captures: 326 | Scout Collections: 480 | Guard Captures: 318 | Epsilon (S/G): 0.239/0.062\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1109/2000\n",
      "Scout count: 555, Guard count: 554\n",
      "Scout ratio: 0.5, Expected guard count: 554.5\n",
      "Phase 2, Progress: 1119/2000\n",
      "Scout count: 558, Guard count: 561\n",
      "Scout ratio: 0.5, Expected guard count: 559.5\n",
      "Phase 2, Progress: 1129/2000\n",
      "Scout count: 563, Guard count: 566\n",
      "Scout ratio: 0.5, Expected guard count: 564.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 1139/2000\n",
      "Scout count: 570, Guard count: 569\n",
      "Scout ratio: 0.5, Expected guard count: 569.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1149/2000\n",
      "Scout count: 575, Guard count: 574\n",
      "Scout ratio: 0.5, Expected guard count: 574.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1159/2000\n",
      "Scout count: 581, Guard count: 578\n",
      "Scout ratio: 0.5, Expected guard count: 579.5\n",
      "Phase 2, Progress: 1169/2000\n",
      "Scout count: 583, Guard count: 586\n",
      "Scout ratio: 0.5, Expected guard count: 584.5\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1179/2000\n",
      "Scout count: 586, Guard count: 593\n",
      "Scout ratio: 0.5, Expected guard count: 589.5\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 1189/2000\n",
      "Scout count: 590, Guard count: 599\n",
      "Scout ratio: 0.5, Expected guard count: 594.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1199/2000\n",
      "Scout count: 595, Guard count: 604\n",
      "Scout ratio: 0.5, Expected guard count: 599.5\n",
      "[Ep 2200] Role: 1 | Loc: [11, 8] | Dir: 2\n",
      "Ep 2200/5000 | Time: 1446.2s | Avg Reward: -1.79 | Scout: 5.35 | Guard: -6.46 | Scout Captures: 343 | Scout Collections: 504 | Guard Captures: 329 | Epsilon (S/G): 0.237/0.060\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 1209/2000\n",
      "Scout count: 601, Guard count: 608\n",
      "Scout ratio: 0.5, Expected guard count: 604.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1000\n",
      "Phase 2, Progress: 1219/2000\n",
      "Scout count: 605, Guard count: 614\n",
      "Scout ratio: 0.5, Expected guard count: 609.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1000\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1229/2000\n",
      "Scout count: 611, Guard count: 618\n",
      "Scout ratio: 0.5, Expected guard count: 614.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1239/2000\n",
      "Scout count: 619, Guard count: 620\n",
      "Scout ratio: 0.5, Expected guard count: 619.5\n",
      "Using historical opponent from episode 1500\n",
      "Stored historical models at episode 2250\n",
      "Phase 2, Progress: 1249/2000\n",
      "Scout count: 622, Guard count: 627\n",
      "Scout ratio: 0.5, Expected guard count: 624.5\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -10.24, Scout: -10.49, Guard: -9.98\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -17.07\n",
      "Combined Evaluation Score: -11.60\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1259/2000\n",
      "Scout count: 627, Guard count: 632\n",
      "Scout ratio: 0.5, Expected guard count: 629.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1269/2000\n",
      "Scout count: 635, Guard count: 634\n",
      "Scout ratio: 0.5, Expected guard count: 634.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1279/2000\n",
      "Scout count: 641, Guard count: 638\n",
      "Scout ratio: 0.5, Expected guard count: 639.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1289/2000\n",
      "Scout count: 646, Guard count: 643\n",
      "Scout ratio: 0.5, Expected guard count: 644.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1299/2000\n",
      "Scout count: 649, Guard count: 650\n",
      "Scout ratio: 0.5, Expected guard count: 649.5\n",
      "[Ep 2300] Role: 1 | Loc: [1, 6] | Dir: 2\n",
      "Ep 2300/5000 | Time: 1525.1s | Avg Reward: -2.50 | Scout: 4.95 | Guard: 3.41 | Scout Captures: 362 | Scout Collections: 535 | Guard Captures: 343 | Epsilon (S/G): 0.234/0.059\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 2, Progress: 1309/2000\n",
      "Scout count: 654, Guard count: 655\n",
      "Scout ratio: 0.5, Expected guard count: 654.5\n",
      "Phase 2, Progress: 1319/2000\n",
      "Scout count: 661, Guard count: 658\n",
      "Scout ratio: 0.5, Expected guard count: 659.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1329/2000\n",
      "Scout count: 664, Guard count: 665\n",
      "Scout ratio: 0.5, Expected guard count: 664.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1339/2000\n",
      "Scout count: 667, Guard count: 672\n",
      "Scout ratio: 0.5, Expected guard count: 669.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1349/2000\n",
      "Scout count: 674, Guard count: 675\n",
      "Scout ratio: 0.5, Expected guard count: 674.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1359/2000\n",
      "Scout count: 680, Guard count: 679\n",
      "Scout ratio: 0.5, Expected guard count: 679.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1369/2000\n",
      "Scout count: 687, Guard count: 682\n",
      "Scout ratio: 0.5, Expected guard count: 684.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1379/2000\n",
      "Scout count: 693, Guard count: 686\n",
      "Scout ratio: 0.5, Expected guard count: 689.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1389/2000\n",
      "Scout count: 695, Guard count: 694\n",
      "Scout ratio: 0.5, Expected guard count: 694.5\n",
      "Phase 2, Progress: 1399/2000\n",
      "Scout count: 702, Guard count: 697\n",
      "Scout ratio: 0.5, Expected guard count: 699.5\n",
      "[Ep 2400] Role: 0 | Loc: [12, 11] | Dir: 0\n",
      "Ep 2400/5000 | Time: 1609.4s | Avg Reward: 0.15 | Scout: 2.31 | Guard: 4.66 | Scout Captures: 378 | Scout Collections: 558 | Guard Captures: 360 | Epsilon (S/G): 0.232/0.057\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1409/2000\n",
      "Scout count: 705, Guard count: 704\n",
      "Scout ratio: 0.5, Expected guard count: 704.5\n",
      "Phase 2, Progress: 1419/2000\n",
      "Scout count: 708, Guard count: 711\n",
      "Scout ratio: 0.5, Expected guard count: 709.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1429/2000\n",
      "Scout count: 712, Guard count: 717\n",
      "Scout ratio: 0.5, Expected guard count: 714.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1439/2000\n",
      "Scout count: 717, Guard count: 722\n",
      "Scout ratio: 0.5, Expected guard count: 719.5\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1449/2000\n",
      "Scout count: 720, Guard count: 729\n",
      "Scout ratio: 0.5, Expected guard count: 724.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1459/2000\n",
      "Scout count: 725, Guard count: 734\n",
      "Scout ratio: 0.5, Expected guard count: 729.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1250\n",
      "Phase 2, Progress: 1469/2000\n",
      "Scout count: 729, Guard count: 740\n",
      "Scout ratio: 0.5, Expected guard count: 734.5\n",
      "Using historical opponent from episode 1250\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1479/2000\n",
      "Scout count: 731, Guard count: 748\n",
      "Scout ratio: 0.5, Expected guard count: 739.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1489/2000\n",
      "Scout count: 737, Guard count: 752\n",
      "Scout ratio: 0.5, Expected guard count: 744.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1500\n",
      "Stored historical models at episode 2500\n",
      "Phase 2, Progress: 1499/2000\n",
      "Scout count: 740, Guard count: 759\n",
      "Scout ratio: 0.5, Expected guard count: 749.5\n",
      "[Ep 2500] Role: 0 | Loc: [3, 15] | Dir: 0\n",
      "Ep 2500/5000 | Time: 1699.4s | Avg Reward: 4.61 | Scout: 2.66 | Guard: -0.96 | Scout Captures: 388 | Scout Collections: 576 | Guard Captures: 382 | Epsilon (S/G): 0.230/0.056\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -9.88, Scout: -8.47, Guard: -11.30\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -5.92\n",
      "Combined Evaluation Score: -9.09\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1509/2000\n",
      "Scout count: 745, Guard count: 764\n",
      "Scout ratio: 0.5, Expected guard count: 754.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1519/2000\n",
      "Scout count: 751, Guard count: 768\n",
      "Scout ratio: 0.5, Expected guard count: 759.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1529/2000\n",
      "Scout count: 758, Guard count: 771\n",
      "Scout ratio: 0.5, Expected guard count: 764.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2500\n",
      "Phase 2, Progress: 1539/2000\n",
      "Scout count: 763, Guard count: 776\n",
      "Scout ratio: 0.5, Expected guard count: 769.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1549/2000\n",
      "Scout count: 767, Guard count: 782\n",
      "Scout ratio: 0.5, Expected guard count: 774.5\n",
      "Phase 2, Progress: 1559/2000\n",
      "Scout count: 771, Guard count: 788\n",
      "Scout ratio: 0.5, Expected guard count: 779.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1569/2000\n",
      "Scout count: 776, Guard count: 793\n",
      "Scout ratio: 0.5, Expected guard count: 784.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1579/2000\n",
      "Scout count: 779, Guard count: 800\n",
      "Scout ratio: 0.5, Expected guard count: 789.5\n",
      "Using historical opponent from episode 1500\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1589/2000\n",
      "Scout count: 783, Guard count: 806\n",
      "Scout ratio: 0.5, Expected guard count: 794.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1599/2000\n",
      "Scout count: 785, Guard count: 814\n",
      "Scout ratio: 0.5, Expected guard count: 799.5\n",
      "[Ep 2600] Role: 0 | Loc: [3, 9] | Dir: 2\n",
      "Ep 2600/5000 | Time: 1795.6s | Avg Reward: 4.64 | Scout: 12.79 | Guard: -0.25 | Scout Captures: 396 | Scout Collections: 590 | Guard Captures: 398 | Epsilon (S/G): 0.228/0.054\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Phase 2, Progress: 1609/2000\n",
      "Scout count: 789, Guard count: 820\n",
      "Scout ratio: 0.5, Expected guard count: 804.5\n",
      "Phase 2, Progress: 1619/2000\n",
      "Scout count: 794, Guard count: 825\n",
      "Scout ratio: 0.5, Expected guard count: 809.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1629/2000\n",
      "Scout count: 800, Guard count: 829\n",
      "Scout ratio: 0.5, Expected guard count: 814.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1639/2000\n",
      "Scout count: 807, Guard count: 832\n",
      "Scout ratio: 0.5, Expected guard count: 819.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1649/2000\n",
      "Scout count: 812, Guard count: 837\n",
      "Scout ratio: 0.5, Expected guard count: 824.5\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1659/2000\n",
      "Scout count: 816, Guard count: 843\n",
      "Scout ratio: 0.5, Expected guard count: 829.5\n",
      "Using historical opponent from episode 1500\n",
      "Phase 2, Progress: 1669/2000\n",
      "Scout count: 821, Guard count: 848\n",
      "Scout ratio: 0.5, Expected guard count: 834.5\n",
      "Phase 2, Progress: 1679/2000\n",
      "Scout count: 826, Guard count: 853\n",
      "Scout ratio: 0.5, Expected guard count: 839.5\n",
      "Phase 2, Progress: 1689/2000\n",
      "Scout count: 831, Guard count: 858\n",
      "Scout ratio: 0.5, Expected guard count: 844.5\n",
      "Phase 2, Progress: 1699/2000\n",
      "Scout count: 838, Guard count: 861\n",
      "Scout ratio: 0.5, Expected guard count: 849.5\n",
      "[Ep 2700] Role: 0 | Loc: [7, 11] | Dir: 1\n",
      "Ep 2700/5000 | Time: 1896.3s | Avg Reward: 3.17 | Scout: 2.66 | Guard: 4.21 | Scout Captures: 414 | Scout Collections: 616 | Guard Captures: 414 | Epsilon (S/G): 0.225/0.053\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1709/2000\n",
      "Scout count: 846, Guard count: 863\n",
      "Scout ratio: 0.5, Expected guard count: 854.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1719/2000\n",
      "Scout count: 852, Guard count: 867\n",
      "Scout ratio: 0.5, Expected guard count: 859.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1729/2000\n",
      "Scout count: 857, Guard count: 872\n",
      "Scout ratio: 0.5, Expected guard count: 864.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1739/2000\n",
      "Scout count: 863, Guard count: 876\n",
      "Scout ratio: 0.5, Expected guard count: 869.5\n",
      "Using historical opponent from episode 2250\n",
      "Stored historical models at episode 2750\n",
      "Phase 2, Progress: 1749/2000\n",
      "Scout count: 867, Guard count: 882\n",
      "Scout ratio: 0.5, Expected guard count: 874.5\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 9.78, Scout: 19.59, Guard: -0.04\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -0.15\n",
      "Combined Evaluation Score: 7.79\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: 7.79\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1759/2000\n",
      "Scout count: 874, Guard count: 885\n",
      "Scout ratio: 0.5, Expected guard count: 879.5\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1769/2000\n",
      "Scout count: 878, Guard count: 891\n",
      "Scout ratio: 0.5, Expected guard count: 884.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1779/2000\n",
      "Scout count: 881, Guard count: 898\n",
      "Scout ratio: 0.5, Expected guard count: 889.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2500\n",
      "Phase 2, Progress: 1789/2000\n",
      "Scout count: 886, Guard count: 903\n",
      "Scout ratio: 0.5, Expected guard count: 894.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1799/2000\n",
      "Scout count: 890, Guard count: 909\n",
      "Scout ratio: 0.5, Expected guard count: 899.5\n",
      "[Ep 2800] Role: 1 | Loc: [8, 13] | Dir: 0\n",
      "Ep 2800/5000 | Time: 1996.1s | Avg Reward: 0.81 | Scout: 1.33 | Guard: 3.93 | Scout Captures: 434 | Scout Collections: 639 | Guard Captures: 430 | Epsilon (S/G): 0.223/0.052\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1809/2000\n",
      "Scout count: 897, Guard count: 912\n",
      "Scout ratio: 0.5, Expected guard count: 904.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2750\n",
      "Phase 2, Progress: 1819/2000\n",
      "Scout count: 901, Guard count: 918\n",
      "Scout ratio: 0.5, Expected guard count: 909.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1829/2000\n",
      "Scout count: 905, Guard count: 924\n",
      "Scout ratio: 0.5, Expected guard count: 914.5\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1839/2000\n",
      "Scout count: 911, Guard count: 928\n",
      "Scout ratio: 0.5, Expected guard count: 919.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1849/2000\n",
      "Scout count: 916, Guard count: 933\n",
      "Scout ratio: 0.5, Expected guard count: 924.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2500\n",
      "Phase 2, Progress: 1859/2000\n",
      "Scout count: 918, Guard count: 941\n",
      "Scout ratio: 0.5, Expected guard count: 929.5\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1869/2000\n",
      "Scout count: 921, Guard count: 948\n",
      "Scout ratio: 0.5, Expected guard count: 934.5\n",
      "Using historical opponent from episode 2750\n",
      "Phase 2, Progress: 1879/2000\n",
      "Scout count: 924, Guard count: 955\n",
      "Scout ratio: 0.5, Expected guard count: 939.5\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2750\n",
      "Phase 2, Progress: 1889/2000\n",
      "Scout count: 928, Guard count: 961\n",
      "Scout ratio: 0.5, Expected guard count: 944.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1899/2000\n",
      "Scout count: 936, Guard count: 963\n",
      "Scout ratio: 0.5, Expected guard count: 949.5\n",
      "[Ep 2900] Role: 0 | Loc: [10, 8] | Dir: 0\n",
      "Ep 2900/5000 | Time: 2094.5s | Avg Reward: 4.51 | Scout: 0.47 | Guard: 7.30 | Scout Captures: 449 | Scout Collections: 659 | Guard Captures: 453 | Epsilon (S/G): 0.221/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 1750\n",
      "Phase 2, Progress: 1909/2000\n",
      "Scout count: 940, Guard count: 969\n",
      "Scout ratio: 0.5, Expected guard count: 954.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1919/2000\n",
      "Scout count: 947, Guard count: 972\n",
      "Scout ratio: 0.5, Expected guard count: 959.5\n",
      "Using historical opponent from episode 1750\n",
      "Using historical opponent from episode 2250\n",
      "Phase 2, Progress: 1929/2000\n",
      "Scout count: 953, Guard count: 976\n",
      "Scout ratio: 0.5, Expected guard count: 964.5\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1939/2000\n",
      "Scout count: 960, Guard count: 979\n",
      "Scout ratio: 0.5, Expected guard count: 969.5\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1949/2000\n",
      "Scout count: 967, Guard count: 982\n",
      "Scout ratio: 0.5, Expected guard count: 974.5\n",
      "Phase 2, Progress: 1959/2000\n",
      "Scout count: 972, Guard count: 987\n",
      "Scout ratio: 0.5, Expected guard count: 979.5\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 2500\n",
      "Phase 2, Progress: 1969/2000\n",
      "Scout count: 978, Guard count: 991\n",
      "Scout ratio: 0.5, Expected guard count: 984.5\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Phase 2, Progress: 1979/2000\n",
      "Scout count: 982, Guard count: 997\n",
      "Scout ratio: 0.5, Expected guard count: 989.5\n",
      "Using historical opponent from episode 2750\n",
      "Phase 2, Progress: 1989/2000\n",
      "Scout count: 989, Guard count: 1000\n",
      "Scout ratio: 0.5, Expected guard count: 994.5\n",
      "Using historical opponent from episode 1750\n",
      "Stored historical models at episode 3000\n",
      "Phase 2, Progress: 1999/2000\n",
      "Scout count: 993, Guard count: 1006\n",
      "Scout ratio: 0.5, Expected guard count: 999.5\n",
      "[Ep 3000] Role: 0 | Loc: [6, 11] | Dir: 3\n",
      "Ep 3000/5000 | Time: 2192.0s | Avg Reward: 1.18 | Scout: 4.49 | Guard: 4.66 | Scout Captures: 464 | Scout Collections: 681 | Guard Captures: 469 | Epsilon (S/G): 0.219/0.050\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 9.36, Scout: 19.73, Guard: -1.02\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -15.15\n",
      "Combined Evaluation Score: 4.45\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Moving to curriculum phase 3\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 9/2000\n",
      "Scout count: 4, Guard count: 5\n",
      "Scout ratio: 0.4, Expected guard count: 5.4\n",
      "Phase 3, Progress: 19/2000\n",
      "Scout count: 5, Guard count: 14\n",
      "Scout ratio: 0.4, Expected guard count: 11.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 29/2000\n",
      "Scout count: 7, Guard count: 22\n",
      "Scout ratio: 0.4, Expected guard count: 17.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 39/2000\n",
      "Scout count: 9, Guard count: 30\n",
      "Scout ratio: 0.4, Expected guard count: 23.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2000\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2000\n",
      "Phase 3, Progress: 49/2000\n",
      "Scout count: 15, Guard count: 34\n",
      "Scout ratio: 0.4, Expected guard count: 29.4\n",
      "Using historical opponent from episode 2000\n",
      "Phase 3, Progress: 59/2000\n",
      "Scout count: 21, Guard count: 38\n",
      "Scout ratio: 0.4, Expected guard count: 35.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 69/2000\n",
      "Scout count: 24, Guard count: 45\n",
      "Scout ratio: 0.4, Expected guard count: 41.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 79/2000\n",
      "Scout count: 28, Guard count: 51\n",
      "Scout ratio: 0.4, Expected guard count: 47.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 89/2000\n",
      "Scout count: 30, Guard count: 59\n",
      "Scout ratio: 0.4, Expected guard count: 53.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 99/2000\n",
      "Scout count: 34, Guard count: 65\n",
      "Scout ratio: 0.4, Expected guard count: 59.4\n",
      "[Ep 3100] Role: 1 | Loc: [9, 13] | Dir: 0\n",
      "Ep 3100/5000 | Time: 2282.2s | Avg Reward: 4.52 | Scout: 2.24 | Guard: 8.71 | Scout Captures: 477 | Scout Collections: 694 | Guard Captures: 494 | Epsilon (S/G): 0.217/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 109/2000\n",
      "Scout count: 38, Guard count: 71\n",
      "Scout ratio: 0.4, Expected guard count: 65.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 119/2000\n",
      "Scout count: 45, Guard count: 74\n",
      "Scout ratio: 0.4, Expected guard count: 71.4\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 129/2000\n",
      "Scout count: 49, Guard count: 80\n",
      "Scout ratio: 0.4, Expected guard count: 77.4\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 139/2000\n",
      "Scout count: 54, Guard count: 85\n",
      "Scout ratio: 0.4, Expected guard count: 83.4\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 149/2000\n",
      "Scout count: 57, Guard count: 92\n",
      "Scout ratio: 0.4, Expected guard count: 89.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 159/2000\n",
      "Scout count: 63, Guard count: 96\n",
      "Scout ratio: 0.4, Expected guard count: 95.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 169/2000\n",
      "Scout count: 67, Guard count: 102\n",
      "Scout ratio: 0.4, Expected guard count: 101.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 179/2000\n",
      "Scout count: 69, Guard count: 110\n",
      "Scout ratio: 0.4, Expected guard count: 107.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 189/2000\n",
      "Scout count: 73, Guard count: 116\n",
      "Scout ratio: 0.4, Expected guard count: 113.4\n",
      "Phase 3, Progress: 199/2000\n",
      "Scout count: 76, Guard count: 123\n",
      "Scout ratio: 0.4, Expected guard count: 119.4\n",
      "[Ep 3200] Role: 1 | Loc: [13, 3] | Dir: 0\n",
      "Ep 3200/5000 | Time: 2381.5s | Avg Reward: 9.93 | Scout: -1.21 | Guard: 10.18 | Scout Captures: 493 | Scout Collections: 718 | Guard Captures: 516 | Epsilon (S/G): 0.215/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 209/2000\n",
      "Scout count: 81, Guard count: 128\n",
      "Scout ratio: 0.4, Expected guard count: 125.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 219/2000\n",
      "Scout count: 84, Guard count: 135\n",
      "Scout ratio: 0.4, Expected guard count: 131.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 229/2000\n",
      "Scout count: 87, Guard count: 142\n",
      "Scout ratio: 0.4, Expected guard count: 137.4\n",
      "Phase 3, Progress: 239/2000\n",
      "Scout count: 95, Guard count: 144\n",
      "Scout ratio: 0.4, Expected guard count: 143.4\n",
      "Using historical opponent from episode 2250\n",
      "Stored historical models at episode 3250\n",
      "Phase 3, Progress: 249/2000\n",
      "Scout count: 98, Guard count: 151\n",
      "Scout ratio: 0.4, Expected guard count: 149.4\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 1.06, Scout: -17.41, Guard: 19.54\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 8.80\n",
      "Combined Evaluation Score: 2.61\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 259/2000\n",
      "Scout count: 102, Guard count: 157\n",
      "Scout ratio: 0.4, Expected guard count: 155.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 269/2000\n",
      "Scout count: 105, Guard count: 164\n",
      "Scout ratio: 0.4, Expected guard count: 161.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 279/2000\n",
      "Scout count: 107, Guard count: 172\n",
      "Scout ratio: 0.4, Expected guard count: 167.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 289/2000\n",
      "Scout count: 109, Guard count: 180\n",
      "Scout ratio: 0.4, Expected guard count: 173.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 299/2000\n",
      "Scout count: 112, Guard count: 187\n",
      "Scout ratio: 0.4, Expected guard count: 179.4\n",
      "[Ep 3300] Role: 1 | Loc: [12, 2] | Dir: 1\n",
      "Ep 3300/5000 | Time: 2481.2s | Avg Reward: 7.96 | Scout: 4.55 | Guard: 0.83 | Scout Captures: 503 | Scout Collections: 736 | Guard Captures: 538 | Epsilon (S/G): 0.214/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 309/2000\n",
      "Scout count: 117, Guard count: 192\n",
      "Scout ratio: 0.4, Expected guard count: 185.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 319/2000\n",
      "Scout count: 121, Guard count: 198\n",
      "Scout ratio: 0.4, Expected guard count: 191.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 329/2000\n",
      "Scout count: 124, Guard count: 205\n",
      "Scout ratio: 0.4, Expected guard count: 197.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 339/2000\n",
      "Scout count: 125, Guard count: 214\n",
      "Scout ratio: 0.4, Expected guard count: 203.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 349/2000\n",
      "Scout count: 132, Guard count: 217\n",
      "Scout ratio: 0.4, Expected guard count: 209.4\n",
      "Phase 3, Progress: 359/2000\n",
      "Scout count: 137, Guard count: 222\n",
      "Scout ratio: 0.4, Expected guard count: 215.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 369/2000\n",
      "Scout count: 141, Guard count: 228\n",
      "Scout ratio: 0.4, Expected guard count: 221.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 379/2000\n",
      "Scout count: 148, Guard count: 231\n",
      "Scout ratio: 0.4, Expected guard count: 227.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 389/2000\n",
      "Scout count: 151, Guard count: 238\n",
      "Scout ratio: 0.4, Expected guard count: 233.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 399/2000\n",
      "Scout count: 154, Guard count: 245\n",
      "Scout ratio: 0.4, Expected guard count: 239.4\n",
      "[Ep 3400] Role: 0 | Loc: [9, 9] | Dir: 2\n",
      "Ep 3400/5000 | Time: 2579.7s | Avg Reward: -0.32 | Scout: -0.75 | Guard: -1.46 | Scout Captures: 516 | Scout Collections: 761 | Guard Captures: 558 | Epsilon (S/G): 0.212/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 409/2000\n",
      "Scout count: 157, Guard count: 252\n",
      "Scout ratio: 0.4, Expected guard count: 245.4\n",
      "Phase 3, Progress: 419/2000\n",
      "Scout count: 163, Guard count: 256\n",
      "Scout ratio: 0.4, Expected guard count: 251.4\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 429/2000\n",
      "Scout count: 166, Guard count: 263\n",
      "Scout ratio: 0.4, Expected guard count: 257.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 439/2000\n",
      "Scout count: 167, Guard count: 272\n",
      "Scout ratio: 0.4, Expected guard count: 263.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2250\n",
      "Phase 3, Progress: 449/2000\n",
      "Scout count: 171, Guard count: 278\n",
      "Scout ratio: 0.4, Expected guard count: 269.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 459/2000\n",
      "Scout count: 176, Guard count: 283\n",
      "Scout ratio: 0.4, Expected guard count: 275.4\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 469/2000\n",
      "Scout count: 181, Guard count: 288\n",
      "Scout ratio: 0.4, Expected guard count: 281.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2250\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 479/2000\n",
      "Scout count: 188, Guard count: 291\n",
      "Scout ratio: 0.4, Expected guard count: 287.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 489/2000\n",
      "Scout count: 190, Guard count: 299\n",
      "Scout ratio: 0.4, Expected guard count: 293.4\n",
      "Using historical opponent from episode 3000\n",
      "Stored historical models at episode 3500\n",
      "Phase 3, Progress: 499/2000\n",
      "Scout count: 194, Guard count: 305\n",
      "Scout ratio: 0.4, Expected guard count: 299.4\n",
      "[Ep 3500] Role: 1 | Loc: [15, 9] | Dir: 0\n",
      "Ep 3500/5000 | Time: 2661.6s | Avg Reward: 2.47 | Scout: -9.26 | Guard: 11.58 | Scout Captures: 536 | Scout Collections: 784 | Guard Captures: 583 | Epsilon (S/G): 0.210/0.050\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 26.26, Scout: 31.88, Guard: 20.63\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -21.67\n",
      "Combined Evaluation Score: 16.67\n",
      "Safetensors model saved to ./model/best_model/model.safetensors\n",
      "New best model saved! Combined Reward: 16.67\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 509/2000\n",
      "Scout count: 197, Guard count: 312\n",
      "Scout ratio: 0.4, Expected guard count: 305.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 519/2000\n",
      "Scout count: 199, Guard count: 320\n",
      "Scout ratio: 0.4, Expected guard count: 311.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 529/2000\n",
      "Scout count: 204, Guard count: 325\n",
      "Scout ratio: 0.4, Expected guard count: 317.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 539/2000\n",
      "Scout count: 208, Guard count: 331\n",
      "Scout ratio: 0.4, Expected guard count: 323.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 549/2000\n",
      "Scout count: 211, Guard count: 338\n",
      "Scout ratio: 0.4, Expected guard count: 329.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 559/2000\n",
      "Scout count: 214, Guard count: 345\n",
      "Scout ratio: 0.4, Expected guard count: 335.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 569/2000\n",
      "Scout count: 219, Guard count: 350\n",
      "Scout ratio: 0.4, Expected guard count: 341.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 579/2000\n",
      "Scout count: 223, Guard count: 356\n",
      "Scout ratio: 0.4, Expected guard count: 347.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 589/2000\n",
      "Scout count: 226, Guard count: 363\n",
      "Scout ratio: 0.4, Expected guard count: 353.4\n",
      "Phase 3, Progress: 599/2000\n",
      "Scout count: 230, Guard count: 369\n",
      "Scout ratio: 0.4, Expected guard count: 359.4\n",
      "[Ep 3600] Role: 1 | Loc: [0, 6] | Dir: 1\n",
      "Ep 3600/5000 | Time: 2758.8s | Avg Reward: -8.53 | Scout: 2.07 | Guard: -4.98 | Scout Captures: 546 | Scout Collections: 801 | Guard Captures: 601 | Epsilon (S/G): 0.209/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 609/2000\n",
      "Scout count: 235, Guard count: 374\n",
      "Scout ratio: 0.4, Expected guard count: 365.4\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 619/2000\n",
      "Scout count: 242, Guard count: 377\n",
      "Scout ratio: 0.4, Expected guard count: 371.4\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 629/2000\n",
      "Scout count: 249, Guard count: 380\n",
      "Scout ratio: 0.4, Expected guard count: 377.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 639/2000\n",
      "Scout count: 253, Guard count: 386\n",
      "Scout ratio: 0.4, Expected guard count: 383.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 649/2000\n",
      "Scout count: 256, Guard count: 393\n",
      "Scout ratio: 0.4, Expected guard count: 389.4\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 659/2000\n",
      "Scout count: 261, Guard count: 398\n",
      "Scout ratio: 0.4, Expected guard count: 395.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2500\n",
      "Phase 3, Progress: 669/2000\n",
      "Scout count: 263, Guard count: 406\n",
      "Scout ratio: 0.4, Expected guard count: 401.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 679/2000\n",
      "Scout count: 265, Guard count: 414\n",
      "Scout ratio: 0.4, Expected guard count: 407.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 689/2000\n",
      "Scout count: 270, Guard count: 419\n",
      "Scout ratio: 0.4, Expected guard count: 413.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2500\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 699/2000\n",
      "Scout count: 273, Guard count: 426\n",
      "Scout ratio: 0.4, Expected guard count: 419.4\n",
      "[Ep 3700] Role: 0 | Loc: [4, 3] | Dir: 3\n",
      "Ep 3700/5000 | Time: 2855.6s | Avg Reward: 6.62 | Scout: 10.35 | Guard: -3.96 | Scout Captures: 553 | Scout Collections: 817 | Guard Captures: 617 | Epsilon (S/G): 0.207/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 709/2000\n",
      "Scout count: 278, Guard count: 431\n",
      "Scout ratio: 0.4, Expected guard count: 425.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 719/2000\n",
      "Scout count: 281, Guard count: 438\n",
      "Scout ratio: 0.4, Expected guard count: 431.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 729/2000\n",
      "Scout count: 289, Guard count: 440\n",
      "Scout ratio: 0.4, Expected guard count: 437.4\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 739/2000\n",
      "Scout count: 293, Guard count: 446\n",
      "Scout ratio: 0.4, Expected guard count: 443.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Stored historical models at episode 3750\n",
      "Phase 3, Progress: 749/2000\n",
      "Scout count: 297, Guard count: 452\n",
      "Scout ratio: 0.4, Expected guard count: 449.4\n",
      "Using historical opponent from episode 2750\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -4.17, Scout: 10.07, Guard: -18.42\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -8.21\n",
      "Combined Evaluation Score: -4.98\n",
      "Phase 3, Progress: 759/2000\n",
      "Scout count: 298, Guard count: 461\n",
      "Scout ratio: 0.4, Expected guard count: 455.4\n",
      "Phase 3, Progress: 769/2000\n",
      "Scout count: 302, Guard count: 467\n",
      "Scout ratio: 0.4, Expected guard count: 461.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 779/2000\n",
      "Scout count: 308, Guard count: 471\n",
      "Scout ratio: 0.4, Expected guard count: 467.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 789/2000\n",
      "Scout count: 314, Guard count: 475\n",
      "Scout ratio: 0.4, Expected guard count: 473.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 799/2000\n",
      "Scout count: 322, Guard count: 477\n",
      "Scout ratio: 0.4, Expected guard count: 479.4\n",
      "Using historical opponent from episode 3000\n",
      "[Ep 3800] Role: 1 | Loc: [14, 0] | Dir: 3\n",
      "Ep 3800/5000 | Time: 2948.8s | Avg Reward: 3.09 | Scout: -1.62 | Guard: 9.36 | Scout Captures: 569 | Scout Collections: 842 | Guard Captures: 637 | Epsilon (S/G): 0.205/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 809/2000\n",
      "Scout count: 325, Guard count: 484\n",
      "Scout ratio: 0.4, Expected guard count: 485.4\n",
      "Phase 3, Progress: 819/2000\n",
      "Scout count: 327, Guard count: 492\n",
      "Scout ratio: 0.4, Expected guard count: 491.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 829/2000\n",
      "Scout count: 333, Guard count: 496\n",
      "Scout ratio: 0.4, Expected guard count: 497.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 839/2000\n",
      "Scout count: 336, Guard count: 503\n",
      "Scout ratio: 0.4, Expected guard count: 503.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 849/2000\n",
      "Scout count: 341, Guard count: 508\n",
      "Scout ratio: 0.4, Expected guard count: 509.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 859/2000\n",
      "Scout count: 342, Guard count: 517\n",
      "Scout ratio: 0.4, Expected guard count: 515.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 869/2000\n",
      "Scout count: 345, Guard count: 524\n",
      "Scout ratio: 0.4, Expected guard count: 521.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 879/2000\n",
      "Scout count: 347, Guard count: 532\n",
      "Scout ratio: 0.4, Expected guard count: 527.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 889/2000\n",
      "Scout count: 351, Guard count: 538\n",
      "Scout ratio: 0.4, Expected guard count: 533.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 899/2000\n",
      "Scout count: 354, Guard count: 545\n",
      "Scout ratio: 0.4, Expected guard count: 539.4\n",
      "[Ep 3900] Role: 0 | Loc: [6, 1] | Dir: 0\n",
      "Ep 3900/5000 | Time: 3047.7s | Avg Reward: -6.84 | Scout: -6.61 | Guard: 1.42 | Scout Captures: 582 | Scout Collections: 859 | Guard Captures: 661 | Epsilon (S/G): 0.204/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 909/2000\n",
      "Scout count: 357, Guard count: 552\n",
      "Scout ratio: 0.4, Expected guard count: 545.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 919/2000\n",
      "Scout count: 362, Guard count: 557\n",
      "Scout ratio: 0.4, Expected guard count: 551.4\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 929/2000\n",
      "Scout count: 366, Guard count: 563\n",
      "Scout ratio: 0.4, Expected guard count: 557.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 939/2000\n",
      "Scout count: 372, Guard count: 567\n",
      "Scout ratio: 0.4, Expected guard count: 563.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 949/2000\n",
      "Scout count: 377, Guard count: 572\n",
      "Scout ratio: 0.4, Expected guard count: 569.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 2750\n",
      "Using historical opponent from episode 2750\n",
      "Phase 3, Progress: 959/2000\n",
      "Scout count: 383, Guard count: 576\n",
      "Scout ratio: 0.4, Expected guard count: 575.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 969/2000\n",
      "Scout count: 384, Guard count: 585\n",
      "Scout ratio: 0.4, Expected guard count: 581.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 979/2000\n",
      "Scout count: 389, Guard count: 590\n",
      "Scout ratio: 0.4, Expected guard count: 587.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 989/2000\n",
      "Scout count: 393, Guard count: 596\n",
      "Scout ratio: 0.4, Expected guard count: 593.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Stored historical models at episode 4000\n",
      "Phase 3, Progress: 999/2000\n",
      "Scout count: 396, Guard count: 603\n",
      "Scout ratio: 0.4, Expected guard count: 599.4\n",
      "[Ep 4000] Role: 0 | Loc: [12, 7] | Dir: 2\n",
      "Ep 4000/5000 | Time: 3144.8s | Avg Reward: 7.39 | Scout: -2.87 | Guard: 5.42 | Scout Captures: 598 | Scout Collections: 878 | Guard Captures: 682 | Epsilon (S/G): 0.202/0.050\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -20.17, Scout: -40.28, Guard: -0.06\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -5.93\n",
      "Combined Evaluation Score: -17.32\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1009/2000\n",
      "Scout count: 399, Guard count: 610\n",
      "Scout ratio: 0.4, Expected guard count: 605.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 1019/2000\n",
      "Scout count: 406, Guard count: 613\n",
      "Scout ratio: 0.4, Expected guard count: 611.4\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 1029/2000\n",
      "Scout count: 411, Guard count: 618\n",
      "Scout ratio: 0.4, Expected guard count: 617.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1039/2000\n",
      "Scout count: 414, Guard count: 625\n",
      "Scout ratio: 0.4, Expected guard count: 623.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1049/2000\n",
      "Scout count: 419, Guard count: 630\n",
      "Scout ratio: 0.4, Expected guard count: 629.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1059/2000\n",
      "Scout count: 423, Guard count: 636\n",
      "Scout ratio: 0.4, Expected guard count: 635.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1069/2000\n",
      "Scout count: 425, Guard count: 644\n",
      "Scout ratio: 0.4, Expected guard count: 641.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1079/2000\n",
      "Scout count: 430, Guard count: 649\n",
      "Scout ratio: 0.4, Expected guard count: 647.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1089/2000\n",
      "Scout count: 434, Guard count: 655\n",
      "Scout ratio: 0.4, Expected guard count: 653.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1099/2000\n",
      "Scout count: 436, Guard count: 663\n",
      "Scout ratio: 0.4, Expected guard count: 659.4\n",
      "[Ep 4100] Role: 0 | Loc: [0, 0] | Dir: 3\n",
      "Ep 4100/5000 | Time: 3238.2s | Avg Reward: 7.60 | Scout: 4.12 | Guard: 5.07 | Scout Captures: 610 | Scout Collections: 894 | Guard Captures: 704 | Epsilon (S/G): 0.200/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1109/2000\n",
      "Scout count: 440, Guard count: 669\n",
      "Scout ratio: 0.4, Expected guard count: 665.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1119/2000\n",
      "Scout count: 444, Guard count: 675\n",
      "Scout ratio: 0.4, Expected guard count: 671.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1129/2000\n",
      "Scout count: 447, Guard count: 682\n",
      "Scout ratio: 0.4, Expected guard count: 677.4\n",
      "Phase 3, Progress: 1139/2000\n",
      "Scout count: 452, Guard count: 687\n",
      "Scout ratio: 0.4, Expected guard count: 683.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1149/2000\n",
      "Scout count: 459, Guard count: 690\n",
      "Scout ratio: 0.4, Expected guard count: 689.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 1159/2000\n",
      "Scout count: 464, Guard count: 695\n",
      "Scout ratio: 0.4, Expected guard count: 695.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1169/2000\n",
      "Scout count: 467, Guard count: 702\n",
      "Scout ratio: 0.4, Expected guard count: 701.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1179/2000\n",
      "Scout count: 472, Guard count: 707\n",
      "Scout ratio: 0.4, Expected guard count: 707.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3000\n",
      "Phase 3, Progress: 1189/2000\n",
      "Scout count: 477, Guard count: 712\n",
      "Scout ratio: 0.4, Expected guard count: 713.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1199/2000\n",
      "Scout count: 483, Guard count: 716\n",
      "Scout ratio: 0.4, Expected guard count: 719.4\n",
      "[Ep 4200] Role: 0 | Loc: [15, 4] | Dir: 1\n",
      "Ep 4200/5000 | Time: 3327.5s | Avg Reward: 3.44 | Scout: 5.64 | Guard: -0.10 | Scout Captures: 624 | Scout Collections: 914 | Guard Captures: 718 | Epsilon (S/G): 0.198/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1209/2000\n",
      "Scout count: 487, Guard count: 722\n",
      "Scout ratio: 0.4, Expected guard count: 725.4\n",
      "Using historical opponent from episode 3000\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1219/2000\n",
      "Scout count: 491, Guard count: 728\n",
      "Scout ratio: 0.4, Expected guard count: 731.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1229/2000\n",
      "Scout count: 496, Guard count: 733\n",
      "Scout ratio: 0.4, Expected guard count: 737.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1239/2000\n",
      "Scout count: 500, Guard count: 739\n",
      "Scout ratio: 0.4, Expected guard count: 743.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3500\n",
      "Stored historical models at episode 4250\n",
      "Phase 3, Progress: 1249/2000\n",
      "Scout count: 502, Guard count: 747\n",
      "Scout ratio: 0.4, Expected guard count: 749.4\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 2.05, Scout: -8.51, Guard: 12.60\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 0.67\n",
      "Combined Evaluation Score: 1.77\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1259/2000\n",
      "Scout count: 504, Guard count: 755\n",
      "Scout ratio: 0.4, Expected guard count: 755.4\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1269/2000\n",
      "Scout count: 509, Guard count: 760\n",
      "Scout ratio: 0.4, Expected guard count: 761.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1279/2000\n",
      "Scout count: 514, Guard count: 765\n",
      "Scout ratio: 0.4, Expected guard count: 767.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1289/2000\n",
      "Scout count: 516, Guard count: 773\n",
      "Scout ratio: 0.4, Expected guard count: 773.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1299/2000\n",
      "Scout count: 520, Guard count: 779\n",
      "Scout ratio: 0.4, Expected guard count: 779.4\n",
      "[Ep 4300] Role: 1 | Loc: [0, 14] | Dir: 3\n",
      "Ep 4300/5000 | Time: 3421.4s | Avg Reward: -4.52 | Scout: -5.54 | Guard: 5.15 | Scout Captures: 641 | Scout Collections: 934 | Guard Captures: 740 | Epsilon (S/G): 0.197/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1309/2000\n",
      "Scout count: 524, Guard count: 785\n",
      "Scout ratio: 0.4, Expected guard count: 785.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1319/2000\n",
      "Scout count: 526, Guard count: 793\n",
      "Scout ratio: 0.4, Expected guard count: 791.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1329/2000\n",
      "Scout count: 530, Guard count: 799\n",
      "Scout ratio: 0.4, Expected guard count: 797.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1339/2000\n",
      "Scout count: 537, Guard count: 802\n",
      "Scout ratio: 0.4, Expected guard count: 803.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1349/2000\n",
      "Scout count: 542, Guard count: 807\n",
      "Scout ratio: 0.4, Expected guard count: 809.4\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1359/2000\n",
      "Scout count: 546, Guard count: 813\n",
      "Scout ratio: 0.4, Expected guard count: 815.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1369/2000\n",
      "Scout count: 551, Guard count: 818\n",
      "Scout ratio: 0.4, Expected guard count: 821.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1379/2000\n",
      "Scout count: 556, Guard count: 823\n",
      "Scout ratio: 0.4, Expected guard count: 827.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1389/2000\n",
      "Scout count: 560, Guard count: 829\n",
      "Scout ratio: 0.4, Expected guard count: 833.4\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1399/2000\n",
      "Scout count: 562, Guard count: 837\n",
      "Scout ratio: 0.4, Expected guard count: 839.4\n",
      "[Ep 4400] Role: 1 | Loc: [10, 13] | Dir: 1\n",
      "Ep 4400/5000 | Time: 3520.8s | Avg Reward: 1.29 | Scout: 3.80 | Guard: 1.83 | Scout Captures: 654 | Scout Collections: 953 | Guard Captures: 756 | Epsilon (S/G): 0.195/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1409/2000\n",
      "Scout count: 565, Guard count: 844\n",
      "Scout ratio: 0.4, Expected guard count: 845.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1419/2000\n",
      "Scout count: 569, Guard count: 850\n",
      "Scout ratio: 0.4, Expected guard count: 851.4\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1429/2000\n",
      "Scout count: 573, Guard count: 856\n",
      "Scout ratio: 0.4, Expected guard count: 857.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1439/2000\n",
      "Scout count: 576, Guard count: 863\n",
      "Scout ratio: 0.4, Expected guard count: 863.4\n",
      "Using historical opponent from episode 3250\n",
      "Phase 3, Progress: 1449/2000\n",
      "Scout count: 578, Guard count: 871\n",
      "Scout ratio: 0.4, Expected guard count: 869.4\n",
      "Phase 3, Progress: 1459/2000\n",
      "Scout count: 583, Guard count: 876\n",
      "Scout ratio: 0.4, Expected guard count: 875.4\n",
      "Using historical opponent from episode 3250\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1469/2000\n",
      "Scout count: 587, Guard count: 882\n",
      "Scout ratio: 0.4, Expected guard count: 881.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1479/2000\n",
      "Scout count: 592, Guard count: 887\n",
      "Scout ratio: 0.4, Expected guard count: 887.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1489/2000\n",
      "Scout count: 596, Guard count: 893\n",
      "Scout ratio: 0.4, Expected guard count: 893.4\n",
      "Using historical opponent from episode 3250\n",
      "Stored historical models at episode 4500\n",
      "Phase 3, Progress: 1499/2000\n",
      "Scout count: 600, Guard count: 899\n",
      "Scout ratio: 0.4, Expected guard count: 899.4\n",
      "Using historical opponent from episode 4250\n",
      "[Ep 4500] Role: 0 | Loc: [13, 4] | Dir: 0\n",
      "Ep 4500/5000 | Time: 3620.6s | Avg Reward: 7.06 | Scout: 2.99 | Guard: 9.36 | Scout Captures: 666 | Scout Collections: 970 | Guard Captures: 779 | Epsilon (S/G): 0.194/0.050\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: -10.96, Scout: -25.92, Guard: 4.00\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -6.30\n",
      "Combined Evaluation Score: -10.03\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1509/2000\n",
      "Scout count: 603, Guard count: 906\n",
      "Scout ratio: 0.4, Expected guard count: 905.4\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1519/2000\n",
      "Scout count: 609, Guard count: 910\n",
      "Scout ratio: 0.4, Expected guard count: 911.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1529/2000\n",
      "Scout count: 611, Guard count: 918\n",
      "Scout ratio: 0.4, Expected guard count: 917.4\n",
      "Phase 3, Progress: 1539/2000\n",
      "Scout count: 614, Guard count: 925\n",
      "Scout ratio: 0.4, Expected guard count: 923.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1549/2000\n",
      "Scout count: 616, Guard count: 933\n",
      "Scout ratio: 0.4, Expected guard count: 929.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1559/2000\n",
      "Scout count: 619, Guard count: 940\n",
      "Scout ratio: 0.4, Expected guard count: 935.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1569/2000\n",
      "Scout count: 624, Guard count: 945\n",
      "Scout ratio: 0.4, Expected guard count: 941.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1579/2000\n",
      "Scout count: 626, Guard count: 953\n",
      "Scout ratio: 0.4, Expected guard count: 947.4\n",
      "Phase 3, Progress: 1589/2000\n",
      "Scout count: 632, Guard count: 957\n",
      "Scout ratio: 0.4, Expected guard count: 953.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1599/2000\n",
      "Scout count: 637, Guard count: 962\n",
      "Scout ratio: 0.4, Expected guard count: 959.4\n",
      "[Ep 4600] Role: 0 | Loc: [3, 11] | Dir: 0\n",
      "Ep 4600/5000 | Time: 3721.4s | Avg Reward: -1.41 | Scout: 10.02 | Guard: -8.36 | Scout Captures: 673 | Scout Collections: 984 | Guard Captures: 794 | Epsilon (S/G): 0.192/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1609/2000\n",
      "Scout count: 640, Guard count: 969\n",
      "Scout ratio: 0.4, Expected guard count: 965.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1619/2000\n",
      "Scout count: 641, Guard count: 978\n",
      "Scout ratio: 0.4, Expected guard count: 971.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1629/2000\n",
      "Scout count: 647, Guard count: 982\n",
      "Scout ratio: 0.4, Expected guard count: 977.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1639/2000\n",
      "Scout count: 652, Guard count: 987\n",
      "Scout ratio: 0.4, Expected guard count: 983.4\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1649/2000\n",
      "Scout count: 657, Guard count: 992\n",
      "Scout ratio: 0.4, Expected guard count: 989.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1659/2000\n",
      "Scout count: 660, Guard count: 999\n",
      "Scout ratio: 0.4, Expected guard count: 995.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1669/2000\n",
      "Scout count: 664, Guard count: 1005\n",
      "Scout ratio: 0.4, Expected guard count: 1001.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1679/2000\n",
      "Scout count: 668, Guard count: 1011\n",
      "Scout ratio: 0.4, Expected guard count: 1007.4\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1689/2000\n",
      "Scout count: 672, Guard count: 1017\n",
      "Scout ratio: 0.4, Expected guard count: 1013.4\n",
      "Using historical opponent from episode 3500\n",
      "Phase 3, Progress: 1699/2000\n",
      "Scout count: 676, Guard count: 1023\n",
      "Scout ratio: 0.4, Expected guard count: 1019.4\n",
      "[Ep 4700] Role: 0 | Loc: [1, 11] | Dir: 1\n",
      "Ep 4700/5000 | Time: 3813.2s | Avg Reward: -7.38 | Scout: -2.28 | Guard: -3.48 | Scout Captures: 687 | Scout Collections: 1008 | Guard Captures: 814 | Epsilon (S/G): 0.191/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1709/2000\n",
      "Scout count: 681, Guard count: 1028\n",
      "Scout ratio: 0.4, Expected guard count: 1025.4\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1719/2000\n",
      "Scout count: 682, Guard count: 1037\n",
      "Scout ratio: 0.4, Expected guard count: 1031.4\n",
      "Using historical opponent from episode 3500\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1729/2000\n",
      "Scout count: 687, Guard count: 1042\n",
      "Scout ratio: 0.4, Expected guard count: 1037.4\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1739/2000\n",
      "Scout count: 687, Guard count: 1052\n",
      "Scout ratio: 0.4, Expected guard count: 1043.4\n",
      "Using historical opponent from episode 4250\n",
      "Stored historical models at episode 4750\n",
      "Phase 3, Progress: 1749/2000\n",
      "Scout count: 692, Guard count: 1057\n",
      "Scout ratio: 0.4, Expected guard count: 1049.4\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 1.54, Scout: -0.19, Guard: 3.27\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: 40.80\n",
      "Combined Evaluation Score: 9.39\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1759/2000\n",
      "Scout count: 699, Guard count: 1060\n",
      "Scout ratio: 0.4, Expected guard count: 1055.4\n",
      "Using historical opponent from episode 4750\n",
      "Phase 3, Progress: 1769/2000\n",
      "Scout count: 703, Guard count: 1066\n",
      "Scout ratio: 0.4, Expected guard count: 1061.4\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1779/2000\n",
      "Scout count: 707, Guard count: 1072\n",
      "Scout ratio: 0.4, Expected guard count: 1067.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1789/2000\n",
      "Scout count: 716, Guard count: 1073\n",
      "Scout ratio: 0.4, Expected guard count: 1073.4\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1799/2000\n",
      "Scout count: 719, Guard count: 1080\n",
      "Scout ratio: 0.4, Expected guard count: 1079.4\n",
      "Using historical opponent from episode 4750\n",
      "[Ep 4800] Role: 0 | Loc: [11, 3] | Dir: 1\n",
      "Ep 4800/5000 | Time: 3917.6s | Avg Reward: -1.46 | Scout: 3.78 | Guard: -0.46 | Scout Captures: 697 | Scout Collections: 1030 | Guard Captures: 828 | Epsilon (S/G): 0.189/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1809/2000\n",
      "Scout count: 723, Guard count: 1086\n",
      "Scout ratio: 0.4, Expected guard count: 1085.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1819/2000\n",
      "Scout count: 730, Guard count: 1089\n",
      "Scout ratio: 0.4, Expected guard count: 1091.4\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1829/2000\n",
      "Scout count: 732, Guard count: 1097\n",
      "Scout ratio: 0.4, Expected guard count: 1097.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1839/2000\n",
      "Scout count: 736, Guard count: 1103\n",
      "Scout ratio: 0.4, Expected guard count: 1103.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1849/2000\n",
      "Scout count: 740, Guard count: 1109\n",
      "Scout ratio: 0.4, Expected guard count: 1109.4\n",
      "Phase 3, Progress: 1859/2000\n",
      "Scout count: 742, Guard count: 1117\n",
      "Scout ratio: 0.4, Expected guard count: 1115.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1869/2000\n",
      "Scout count: 744, Guard count: 1125\n",
      "Scout ratio: 0.4, Expected guard count: 1121.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4750\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1879/2000\n",
      "Scout count: 747, Guard count: 1132\n",
      "Scout ratio: 0.4, Expected guard count: 1127.4\n",
      "Using historical opponent from episode 4500\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1889/2000\n",
      "Scout count: 750, Guard count: 1139\n",
      "Scout ratio: 0.4, Expected guard count: 1133.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1899/2000\n",
      "Scout count: 753, Guard count: 1146\n",
      "Scout ratio: 0.4, Expected guard count: 1139.4\n",
      "[Ep 4900] Role: 0 | Loc: [4, 14] | Dir: 1\n",
      "Ep 4900/5000 | Time: 4020.6s | Avg Reward: 7.59 | Scout: 4.44 | Guard: 6.63 | Scout Captures: 710 | Scout Collections: 1050 | Guard Captures: 850 | Epsilon (S/G): 0.188/0.050\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Using historical opponent from episode 4750\n",
      "Using historical opponent from episode 4500\n",
      "Phase 3, Progress: 1909/2000\n",
      "Scout count: 756, Guard count: 1153\n",
      "Scout ratio: 0.4, Expected guard count: 1145.4\n",
      "Using historical opponent from episode 3750\n",
      "Phase 3, Progress: 1919/2000\n",
      "Scout count: 762, Guard count: 1157\n",
      "Scout ratio: 0.4, Expected guard count: 1151.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4000\n",
      "Phase 3, Progress: 1929/2000\n",
      "Scout count: 768, Guard count: 1161\n",
      "Scout ratio: 0.4, Expected guard count: 1157.4\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 4750\n",
      "Phase 3, Progress: 1939/2000\n",
      "Scout count: 771, Guard count: 1168\n",
      "Scout ratio: 0.4, Expected guard count: 1163.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4750\n",
      "Phase 3, Progress: 1949/2000\n",
      "Scout count: 775, Guard count: 1174\n",
      "Scout ratio: 0.4, Expected guard count: 1169.4\n",
      "Using historical opponent from episode 4750\n",
      "Phase 3, Progress: 1959/2000\n",
      "Scout count: 780, Guard count: 1179\n",
      "Scout ratio: 0.4, Expected guard count: 1175.4\n",
      "Using historical opponent from episode 4750\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1969/2000\n",
      "Scout count: 784, Guard count: 1185\n",
      "Scout ratio: 0.4, Expected guard count: 1181.4\n",
      "Using historical opponent from episode 4750\n",
      "Using historical opponent from episode 4000\n",
      "Using historical opponent from episode 3750\n",
      "Using historical opponent from episode 4250\n",
      "Phase 3, Progress: 1979/2000\n",
      "Scout count: 787, Guard count: 1192\n",
      "Scout ratio: 0.4, Expected guard count: 1187.4\n",
      "Using historical opponent from episode 4250\n",
      "Using historical opponent from episode 4750\n",
      "Phase 3, Progress: 1989/2000\n",
      "Scout count: 792, Guard count: 1197\n",
      "Scout ratio: 0.4, Expected guard count: 1193.4\n",
      "Using historical opponent from episode 4250\n",
      "Stored historical models at episode 5000\n",
      "Phase 3, Progress: 1999/2000\n",
      "Scout count: 799, Guard count: 1200\n",
      "Scout ratio: 0.4, Expected guard count: 1199.4\n",
      "[Ep 5000] Role: 0 | Loc: [12, 14] | Dir: 1\n",
      "Ep 5000/5000 | Time: 4115.2s | Avg Reward: -6.87 | Scout: 0.16 | Guard: -0.18 | Scout Captures: 726 | Scout Collections: 1071 | Guard Captures: 866 | Epsilon (S/G): 0.186/0.050\n",
      "Evaluating against current opponent models...\n",
      "Current Opponent Evaluation: Overall: 6.21, Scout: -0.98, Guard: 13.40\n",
      "Evaluating against historical opponent models...\n",
      "Historical Opponent Evaluation: -20.35\n",
      "Combined Evaluation Score: 0.90\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Safetensors model saved to ./modelnew/model.safetensors\n",
      "Training complete! Scout episodes: 799, Guard episodes: 1201\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "# The DQN architecture with gradient accumulation for more stable updates\n",
    "class ImprovedDQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ImprovedDQN, self).__init__()\n",
    "        \n",
    "        # Replace BatchNorm with LayerNorm which works with any batch size\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.LayerNorm(256),  # LayerNorm instead of BatchNorm\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),  # LayerNorm instead of BatchNorm\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Value stream\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream with proper initialization\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights for better performance\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        value = self.value(x)\n",
    "        adv = self.advantage(x)\n",
    "        # Dueling architecture\n",
    "        return value + adv - adv.mean(1, keepdim=True)\n",
    "\n",
    "# Enhanced Prioritized Replay Buffer with separate buffers and better sampling\n",
    "class StabilizedReplayBuffer:\n",
    "    def __init__(self, capacity=100000, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        # Separate buffers for scout and guard\n",
    "        self.buffers = {\n",
    "            0: {'data': [], 'priorities': []},  # Guard\n",
    "            1: {'data': [], 'priorities': []}   # Scout\n",
    "        }\n",
    "        self.pos = {0: 0, 1: 0}\n",
    "        self.alpha = alpha\n",
    "        # Beta annealing for importance sampling\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame_idx = 0\n",
    "        \n",
    "    def get_beta(self):\n",
    "        \"\"\"Calculate current beta value for importance sampling\"\"\"\n",
    "        fraction = min(self.frame_idx / self.beta_frames, 1.0)\n",
    "        return min(self.beta_start + fraction * (1.0 - self.beta_start), 1.0)\n",
    "        \n",
    "    def push(self, s, a, r, ns, d, role):\n",
    "        \"\"\"Add experience to the buffer with improved priority assignment\"\"\"\n",
    "        self.frame_idx += 1\n",
    "        buffer = self.buffers[role]\n",
    "        \n",
    "        # Use max priority for new samples or a default value\n",
    "        max_prio = max(buffer['priorities'], default=1.0)\n",
    "        data = (s, a, r, ns, d)\n",
    "        \n",
    "        if len(buffer['data']) < self.capacity:\n",
    "            buffer['data'].append(data)\n",
    "            buffer['priorities'].append(max_prio)\n",
    "        else:\n",
    "            buffer['data'][self.pos[role]] = data\n",
    "            buffer['priorities'][self.pos[role]] = max_prio\n",
    "            \n",
    "        self.pos[role] = (self.pos[role] + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size, role):\n",
    "        \"\"\"Sample with prioritization and proper beta scheduling\"\"\"\n",
    "        buffer = self.buffers[role]\n",
    "        if len(buffer['data']) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Get current beta value\n",
    "        beta = self.get_beta()\n",
    "            \n",
    "        # Calculate sampling probabilities\n",
    "        probs = np.array(buffer['priorities']) ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Sample with priority\n",
    "        indices = np.random.choice(len(buffer['data']), batch_size, p=probs)\n",
    "        samples = [buffer['data'][i] for i in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = ((len(buffer['data']) * probs[indices]) ** -beta).astype(np.float32)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        s, a, r, ns, d = zip(*samples)\n",
    "        return (np.array(s), np.array(a), np.array(r), np.array(ns),\n",
    "                np.array(d), weights, indices)\n",
    "                \n",
    "    def update_priorities(self, indices, priorities, role):\n",
    "        \"\"\"Update priorities with clipping to prevent extreme values\"\"\"\n",
    "        priorities = np.clip(priorities, 0.01, 10.0)  # Clip priorities for stability\n",
    "        for i, p in zip(indices, priorities):\n",
    "            self.buffers[role]['priorities'][i] = p\n",
    "            \n",
    "    def __len__(self, role):\n",
    "        return len(self.buffers[role]['data'])\n",
    "\n",
    "class RobustDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Role-specific configurations\n",
    "        self.configs = {\n",
    "            0: {  # Guard\n",
    "                \"lr\": 2e-4,  # Increased from original\n",
    "                \"update_freq\": 250,\n",
    "                \"grad_steps\": 1\n",
    "            },\n",
    "            1: {  # Scout\n",
    "                \"lr\": 5e-5,  # Lower for more stability\n",
    "                \"update_freq\": 200,\n",
    "                \"grad_steps\": 2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Build separate agents for scout and guard\n",
    "        self.agents = {\n",
    "            0: self._build_agent(state_dim, action_dim, self.configs[0]),  # Guard\n",
    "            1: self._build_agent(state_dim, action_dim, self.configs[1])   # Scout\n",
    "        }\n",
    "        \n",
    "        # Exploration parameters with slower decay\n",
    "        self.epsilon = {\n",
    "            0: 0.1,  # Guard starts with lower exploration\n",
    "            1: 0.3   # Scout needs more exploration\n",
    "        }\n",
    "        self.epsilon_decay = {\n",
    "            0: 0.9995,  # Slower decay\n",
    "            1: 0.9998   # Even slower decay for scout\n",
    "        }\n",
    "        self.epsilon_final = {\n",
    "            0: 0.05,\n",
    "            1: 0.1   # Higher final epsilon for scout\n",
    "        }\n",
    "        \n",
    "        # Training counters\n",
    "        self.update_counter = {0: 0, 1: 0}\n",
    "        \n",
    "        # Historical models for self-play (new)\n",
    "        self.historical_models = {\n",
    "            0: [],  # Guard history\n",
    "            1: []   # Scout history\n",
    "        }\n",
    "        self.historical_model_episodes = []\n",
    "        \n",
    "    def _build_agent(self, state_dim, action_dim, config):\n",
    "        \"\"\"Build agent components with improved configuration\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = ImprovedDQN(state_dim, action_dim).to(device)\n",
    "        target = ImprovedDQN(state_dim, action_dim).to(device)\n",
    "        target.load_state_dict(model.state_dict())\n",
    "        \n",
    "        # Freeze target network parameters\n",
    "        for param in target.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Use Adam with smaller epsilon for numerical stability\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-5)\n",
    "        \n",
    "        # Add learning rate scheduler for better convergence\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.5)\n",
    "        \n",
    "        return {\n",
    "            \"model\": model, \n",
    "            \"target\": target, \n",
    "            \"optimizer\": optimizer,\n",
    "            \"scheduler\": scheduler,\n",
    "            \"device\": device,\n",
    "            \"config\": config\n",
    "        }\n",
    "    \n",
    "    # Store historical models for self-play (new method)\n",
    "    def store_historical_model(self, episode):\n",
    "        \"\"\"Store a snapshot of current models for later self-play\"\"\"\n",
    "        guard_model = copy.deepcopy(self.agents[0][\"model\"].state_dict())\n",
    "        scout_model = copy.deepcopy(self.agents[1][\"model\"].state_dict())\n",
    "        \n",
    "        self.historical_models[0].append(guard_model)\n",
    "        self.historical_models[1].append(scout_model)\n",
    "        self.historical_model_episodes.append(episode)\n",
    "        \n",
    "        # Keep only last 5 historical models to save memory\n",
    "        if len(self.historical_models[0]) > 5:\n",
    "            self.historical_models[0].pop(0)\n",
    "            self.historical_models[1].pop(0)\n",
    "            self.historical_model_episodes.pop(0)\n",
    "        \n",
    "        print(f\"Stored historical models at episode {episode}\")\n",
    "    \n",
    "    # Act using either current or historical model (modified method)\n",
    "    def act(self, state, role, evaluation=False, use_historical=False, historical_idx=None):\n",
    "        \"\"\"Select action using epsilon-greedy with optional historical model\"\"\"\n",
    "        epsilon = 0.01 if evaluation else self.epsilon[role]\n",
    "        device = self.agents[role][\"device\"]\n",
    "        \n",
    "        # Epsilon-greedy exploration\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, 4)\n",
    "        \n",
    "        # Convert state to tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use historical model if specified\n",
    "        if use_historical and historical_idx is not None and len(self.historical_models[role]) > historical_idx:\n",
    "            # Create temporary model with historical weights\n",
    "            temp_model = ImprovedDQN(self.agents[role][\"model\"].feature[0].in_features, \n",
    "                                     self.agents[role][\"model\"].advantage[-1].out_features).to(device)\n",
    "            temp_model.load_state_dict(self.historical_models[role][historical_idx])\n",
    "            temp_model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = temp_model(state)\n",
    "                action = q_values.argmax().item()\n",
    "                \n",
    "            return action\n",
    "        \n",
    "        # Otherwise use current model\n",
    "        self.agents[role][\"model\"].eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.agents[role][\"model\"](state)\n",
    "            action = q_values.argmax().item()\n",
    "        self.agents[role][\"model\"].train()\n",
    "        \n",
    "        return action\n",
    "        \n",
    "    def decay_epsilon(self, role):\n",
    "        \"\"\"Decay epsilon according to role-specific schedule\"\"\"\n",
    "        self.epsilon[role] = max(\n",
    "            self.epsilon_final[role],\n",
    "            self.epsilon[role] * self.epsilon_decay[role]\n",
    "        )\n",
    "            \n",
    "    def update(self, buffer, role, batch_size=64):\n",
    "        \"\"\"Update the agent with improved training stability measures\"\"\"\n",
    "        result = buffer.sample(batch_size, role)\n",
    "        if result is None:\n",
    "            return\n",
    "            \n",
    "        s, a, r, ns, d, w, idx = result\n",
    "        agent = self.agents[role]\n",
    "        device = agent[\"device\"]\n",
    "        config = agent[\"config\"]\n",
    "        \n",
    "        # Convert numpy arrays to tensors\n",
    "        s_t = torch.tensor(s, dtype=torch.float32).to(device)\n",
    "        ns_t = torch.tensor(ns, dtype=torch.float32).to(device)\n",
    "        a_t = torch.tensor(a).long().unsqueeze(1).to(device)\n",
    "        r_t = torch.tensor(r).unsqueeze(1).to(device)\n",
    "        d_t = torch.tensor(d).unsqueeze(1).float().to(device)\n",
    "        w_t = torch.tensor(w).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Ensure model is in training mode for batch updates\n",
    "        agent[\"model\"].train()\n",
    "        agent[\"target\"].eval()\n",
    "        \n",
    "        # Update step with multiple gradient accumulations for stability\n",
    "        for _ in range(config[\"grad_steps\"]):\n",
    "            # Calculate current Q values\n",
    "            q_vals = agent[\"model\"](s_t).gather(1, a_t)\n",
    "            \n",
    "            # Double DQN update - reduces overestimation bias\n",
    "            with torch.no_grad():\n",
    "                next_actions = agent[\"model\"](ns_t).argmax(1, keepdim=True)\n",
    "                next_q = agent[\"target\"](ns_t).gather(1, next_actions)\n",
    "            \n",
    "                # Expected Q values with proper scaling\n",
    "                expected = r_t + self.gamma * next_q * (1 - d_t)\n",
    "            \n",
    "            # Use Huber loss for more stable updates\n",
    "            loss = F.smooth_l1_loss(q_vals, expected, reduction='none')\n",
    "            weighted_loss = (w_t * loss).mean()\n",
    "            \n",
    "            # Calculate TD errors for priority update\n",
    "            with torch.no_grad():\n",
    "                td_err = (q_vals - expected).abs().detach().cpu().numpy().flatten()\n",
    "            \n",
    "            # Optimization step\n",
    "            agent[\"optimizer\"].zero_grad()\n",
    "            weighted_loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(agent[\"model\"].parameters(), max_norm=1.0)\n",
    "            \n",
    "            agent[\"optimizer\"].step()\n",
    "        \n",
    "        # Update priorities in the replay buffer\n",
    "        buffer.update_priorities(idx, td_err, role)\n",
    "        \n",
    "        # Step the learning rate scheduler\n",
    "        agent[\"scheduler\"].step()\n",
    "        \n",
    "        # Target network update with fixed frequency\n",
    "        self.update_counter[role] += 1\n",
    "        if self.update_counter[role] % config[\"update_freq\"] == 0:\n",
    "            agent[\"target\"].load_state_dict(agent[\"model\"].state_dict())\n",
    "            \n",
    "    def save(self, path_prefix=\"./modelnew/\"):\n",
    "            \"\"\"Save both scout and guard models using safetensors.\"\"\"\n",
    "            try:\n",
    "                from safetensors.torch import save_file\n",
    "                os.makedirs(path_prefix, exist_ok=True)\n",
    "                flattened = {}\n",
    "                for role_name, role_id in [(\"guard\", 0), (\"scout\", 1)]:\n",
    "                    for part in [\"model\", \"target\"]:\n",
    "                        state_dict = self.agents[role_id][part].state_dict()\n",
    "                        for key, tensor in state_dict.items():\n",
    "                            if isinstance(tensor, torch.Tensor):\n",
    "                                flattened[f\"{role_name}_{part}.{key}\"] = tensor\n",
    "\n",
    "                save_file(flattened, os.path.join(path_prefix, \"model.safetensors\"))\n",
    "                print(f\"Safetensors model saved to {path_prefix}model.safetensors\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving model: {e}\")\n",
    "\n",
    "    def load(self, path):\n",
    "            \"\"\"Load model from safetensors file\"\"\"\n",
    "            try:\n",
    "                from safetensors.torch import load_file\n",
    "                loaded = load_file(path)\n",
    "\n",
    "                # Extract and load the model weights\n",
    "                for role_name, role_id in [(\"guard\", 0), (\"scout\", 1)]:\n",
    "                    for part in [\"model\", \"target\"]:\n",
    "                        model = self.agents[role_id][part]\n",
    "                        state_dict = model.state_dict()\n",
    "\n",
    "                        # Update state dict with loaded weights\n",
    "                        for key in state_dict:\n",
    "                            loaded_key = f\"{role_name}_{part}.{key}\"\n",
    "                            if loaded_key in loaded:\n",
    "                                state_dict[key] = loaded[loaded_key]\n",
    "\n",
    "                        # Load updated state dict\n",
    "                        model.load_state_dict(state_dict)\n",
    "                print(f\"Model loaded from {path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "            \n",
    "# Enhanced flatten observation function with normalization\n",
    "def compute_improved_reward(obs, reward, role, env, current_phase=0):\n",
    "    \"\"\"Shaped rewards with better learning signals and stability penalties\"\"\"\n",
    "    # Safety check for None observation\n",
    "    if obs is None:\n",
    "        print(\"Warning: Received None observation in compute_improved_reward. Returning original reward.\")\n",
    "        return reward\n",
    "        \n",
    "    agent_pos = obs['location']\n",
    "    \n",
    "    # Start with original reward\n",
    "    shaped_reward = reward\n",
    "    \n",
    "    if role == 1:  # scout\n",
    "        # Scout reward modifiers\n",
    "        \n",
    "        # 1. Make capture penalty less severe\n",
    "        if reward <= -40:  # Captured\n",
    "            shaped_reward = -10  # Less severe than before\n",
    "        \n",
    "        # 2. Increase proximity rewards\n",
    "        targets = env.recon_points.union(env.missions) if hasattr(env, 'recon_points') and hasattr(env, 'missions') else set()\n",
    "        if targets:\n",
    "            # Calculate distances to targets\n",
    "            distances = [abs(agent_pos[0] - tx) + abs(agent_pos[1] - ty) for tx, ty in targets]\n",
    "            min_dist = min(distances) if distances else 16\n",
    "            \n",
    "            # Stronger proximity reward with slower decay\n",
    "            proximity_reward = 1.2 * math.exp(-0.15 * min_dist)\n",
    "            shaped_reward += proximity_reward\n",
    "            \n",
    "            # Progress reward - increase for getting closer\n",
    "            if hasattr(env, 'last_scout_pos') and env.last_scout_pos is not None:\n",
    "                old_distances = [abs(env.last_scout_pos[0] - tx) + abs(env.last_scout_pos[1] - ty) \n",
    "                               for tx, ty in targets]\n",
    "                old_min_dist = min(old_distances) if old_distances else 16\n",
    "                \n",
    "                if min_dist < old_min_dist:\n",
    "                    shaped_reward += 0.5  # Higher progress bonus\n",
    "        \n",
    "        # Store position for next step\n",
    "        env.last_scout_pos = agent_pos.copy()\n",
    "        \n",
    "        # 3. Amplify collection rewards more\n",
    "        if reward >= 1:  # Collected something\n",
    "            shaped_reward *= 1.5  # Stronger amplification\n",
    "            \n",
    "    else:  # guard\n",
    "        # Add small positive reward just for being a guard to offset negative bias\n",
    "        shaped_reward += 0.1\n",
    "        \n",
    "        # Focus more on patrolling high-value areas\n",
    "        targets = env.missions if hasattr(env, 'missions') else set()\n",
    "        if targets:\n",
    "            distances = [abs(agent_pos[0] - tx) + abs(agent_pos[1] - ty) for tx, ty in targets]\n",
    "            min_dist = min(distances) if distances else 16\n",
    "            \n",
    "            # Stronger rewards for guards near targets\n",
    "            shaped_reward += 0.2 * math.exp(-0.2 * min_dist)\n",
    "        \n",
    "        # Reward guards for captures (assumption: high positive reward means capture)\n",
    "        if reward > 5:  # If there's a capture reward\n",
    "            shaped_reward *= 1.5  # Amplify it\n",
    "    \n",
    "    # Add reward stability penalty to prevent wild oscillations (new)\n",
    "    role_key = f'last_reward_{role}'\n",
    "    if hasattr(env, role_key) and getattr(env, role_key) is not None:\n",
    "        reward_delta = abs(shaped_reward - getattr(env, role_key))\n",
    "        shaped_reward -= 0.1 * reward_delta  # Small penalty for dramatic reward changes\n",
    "    \n",
    "    # Store the current reward for next time\n",
    "    setattr(env, role_key, shaped_reward)\n",
    "    \n",
    "    # Add exploration bonus in later phases (new)\n",
    "    if current_phase >= 1:  # In Phase 2 or later\n",
    "        # Small random exploration bonus to encourage trying new strategies\n",
    "        shaped_reward += 0.1 * random.random()\n",
    "    \n",
    "    return shaped_reward\n",
    "\n",
    "# Enhanced flatten observation function with normalization and safety checks\n",
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Convert observation to a flat vector with consistent size and better normalization.\n",
    "    \"\"\"\n",
    "    # Safety check for None observation\n",
    "    if obs is None:\n",
    "        print(\"Warning: Received None observation in preprocess_observation. Returning None.\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Get viewcone and normalize to [0,1]\n",
    "        # Handle potential missing keys\n",
    "        if 'viewcone' not in obs:\n",
    "            print(\"Warning: 'viewcone' not found in observation. Using zeros.\")\n",
    "            flat_view = np.zeros(36)  # Assuming 36 viewcone elements\n",
    "        else:\n",
    "            flat_view = np.array(obs['viewcone']).flatten() / 255.0\n",
    "        \n",
    "        # Direction as one-hot encoding\n",
    "        if 'direction' not in obs:\n",
    "            print(\"Warning: 'direction' not found in observation. Using zeros.\")\n",
    "            direction_onehot = np.zeros(4)\n",
    "        else:\n",
    "            direction_onehot = np.zeros(4)\n",
    "            direction_onehot[obs['direction']] = 1\n",
    "        \n",
    "        # Role indicator (scout or guard)\n",
    "        if 'scout' not in obs:\n",
    "            print(\"Warning: 'scout' not found in observation. Using default False.\")\n",
    "            is_scout = np.array([0])\n",
    "        else:\n",
    "            is_scout = np.array([obs['scout']])\n",
    "        \n",
    "        # Location normalized to [0,1]\n",
    "        if 'location' not in obs:\n",
    "            print(\"Warning: 'location' not found in observation. Using zeros.\")\n",
    "            location = np.zeros(2)\n",
    "        else:\n",
    "            location = np.array(obs['location']) / 15.0\n",
    "        \n",
    "        # Step count normalized\n",
    "        if 'step' not in obs:\n",
    "            print(\"Warning: 'step' not found in observation. Using zero.\")\n",
    "            step = np.array([0])\n",
    "        else:\n",
    "            step = np.array([obs['step'] / 100.0])\n",
    "        \n",
    "        # Combine all features\n",
    "        return np.concatenate([flat_view, direction_onehot, is_scout, location, step])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocess_observation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main training function with improved curriculum and historical self-play\n",
    "def train_with_curriculum(env_class, episodes=5000, save_interval=100):\n",
    "    \"\"\"Training with curriculum learning, improved stability, and historical self-play\"\"\"\n",
    "    \n",
    "    # Initialize environment\n",
    "    env = env_class()\n",
    "    state_dim = 43  # Based on flatten_obs output\n",
    "    action_dim = 5\n",
    "    \n",
    "    # Create agent with optimized parameters\n",
    "    agent = RobustDQNAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    \n",
    "    # Use replay buffer with proper prioritization\n",
    "    buffer = StabilizedReplayBuffer(capacity=100000)\n",
    "    \n",
    "    # Training tracking\n",
    "    rewards_window = {\n",
    "        'all': [],\n",
    "        'scout': [],\n",
    "        'guard': []\n",
    "    }\n",
    "    scout_captures = 0\n",
    "    scout_collections = 0\n",
    "    guard_captures = 0\n",
    "    \n",
    "    # Ensure environment has attributes we need\n",
    "    if not hasattr(env, 'last_scout_pos'):\n",
    "        env.last_scout_pos = None\n",
    "    if not hasattr(env, 'last_reward_0'):\n",
    "        env.last_reward_0 = None  # Guard reward tracking\n",
    "    if not hasattr(env, 'last_reward_1'):\n",
    "        env.last_reward_1 = None  # Scout reward tracking\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Modified curriculum with more gradual learning rate transitions\n",
    "    curriculum = [\n",
    "        # Phase 1: More balanced approach from the start\n",
    "        {'episodes': 1000, 'scout_ratio': 0.6, 'scout_lr': 5e-5, 'guard_lr': 2e-4},\n",
    "        # Phase 2: More gradual learning rate transition\n",
    "        {'episodes': 2000, 'scout_ratio': 0.5, 'scout_lr': 4e-5, 'guard_lr': 1.5e-4},\n",
    "        # Phase 3: Even more gradual reduction\n",
    "        {'episodes': 2000, 'scout_ratio': 0.4, 'scout_lr': 3e-5, 'guard_lr': 1e-4}\n",
    "    ]\n",
    "    \n",
    "    # Track progress through curriculum\n",
    "    current_phase = 0\n",
    "    phase_progress = 0\n",
    "    phase_role_counts = {0: 0, 1: 0}  # Count episodes by role\n",
    "    \n",
    "    # Initialize counter for role balancing\n",
    "    episodes_since_guard = 0\n",
    "    episodes_since_scout = 0\n",
    "    \n",
    "    # Historical model snapshots\n",
    "    historical_snapshot_interval = 250\n",
    "    \n",
    "    # Optional: Enable periodic evaluation\n",
    "    evaluation_interval = 250\n",
    "    best_eval_reward = -float('inf')\n",
    "    \n",
    "    for ep in range(1, episodes+1):\n",
    "        # Update curriculum phase if needed\n",
    "        if current_phase < len(curriculum) - 1:\n",
    "            if phase_progress >= curriculum[current_phase]['episodes']:\n",
    "                current_phase += 1\n",
    "                phase_progress = 0\n",
    "                phase_role_counts = {0: 0, 1: 0}\n",
    "                print(f\"Moving to curriculum phase {current_phase+1}\")\n",
    "                \n",
    "                # Update learning rates\n",
    "                for role in [0, 1]:\n",
    "                    new_lr = curriculum[current_phase][f\"{'scout' if role==1 else 'guard'}_lr\"]\n",
    "                    for param_group in agent.agents[role][\"optimizer\"].param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "        \n",
    "        # Store historical models periodically\n",
    "        if ep % historical_snapshot_interval == 0:\n",
    "            agent.store_historical_model(ep)\n",
    "        \n",
    "        # Improved role selection logic for balanced training\n",
    "        scout_ratio = curriculum[current_phase]['scout_ratio']\n",
    "        expected_guard_count = phase_progress * (1 - scout_ratio)\n",
    "        \n",
    "        # Debug stats every 10 episodes\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Phase {current_phase+1}, Progress: {phase_progress}/{curriculum[current_phase]['episodes']}\")\n",
    "            print(f\"Scout count: {phase_role_counts[1]}, Guard count: {phase_role_counts[0]}\")\n",
    "            print(f\"Scout ratio: {scout_ratio}, Expected guard count: {expected_guard_count:.1f}\")\n",
    "        \n",
    "        # Force a guard episode if we're behind or it's been too long since last guard episode\n",
    "        if phase_role_counts[0] < expected_guard_count - 5 or episodes_since_guard > 15:\n",
    "            role = 0  # Guard\n",
    "            episodes_since_guard = 0\n",
    "            episodes_since_scout += 1\n",
    "        # Force a scout episode if it's been too long since last scout episode\n",
    "        elif episodes_since_scout > 15:\n",
    "            role = 1  # Scout\n",
    "            episodes_since_scout = 0\n",
    "            episodes_since_guard += 1\n",
    "        # Regular probability-based selection\n",
    "        else:\n",
    "            if random.random() < scout_ratio:\n",
    "                role = 1  # Scout\n",
    "                episodes_since_guard += 1\n",
    "                episodes_since_scout = 0\n",
    "            else:\n",
    "                role = 0  # Guard\n",
    "                episodes_since_scout += 1\n",
    "                episodes_since_guard = 0\n",
    "            \n",
    "        # Update phase tracking\n",
    "        phase_progress += 1\n",
    "        phase_role_counts[role] += 1\n",
    "        \n",
    "        # Set environment role\n",
    "        env.is_scout = (role == 1)\n",
    "        \n",
    "        # Use historical opponent occasionally for self-play (new)\n",
    "        use_historical_opponent = (len(agent.historical_models[0]) > 0 and\n",
    "                                  random.random() < 0.2)  # 20% chance\n",
    "        historical_idx = random.randint(0, len(agent.historical_models[0])-1) if use_historical_opponent else None\n",
    "        \n",
    "        if use_historical_opponent:\n",
    "            print(f\"Using historical opponent from episode {agent.historical_model_episodes[historical_idx]}\")\n",
    "        \n",
    "        # Reset environment\n",
    "        obs = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Track recon/mission collection\n",
    "        initial_recon_count = len(env.recon_points) if hasattr(env, 'recon_points') else 0\n",
    "        initial_mission_count = len(env.missions) if hasattr(env, 'missions') else 0\n",
    "        \n",
    "        # Episode loop\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Select action (potentially using historical model for opponent)\n",
    "            action = agent.act(state, role)\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            \n",
    "            # Shape reward for better learning signal, with current phase information\n",
    "            shaped_reward = compute_improved_reward(next_obs, reward, role, env, current_phase)\n",
    "            \n",
    "            # Store experience in buffer\n",
    "            buffer.push(state, action, shaped_reward, next_state, done, role)\n",
    "            \n",
    "            # Update agent with adaptive frequency\n",
    "            # More frequent updates to ensure learning\n",
    "            update_freq = 1 if ep < 1000 or role == 0 else 2\n",
    "            if steps % update_freq == 0:\n",
    "                agent.update(buffer, role)\n",
    "            \n",
    "            # Print debug info periodically\n",
    "            if ep % 100 == 0 and steps == 0:\n",
    "                print(f\"[Ep {ep}] Role: {role} | Loc: {obs['location']} | Dir: {obs['direction']}\")\n",
    "                \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        # Check for scout captures or collections\n",
    "        if role == 1:  # scout\n",
    "            if reward <= -40:  # Captured\n",
    "                scout_captures += 1\n",
    "            \n",
    "            # Check for collections\n",
    "            if hasattr(env, 'recon_points') and hasattr(env, 'missions'):\n",
    "                recon_collected = initial_recon_count - len(env.recon_points)\n",
    "                missions_completed = initial_mission_count - len(env.missions)\n",
    "                if recon_collected > 0 or missions_completed > 0:\n",
    "                    scout_collections += 1\n",
    "        else:  # guard\n",
    "            # Assume positive reward for guard might be capture\n",
    "            if reward > 5:\n",
    "                guard_captures += 1\n",
    "        \n",
    "        # Decay exploration rate\n",
    "        agent.decay_epsilon(role)\n",
    "        \n",
    "        # Track rewards by role (using sliding window)\n",
    "        window_size = 50\n",
    "        if len(rewards_window['all']) >= window_size:\n",
    "            rewards_window['all'].pop(0)\n",
    "        rewards_window['all'].append(episode_reward)\n",
    "        \n",
    "        if role == 1:  # scout\n",
    "            if len(rewards_window['scout']) >= window_size:\n",
    "                rewards_window['scout'].pop(0)\n",
    "            rewards_window['scout'].append(episode_reward)\n",
    "        else:  # guard\n",
    "            if len(rewards_window['guard']) >= window_size:\n",
    "                rewards_window['guard'].pop(0)\n",
    "            rewards_window['guard'].append(episode_reward)\n",
    "        \n",
    "        # Log training progress periodically\n",
    "        if ep % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Calculate stats\n",
    "            avg_r = np.mean(rewards_window['all']) if rewards_window['all'] else 0\n",
    "            avg_scout = np.mean(rewards_window['scout']) if rewards_window['scout'] else 0\n",
    "            avg_guard = np.mean(rewards_window['guard']) if rewards_window['guard'] else 0\n",
    "            \n",
    "            # Log training status with guard captures\n",
    "            print(f\"Ep {ep}/{episodes} | \"\n",
    "                  f\"Time: {elapsed:.1f}s | \"\n",
    "                  f\"Avg Reward: {avg_r:.2f} | \"\n",
    "                  f\"Scout: {avg_scout:.2f} | \"\n",
    "                  f\"Guard: {avg_guard:.2f} | \"\n",
    "                  f\"Scout Captures: {scout_captures} | \"\n",
    "                  f\"Scout Collections: {scout_collections} | \"\n",
    "                  f\"Guard Captures: {guard_captures} | \"\n",
    "                  f\"Epsilon (S/G): {agent.epsilon[1]:.3f}/{agent.epsilon[0]:.3f}\")\n",
    "        \n",
    "        # Enhanced evaluation with both current and historical opponents\n",
    "        if ep % evaluation_interval == 0:\n",
    "            eval_rewards = []\n",
    "            eval_scout_rewards = []\n",
    "            eval_guard_rewards = []\n",
    "            \n",
    "            # First evaluate with current opponent models\n",
    "            print(\"Evaluating against current opponent models...\")\n",
    "            for _ in range(10):\n",
    "                # Ensure we test both roles\n",
    "                if _ < 5:\n",
    "                    eval_role = 1  # Scout\n",
    "                else:\n",
    "                    eval_role = 0  # Guard\n",
    "                    \n",
    "                env.is_scout = (eval_role == 1)\n",
    "                eval_obs = env.reset()\n",
    "                eval_state = preprocess_observation(eval_obs)\n",
    "                eval_episode_reward = 0\n",
    "                eval_done = False\n",
    "                \n",
    "                while not eval_done:\n",
    "                    # Use evaluation mode (minimal exploration)\n",
    "                    eval_action = agent.act(eval_state, eval_role, evaluation=True)\n",
    "                    eval_next_obs, eval_reward, eval_done = env.step(eval_action)\n",
    "                    eval_next_state = preprocess_observation(eval_next_obs)\n",
    "                    eval_episode_reward += eval_reward\n",
    "                    eval_state = eval_next_state\n",
    "                \n",
    "                eval_rewards.append(eval_episode_reward)\n",
    "                if eval_role == 1:\n",
    "                    eval_scout_rewards.append(eval_episode_reward)\n",
    "                else:\n",
    "                    eval_guard_rewards.append(eval_episode_reward)\n",
    "            \n",
    "            avg_eval_reward = np.mean(eval_rewards)\n",
    "            avg_eval_scout = np.mean(eval_scout_rewards) if eval_scout_rewards else 0\n",
    "            avg_eval_guard = np.mean(eval_guard_rewards) if eval_guard_rewards else 0\n",
    "            \n",
    "            print(f\"Current Opponent Evaluation: Overall: {avg_eval_reward:.2f}, Scout: {avg_eval_scout:.2f}, Guard: {avg_eval_guard:.2f}\")\n",
    "            \n",
    "            # Also evaluate against historical opponents if available\n",
    "            if len(agent.historical_models[0]) > 0:\n",
    "                historical_eval_rewards = []\n",
    "                \n",
    "                print(\"Evaluating against historical opponent models...\")\n",
    "                # Use oldest historical model for diversity\n",
    "                historical_idx = 0\n",
    "                \n",
    "                for _ in range(6):\n",
    "                    # Test both roles\n",
    "                    eval_role = 1 if _ < 3 else 0\n",
    "                    \n",
    "                    env.is_scout = (eval_role == 1)\n",
    "                    eval_obs = env.reset()\n",
    "                    eval_state = preprocess_observation(eval_obs)\n",
    "                    eval_episode_reward = 0\n",
    "                    eval_done = False\n",
    "                    \n",
    "                    while not eval_done:\n",
    "                        # Current agent plays against historical opponent\n",
    "                        eval_action = agent.act(eval_state, eval_role, evaluation=True)\n",
    "                        eval_next_obs, eval_reward, eval_done = env.step(eval_action)\n",
    "                        eval_next_state = preprocess_observation(eval_next_obs)\n",
    "                        eval_episode_reward += eval_reward\n",
    "                        eval_state = eval_next_state\n",
    "                    \n",
    "                    historical_eval_rewards.append(eval_episode_reward)\n",
    "                \n",
    "                avg_historical_eval = np.mean(historical_eval_rewards)\n",
    "                print(f\"Historical Opponent Evaluation: {avg_historical_eval:.2f}\")\n",
    "                \n",
    "                # Combined evaluation score (80% current, 20% historical)\n",
    "                combined_eval = 0.8 * avg_eval_reward + 0.2 * avg_historical_eval\n",
    "                print(f\"Combined Evaluation Score: {combined_eval:.2f}\")\n",
    "                \n",
    "                # Use combined score for best model tracking\n",
    "                if combined_eval > best_eval_reward:\n",
    "                    best_eval_reward = combined_eval\n",
    "                    agent.save(\"./model/best_model/\")\n",
    "                    print(f\"New best model saved! Combined Reward: {best_eval_reward:.2f}\")\n",
    "            else:\n",
    "                # If no historical models yet, just use current evaluation\n",
    "                if avg_eval_reward > best_eval_reward:\n",
    "                    best_eval_reward = avg_eval_reward\n",
    "                    agent.save(\"./model/best_model/\")\n",
    "                    print(f\"New best model saved! Reward: {best_eval_reward:.2f}\")\n",
    "        \n",
    "        # Regular model saving\n",
    "        if ep % save_interval == 0:\n",
    "            agent.save()\n",
    "    \n",
    "    # Final save\n",
    "    agent.save()\n",
    "    print(f\"Training complete! Scout episodes: {phase_role_counts[1]}, \"\n",
    "          f\"Guard episodes: {phase_role_counts[0]}\")\n",
    "    return agent\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Start improved training \n",
    "    agent = train_with_curriculum(TILAIEnv, episodes=5000, save_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27650385-2ec7-4eaa-ad2d-1c78030beda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "harikernel",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Hari Training Kernel",
   "language": "python",
   "name": "harikernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
